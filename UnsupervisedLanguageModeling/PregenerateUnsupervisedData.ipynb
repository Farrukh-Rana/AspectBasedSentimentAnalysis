{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm, trange\n",
    "from tempfile import TemporaryDirectory\n",
    "import shelve\n",
    "\n",
    "from random import random, randrange, randint, shuffle, choice, sample\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentDatabase:\n",
    "    def __init__(self, reduce_memory=False):\n",
    "        if reduce_memory:\n",
    "            self.temp_dir = TemporaryDirectory()\n",
    "            self.working_dir = Path(self.temp_dir.name)\n",
    "            self.document_shelf_filepath = self.working_dir / 'shelf.db'\n",
    "            self.document_shelf = shelve.open(str(self.document_shelf_filepath),\n",
    "                                              flag='n', protocol=-1)\n",
    "            self.documents = None\n",
    "        else:\n",
    "            self.documents = []\n",
    "            self.document_shelf = None\n",
    "            self.document_shelf_filepath = None\n",
    "            self.temp_dir = None\n",
    "        self.doc_lengths = []\n",
    "        self.doc_cumsum = None\n",
    "        self.cumsum_max = None\n",
    "        self.reduce_memory = reduce_memory\n",
    "\n",
    "    def add_document(self, document):\n",
    "        if not document:\n",
    "            return\n",
    "        if self.reduce_memory:\n",
    "            current_idx = len(self.doc_lengths)\n",
    "            self.document_shelf[str(current_idx)] = document\n",
    "        else:\n",
    "            self.documents.append(document)\n",
    "        self.doc_lengths.append(len(document))\n",
    "\n",
    "    def _precalculate_doc_weights(self):\n",
    "        self.doc_cumsum = np.cumsum(self.doc_lengths)\n",
    "        self.cumsum_max = self.doc_cumsum[-1]\n",
    "\n",
    "    def sample_doc(self, current_idx, sentence_weighted=True):\n",
    "        # Uses the current iteration counter to ensure we don't sample the same doc twice\n",
    "        if sentence_weighted:\n",
    "            # With sentence weighting, we sample docs proportionally to their sentence length\n",
    "            if self.doc_cumsum is None or len(self.doc_cumsum) != len(self.doc_lengths):\n",
    "                self._precalculate_doc_weights()\n",
    "            rand_start = self.doc_cumsum[current_idx]\n",
    "            rand_end = rand_start + self.cumsum_max - self.doc_lengths[current_idx]\n",
    "            sentence_index = randrange(rand_start, rand_end) % self.cumsum_max\n",
    "            sampled_doc_index = np.searchsorted(self.doc_cumsum, sentence_index, side='right')\n",
    "        else:\n",
    "            # If we don't use sentence weighting, then every doc has an equal chance to be chosen\n",
    "            sampled_doc_index = (current_idx + randrange(1, len(self.doc_lengths))) % len(self.doc_lengths)\n",
    "        assert sampled_doc_index != current_idx\n",
    "        if self.reduce_memory:\n",
    "            return self.document_shelf[str(sampled_doc_index)]\n",
    "        else:\n",
    "            return self.documents[sampled_doc_index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.doc_lengths)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        if self.reduce_memory:\n",
    "            return self.document_shelf[str(item)]\n",
    "        else:\n",
    "            return self.documents[item]\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, traceback):\n",
    "        if self.document_shelf is not None:\n",
    "            self.document_shelf.close()\n",
    "        if self.temp_dir is not None:\n",
    "            self.temp_dir.cleanup()\n",
    "\n",
    "\n",
    "def truncate_seq_pair(tokens_a, tokens_b, max_num_tokens):\n",
    "    \"\"\"Truncates a pair of sequences to a maximum sequence length. Lifted from Google's BERT repo.\"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_num_tokens:\n",
    "            break\n",
    "\n",
    "        trunc_tokens = tokens_a if len(tokens_a) > len(tokens_b) else tokens_b\n",
    "        assert len(trunc_tokens) >= 1\n",
    "\n",
    "        # We want to sometimes truncate from the front and sometimes from the\n",
    "        # back to add more randomness and avoid biases.\n",
    "        if random() < 0.5:\n",
    "            del trunc_tokens[0]\n",
    "        else:\n",
    "            trunc_tokens.pop()\n",
    "\n",
    "\n",
    "def create_masked_lm_predictions(tokens, masked_lm_prob, max_predictions_per_seq, vocab_list):\n",
    "    \"\"\"Creates the predictions for the masked LM objective. This is mostly copied from the Google BERT repo, but\n",
    "    with several refactors to clean it up and remove a lot of unnecessary variables.\"\"\"\n",
    "    cand_indices = []\n",
    "    for (i, token) in enumerate(tokens):\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "            continue\n",
    "        cand_indices.append(i)\n",
    "\n",
    "    num_to_mask = min(max_predictions_per_seq,\n",
    "                      max(1, int(round(len(tokens) * masked_lm_prob))))\n",
    "    shuffle(cand_indices)\n",
    "    mask_indices = sorted(sample(cand_indices, num_to_mask))\n",
    "    masked_token_labels = []\n",
    "    for index in mask_indices:\n",
    "        # 80% of the time, replace with [MASK]\n",
    "        if random() < 0.8:\n",
    "            masked_token = \"[MASK]\"\n",
    "        else:\n",
    "            # 10% of the time, keep original\n",
    "            if random() < 0.5:\n",
    "                masked_token = tokens[index]\n",
    "            # 10% of the time, replace with random word\n",
    "            else:\n",
    "                masked_token = choice(vocab_list)\n",
    "        masked_token_labels.append(tokens[index])\n",
    "        # Once we've saved the true label for that token, we can overwrite it with the masked version\n",
    "        tokens[index] = masked_token\n",
    "\n",
    "    return tokens, mask_indices, masked_token_labels\n",
    "\n",
    "\n",
    "def create_instances_from_document(doc_database, doc_idx, max_seq_length, short_seq_prob,\n",
    "        masked_lm_prob, max_predictions_per_seq, vocab_list):\n",
    "    \"\"\"This code is mostly a duplicate of the equivalent function from Google BERT's repo.\n",
    "    However, we make some changes and improvements. Sampling is improved and no longer requires a loop in this function.\n",
    "    Also, documents are sampled proportionally to the number of sentences they contain, which means each sentence\n",
    "    (rather than each document) has an equal chance of being sampled as a false example for the NextSentence task.\"\"\"\n",
    "    document = doc_database[doc_idx]\n",
    "    # Account for [CLS], [SEP], [SEP]\n",
    "    max_num_tokens = max_seq_length - 3\n",
    "\n",
    "    # We *usually* want to fill up the entire sequence since we are padding\n",
    "    # to `max_seq_length` anyways, so short sequences are generally wasted\n",
    "    # computation. However, we *sometimes*\n",
    "    # (i.e., short_seq_prob == 0.1 == 10% of the time) want to use shorter\n",
    "    # sequences to minimize the mismatch between pre-training and fine-tuning.\n",
    "    # The `target_seq_length` is just a rough target however, whereas\n",
    "    # `max_seq_length` is a hard limit.\n",
    "    target_seq_length = max_num_tokens\n",
    "    if random() < short_seq_prob:\n",
    "        target_seq_length = randint(2, max_num_tokens)\n",
    "\n",
    "    # We DON'T just concatenate all of the tokens from a document into a long\n",
    "    # sequence and choose an arbitrary split point because this would make the\n",
    "    # next sentence prediction task too easy. Instead, we split the input into\n",
    "    # segments \"A\" and \"B\" based on the actual \"sentences\" provided by the user\n",
    "    # input.\n",
    "    instances = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    i = 0\n",
    "    while i < len(document):\n",
    "        segment = document[i]\n",
    "        current_chunk.append(segment)\n",
    "        current_length += len(segment)\n",
    "        if i == len(document) - 1 or current_length >= target_seq_length:\n",
    "            if current_chunk:\n",
    "                # `a_end` is how many segments from `current_chunk` go into the `A`\n",
    "                # (first) sentence.\n",
    "                a_end = 1\n",
    "                if len(current_chunk) >= 2:\n",
    "                    a_end = randrange(1, len(current_chunk))\n",
    "\n",
    "                tokens_a = []\n",
    "                for j in range(a_end):\n",
    "                    tokens_a.extend(current_chunk[j])\n",
    "\n",
    "                tokens_b = []\n",
    "\n",
    "                # Random next\n",
    "                if len(current_chunk) == 1 or random() < 0.5:\n",
    "                    is_random_next = True\n",
    "                    target_b_length = target_seq_length - len(tokens_a)\n",
    "\n",
    "                    # Sample a random document, with longer docs being sampled more frequently\n",
    "                    random_document = doc_database.sample_doc(current_idx=doc_idx, sentence_weighted=True)\n",
    "\n",
    "                    random_start = randrange(0, len(random_document))\n",
    "                    for j in range(random_start, len(random_document)):\n",
    "                        tokens_b.extend(random_document[j])\n",
    "                        if len(tokens_b) >= target_b_length:\n",
    "                            break\n",
    "                    # We didn't actually use these segments so we \"put them back\" so\n",
    "                    # they don't go to waste.\n",
    "                    num_unused_segments = len(current_chunk) - a_end\n",
    "                    i -= num_unused_segments\n",
    "                # Actual next\n",
    "                else:\n",
    "                    is_random_next = False\n",
    "                    for j in range(a_end, len(current_chunk)):\n",
    "                        tokens_b.extend(current_chunk[j])\n",
    "                truncate_seq_pair(tokens_a, tokens_b, max_num_tokens)\n",
    "\n",
    "                #assert len(tokens_a) >= 1\n",
    "                #assert len(tokens_b) >= 1\n",
    "                if (len(tokens_a) < 1 or len(tokens_b) < 1):\n",
    "                    print(tokens_a)\n",
    "                    print(tokens_b)\n",
    "                else:\n",
    "                    tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "                    # The segment IDs are 0 for the [CLS] token, the A tokens and the first [SEP]\n",
    "                    # They are 1 for the B tokens and the final [SEP]\n",
    "                    segment_ids = [0 for _ in range(len(tokens_a) + 2)] + [1 for _ in range(len(tokens_b) + 1)]\n",
    "\n",
    "                    tokens, masked_lm_positions, masked_lm_labels = create_masked_lm_predictions(\n",
    "                        tokens, masked_lm_prob, max_predictions_per_seq, vocab_list)\n",
    "\n",
    "                    instance = {\n",
    "                        \"tokens\": tokens,\n",
    "                        \"segment_ids\": segment_ids,\n",
    "                        \"is_random_next\": is_random_next,\n",
    "                        \"masked_lm_positions\": masked_lm_positions,\n",
    "                        \"masked_lm_labels\": masked_lm_labels}\n",
    "                    instances.append(instance)\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "        i += 1\n",
    "\n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pregenerateData():\n",
    "    args = readConfig()\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n",
    "    vocab_list = list(tokenizer.vocab.keys())\n",
    "    with DocumentDatabase(reduce_memory=args.reduce_memory) as docs:\n",
    "        with args.train_corpus.open() as f:\n",
    "            doc = []\n",
    "            for line in tqdm(f, desc=\"Loading Dataset\", unit=\" lines\"):\n",
    "                line = line.strip()\n",
    "                if line == \"\":\n",
    "                    docs.add_document(doc)\n",
    "                    doc = []\n",
    "                else:\n",
    "                    tokens = tokenizer.tokenize(line)\n",
    "                    doc.append(tokens)\n",
    "            if doc:\n",
    "                docs.add_document(doc)  # If the last doc didn't end on a newline, make sure it still gets added\n",
    "        if len(docs) <= 1:\n",
    "            exit(\"ERROR: No document breaks were found in the input file! These are necessary to allow the script to \"\n",
    "                 \"ensure that random NextSentences are not sampled from the same document. Please add blank lines to \"\n",
    "                 \"indicate breaks between documents in your input file. If your dataset does not contain multiple \"\n",
    "                 \"documents, blank lines can be inserted at any natural boundary, such as the ends of chapters, \"\n",
    "                 \"sections or paragraphs.\")\n",
    "\n",
    "        args.output_dir.mkdir(exist_ok=True)\n",
    "        for epoch in trange(args.epochs_to_generate, desc=\"Epoch\"):\n",
    "            epoch_filename = args.output_dir / f\"epoch_{epoch}.json\"\n",
    "            num_instances = 0\n",
    "            with epoch_filename.open('w') as epoch_file:\n",
    "                for doc_idx in trange(len(docs), desc=\"Document\"):\n",
    "                    doc_instances = create_instances_from_document(\n",
    "                        docs, doc_idx, max_seq_length=args.max_seq_len, short_seq_prob=args.short_seq_prob,\n",
    "                        masked_lm_prob=args.masked_lm_prob, max_predictions_per_seq=args.max_predictions_per_seq,\n",
    "                        vocab_list=vocab_list)\n",
    "                    doc_instances = [json.dumps(instance) for instance in doc_instances]\n",
    "                    for instance in doc_instances:\n",
    "                        epoch_file.write(instance + '\\n')\n",
    "                        num_instances += 1\n",
    "            metrics_file = args.output_dir / f\"epoch_{epoch}_metrics.json\"\n",
    "            with metrics_file.open('w') as metrics_file:\n",
    "                metrics = {\n",
    "                    \"num_training_examples\": num_instances,\n",
    "                    \"max_seq_len\": args.max_seq_len\n",
    "                }\n",
    "                metrics_file.write(json.dumps(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readConfig():\n",
    "    args = Namespace()\n",
    "    args.train_corpus = Path('../../data/OrganicDataset/Unannotated/allRelevantCommentsUnannotatedEnglish_V02.txt')\n",
    "    args.bert_model = 'bert-base-uncased'\n",
    "    args.output_dir = Path('./pretrained/Current/Data/')\n",
    "    args.do_lower_case = True\n",
    "    args.epochs_to_generate = 3\n",
    "    args.max_seq_len = 128\n",
    "    args.reduce_memory = False\n",
    "    args.short_seq_prob = 0.1\n",
    "    args.masked_lm_prob = 0.15\n",
    "    args.max_predictions_per_seq = 20\n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset: 203196 lines [01:31, 2216.98 lines/s]\n",
      "Epoch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Document:   0%|          | 0/3578 [00:00<?, ?it/s]\u001b[A\n",
      "Document:   1%|          | 38/3578 [00:00<00:09, 378.25it/s]\u001b[A\n",
      "Document:   2%|▏         | 81/3578 [00:00<00:08, 392.39it/s]\u001b[A\n",
      "Document:   3%|▎         | 113/3578 [00:00<00:09, 366.00it/s]\u001b[A\n",
      "Document:   4%|▍         | 138/3578 [00:00<00:12, 285.57it/s]\u001b[A\n",
      "Document:   4%|▍         | 161/3578 [00:00<00:14, 228.83it/s]\u001b[A\n",
      "Document:   5%|▌         | 182/3578 [00:00<00:17, 189.04it/s]\u001b[A\n",
      "Document:   6%|▌         | 201/3578 [00:00<00:19, 177.11it/s]\u001b[A\n",
      "Document:   6%|▌         | 219/3578 [00:00<00:20, 167.87it/s]\u001b[A\n",
      "Document:   7%|▋         | 239/3578 [00:01<00:18, 176.05it/s]\u001b[A\n",
      "Document:   8%|▊         | 270/3578 [00:01<00:16, 201.49it/s]\u001b[A\n",
      "Document:   8%|▊         | 292/3578 [00:01<00:16, 203.76it/s]\u001b[A\n",
      "Document:  10%|█         | 373/3578 [00:01<00:12, 262.47it/s]\u001b[A\n",
      "Document:  12%|█▏        | 427/3578 [00:01<00:10, 309.59it/s]\u001b[A\n",
      "Document:  15%|█▍        | 530/3578 [00:01<00:07, 391.12it/s]\u001b[A\n",
      "Document:  17%|█▋        | 591/3578 [00:01<00:07, 402.89it/s]\u001b[A\n",
      "Document:  19%|█▉        | 697/3578 [00:01<00:05, 493.97it/s]\u001b[A\n",
      "Document:  22%|██▏       | 778/3578 [00:01<00:05, 557.62it/s]\u001b[A\n",
      "Document:  24%|██▍       | 852/3578 [00:02<00:05, 514.42it/s]\u001b[A\n",
      "Document:  26%|██▌       | 917/3578 [00:02<00:07, 375.28it/s]\u001b[A\n",
      "Document:  27%|██▋       | 969/3578 [00:02<00:08, 305.37it/s]\u001b[A\n",
      "Document:  28%|██▊       | 1012/3578 [00:02<00:09, 262.04it/s]\u001b[A\n",
      "Document:  29%|██▉       | 1048/3578 [00:03<00:12, 204.99it/s]\u001b[A\n",
      "Document:  30%|███       | 1078/3578 [00:03<00:14, 177.36it/s]\u001b[A\n",
      "Document:  31%|███       | 1103/3578 [00:03<00:15, 160.26it/s]\u001b[A\n",
      "Document:  31%|███▏      | 1125/3578 [00:03<00:14, 168.27it/s]\u001b[A\n",
      "Document:  32%|███▏      | 1146/3578 [00:03<00:13, 173.97it/s]\u001b[A\n",
      "Document:  33%|███▎      | 1184/3578 [00:03<00:11, 207.69it/s]\u001b[A\n",
      "Document:  34%|███▍      | 1219/3578 [00:03<00:09, 236.45it/s]\u001b[A\n",
      "Document:  35%|███▌      | 1262/3578 [00:04<00:08, 272.58it/s]\u001b[A\n",
      "Document:  37%|███▋      | 1309/3578 [00:04<00:07, 300.73it/s]\u001b[A\n",
      "Document:  38%|███▊      | 1350/3578 [00:04<00:06, 326.79it/s]\u001b[A\n",
      "Document:  39%|███▉      | 1389/3578 [00:04<00:06, 342.31it/s]\u001b[A\n",
      "Document:  40%|███▉      | 1427/3578 [00:04<00:08, 257.69it/s]\u001b[A\n",
      "Document:  41%|████      | 1458/3578 [00:05<00:14, 147.32it/s]\u001b[A\n",
      "Document:  42%|████▏     | 1506/3578 [00:05<00:11, 185.74it/s]\u001b[A\n",
      "Document:  44%|████▎     | 1562/3578 [00:05<00:08, 232.29it/s]\u001b[A\n",
      "Document:  45%|████▌     | 1626/3578 [00:05<00:06, 287.12it/s]\u001b[A\n",
      "Document:  47%|████▋     | 1690/3578 [00:05<00:05, 343.14it/s]\u001b[A\n",
      "Document:  49%|████▉     | 1764/3578 [00:05<00:04, 407.60it/s]\u001b[A\n",
      "Document:  52%|█████▏    | 1866/3578 [00:05<00:03, 494.59it/s]\u001b[A\n",
      "Document:  54%|█████▍    | 1936/3578 [00:05<00:03, 539.02it/s]\u001b[A\n",
      "Document:  56%|█████▋    | 2017/3578 [00:05<00:02, 572.48it/s]\u001b[A\n",
      "Document:  58%|█████▊    | 2092/3578 [00:05<00:02, 613.58it/s]\u001b[A\n",
      "Document:  60%|██████    | 2163/3578 [00:06<00:02, 611.07it/s]\u001b[A\n",
      "Document:  62%|██████▏   | 2232/3578 [00:06<00:02, 631.94it/s]\u001b[A\n",
      "Document:  64%|██████▍   | 2306/3578 [00:06<00:01, 659.22it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['i', \"'\", 've', 'been', 'an', 'rei', 'member', 'for', '35', 'y', '##rs', '.', ',', 'always', 'an', 'excellent', 'experience', 'with', 'customer', 'service', ',', 'etc', '.', 'but', 'i', 'saw', 'the', 'new', 'logo', 'on', 'items', 'in', 'store', 'yesterday', 'and', 'it', 'doesn', \"'\", 't', 'wow', 'me', '.', '.', '.', 'too', 'big', ',', 'not', 'discrete', '.', 'kind', 'of', 'a', 'negative', ',', 'aesthetic', '##ally', 'speaking', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Document:  67%|██████▋   | 2385/3578 [00:06<00:01, 692.55it/s]\u001b[A\n",
      "Document:  69%|██████▉   | 2470/3578 [00:06<00:01, 730.22it/s]\u001b[A\n",
      "Document:  71%|███████   | 2546/3578 [00:06<00:01, 684.31it/s]\u001b[A\n",
      "Document:  74%|███████▍  | 2639/3578 [00:06<00:01, 739.83it/s]\u001b[A\n",
      "Document:  76%|███████▌  | 2724/3578 [00:06<00:01, 766.57it/s]\u001b[A\n",
      "Document:  78%|███████▊  | 2803/3578 [00:06<00:01, 699.12it/s]\u001b[A\n",
      "Document:  80%|████████  | 2876/3578 [00:07<00:03, 205.70it/s]\u001b[A\n",
      "Document:  82%|████████▏ | 2930/3578 [00:08<00:03, 191.48it/s]\u001b[A\n",
      "Document:  83%|████████▎ | 2973/3578 [00:08<00:02, 223.80it/s]\u001b[A\n",
      "Document:  84%|████████▍ | 3014/3578 [00:08<00:02, 216.11it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'link', 'to', 'the', 'study', ']', '(', 'i', 'think', 'that', 'the', 'authors', 'of', 'this', 'paper', 'just', '*', 'might', '*', 'know', 'a', 'thing', 'or', 'two', 'about', 'the', 'subject', 'they', \"'\", 're', 'talking', 'about', '.', 'also', ',', 'they', 'provide', 'their', 'data', '.', 'feel', 'free', 'to', 'argue', 'against', 'the', 'data', 'at', 'any', 'time', '-', 'but', 'please', 'just', 'stop', 'dismissing', 'it', 'out', 'of', 'hand', 'because', 'of', 'your', 'own', 'personal', 'bias', '.']\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Document:  85%|████████▌ | 3049/3578 [00:08<00:02, 207.27it/s]\u001b[A\n",
      "Document:  86%|████████▌ | 3079/3578 [00:08<00:02, 217.82it/s]\u001b[A\n",
      "Document:  87%|████████▋ | 3108/3578 [00:09<00:02, 181.41it/s]\u001b[A\n",
      "Document:  88%|████████▊ | 3154/3578 [00:09<00:01, 221.67it/s]\u001b[A\n",
      "Document:  89%|████████▉ | 3185/3578 [00:09<00:01, 223.94it/s]\u001b[A\n",
      "Document:  90%|████████▉ | 3214/3578 [00:09<00:02, 150.42it/s]\u001b[A\n",
      "Document:  90%|█████████ | 3237/3578 [00:10<00:03, 96.35it/s] \u001b[A\n",
      "Document:  91%|█████████ | 3255/3578 [00:10<00:04, 71.93it/s]\u001b[A\n",
      "Document:  91%|█████████▏| 3269/3578 [00:10<00:04, 63.18it/s]\u001b[A\n",
      "Document:  92%|█████████▏| 3280/3578 [00:11<00:05, 50.93it/s]\u001b[A\n",
      "Document:  92%|█████████▏| 3293/3578 [00:11<00:04, 59.45it/s]\u001b[A\n",
      "Document:  92%|█████████▏| 3305/3578 [00:11<00:04, 67.48it/s]\u001b[A\n",
      "Document:  93%|█████████▎| 3321/3578 [00:11<00:03, 80.95it/s]\u001b[A\n",
      "Document:  93%|█████████▎| 3340/3578 [00:11<00:02, 97.26it/s]\u001b[A\n",
      "Document:  94%|█████████▍| 3362/3578 [00:11<00:01, 113.34it/s]\u001b[A\n",
      "Document:  94%|█████████▍| 3377/3578 [00:11<00:01, 120.58it/s]\u001b[A\n",
      "Document:  95%|█████████▌| 3405/3578 [00:11<00:01, 144.86it/s]\u001b[A\n",
      "Document:  96%|█████████▌| 3424/3578 [00:11<00:01, 150.05it/s]\u001b[A\n",
      "Document:  96%|█████████▌| 3443/3578 [00:12<00:00, 155.82it/s]\u001b[A\n",
      "Document:  97%|█████████▋| 3469/3578 [00:12<00:00, 177.00it/s]\u001b[A\n",
      "Document:  98%|█████████▊| 3505/3578 [00:12<00:00, 190.07it/s]\u001b[A\n",
      "Document:  99%|█████████▊| 3532/3578 [00:12<00:00, 207.61it/s]\u001b[A\n",
      "Document:  99%|█████████▉| 3560/3578 [00:12<00:00, 219.50it/s]\u001b[A\n",
      "Epoch:  33%|███▎      | 1/3 [00:12<00:25, 12.60s/it]84.07it/s]\u001b[A\n",
      "Document:   0%|          | 0/3578 [00:00<?, ?it/s]\u001b[A\n",
      "Document:   1%|          | 33/3578 [00:00<00:10, 324.65it/s]\u001b[A\n",
      "Document:   2%|▏         | 80/3578 [00:00<00:09, 356.89it/s]\u001b[A\n",
      "Document:   3%|▎         | 115/3578 [00:00<00:09, 351.79it/s]\u001b[A\n",
      "Document:   4%|▍         | 140/3578 [00:00<00:12, 281.89it/s]\u001b[A\n",
      "Document:   5%|▍         | 164/3578 [00:00<00:14, 233.37it/s]\u001b[A\n",
      "Document:   5%|▌         | 186/3578 [00:00<00:17, 195.61it/s]\u001b[A\n",
      "Document:   6%|▌         | 206/3578 [00:00<00:18, 182.76it/s]\u001b[A\n",
      "Document:   6%|▋         | 225/3578 [00:00<00:18, 177.59it/s]\u001b[A\n",
      "Document:   7%|▋         | 248/3578 [00:01<00:17, 187.56it/s]\u001b[A\n",
      "Document:   8%|▊         | 282/3578 [00:01<00:15, 215.31it/s]\u001b[A\n",
      "Document:   9%|▊         | 306/3578 [00:01<00:14, 220.75it/s]\u001b[A\n",
      "Document:  11%|█▏        | 411/3578 [00:01<00:10, 289.21it/s]\u001b[A\n",
      "Document:  14%|█▎        | 487/3578 [00:01<00:08, 355.22it/s]\u001b[A\n",
      "Document:  15%|█▌        | 553/3578 [00:01<00:07, 412.05it/s]\u001b[A\n",
      "Document:  17%|█▋        | 622/3578 [00:01<00:06, 468.05it/s]\u001b[A\n",
      "Document:  20%|██        | 727/3578 [00:01<00:05, 561.22it/s]\u001b[A\n",
      "Document:  22%|██▏       | 802/3578 [00:01<00:04, 578.19it/s]\u001b[A\n",
      "Document:  24%|██▍       | 873/3578 [00:02<00:05, 465.69it/s]\u001b[A\n",
      "Document:  26%|██▌       | 933/3578 [00:02<00:07, 361.93it/s]\u001b[A\n",
      "Document:  27%|██▋       | 982/3578 [00:02<00:08, 299.56it/s]\u001b[A\n",
      "Document:  29%|██▊       | 1023/3578 [00:02<00:10, 251.49it/s]\u001b[A\n",
      "Document:  30%|██▉       | 1057/3578 [00:03<00:12, 197.89it/s]\u001b[A\n",
      "Document:  30%|███       | 1085/3578 [00:03<00:14, 169.36it/s]\u001b[A\n",
      "Document:  31%|███       | 1109/3578 [00:03<00:15, 162.80it/s]\u001b[A\n",
      "Document:  32%|███▏      | 1130/3578 [00:03<00:14, 163.44it/s]\u001b[A\n",
      "Document:  33%|███▎      | 1164/3578 [00:03<00:12, 192.11it/s]\u001b[A\n",
      "Document:  34%|███▎      | 1199/3578 [00:03<00:10, 221.68it/s]\u001b[A\n",
      "Document:  35%|███▍      | 1240/3578 [00:03<00:09, 257.08it/s]\u001b[A\n",
      "Document:  36%|███▋      | 1300/3578 [00:04<00:07, 308.77it/s]\u001b[A\n",
      "Document:  37%|███▋      | 1340/3578 [00:04<00:07, 317.06it/s]\u001b[A\n",
      "Document:  39%|███▊      | 1378/3578 [00:04<00:06, 320.53it/s]\u001b[A\n",
      "Document:  40%|███▉      | 1418/3578 [00:04<00:06, 332.02it/s]\u001b[A\n",
      "Document:  41%|████      | 1455/3578 [00:04<00:13, 153.95it/s]\u001b[A\n",
      "Document:  42%|████▏     | 1503/3578 [00:04<00:10, 193.30it/s]\u001b[A\n",
      "Document:  44%|████▎     | 1560/3578 [00:05<00:08, 240.31it/s]\u001b[A\n",
      "Document:  46%|████▌     | 1632/3578 [00:05<00:06, 299.89it/s]\u001b[A\n",
      "Document:  47%|████▋     | 1693/3578 [00:05<00:05, 353.43it/s]\u001b[A\n",
      "Document:  49%|████▉     | 1764/3578 [00:05<00:04, 415.54it/s]\u001b[A\n",
      "Document:  52%|█████▏    | 1867/3578 [00:05<00:03, 505.91it/s]\u001b[A\n",
      "Document:  54%|█████▍    | 1946/3578 [00:05<00:02, 567.07it/s]\u001b[A\n",
      "Document:  57%|█████▋    | 2047/3578 [00:05<00:02, 648.19it/s]\u001b[A\n",
      "Document:  59%|█████▉    | 2128/3578 [00:05<00:02, 670.83it/s]\u001b[A\n",
      "Document:  62%|██████▏   | 2210/3578 [00:05<00:01, 705.51it/s]\u001b[A\n",
      "Document:  64%|██████▍   | 2289/3578 [00:06<00:01, 697.51it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['pasture', '-', 'raised', 'milk', 'and', 'eggs', ',', 'grass', '-', 'fed', 'butter', 'and', 'beef', ',', 'and', 'raw', 'milk', 'cheese', '##s', ',', 'my', 'health', 'has', 'improved', '.', 'my', 'oral', 'health', 'is', 'better', 'and', 'my', 'eyes', '##ight', 'improved', 'in', 'one', 'year', '.', 'i', 'didn', \"'\", 't', 'even', 'know', 'eyes', '##ight', 'could', 'improve', 'before', 'then', '.', 'so', 'stanford', 'can', \"'\", 't', 'tell', 'me', 'nothin', \"'\", 'about', 'organic', 'food', '!', '!', '!', 'who', 'paid', 'for', 'this', 'study', '?', 'total', 'mis', '##in', '##form', '##ation', '!', 'i', 'could', 'not', 'believe', 'it', 'when', 'i', 'heard', 'this', 'on', 'cnn', '.', 'but', 'then', 'again', ',', 'it', 'is', 'right', 'up', 'their', 'alley', '.', 'so', ',', 'here', 'are', 'the', 'results', 'from', 'my', 'own', 'study', 'that', 'i', 'conducted', ':', 'don', '##t', 'drink', 'the', 'ko', '##ol', 'aid', '.', 'support']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Document:  66%|██████▋   | 2373/3578 [00:06<00:01, 734.40it/s]\u001b[A\n",
      "Document:  69%|██████▉   | 2468/3578 [00:06<00:01, 788.03it/s]\u001b[A\n",
      "Document:  71%|███████▏  | 2552/3578 [00:06<00:01, 732.59it/s]\u001b[A\n",
      "Document:  74%|███████▎  | 2638/3578 [00:06<00:01, 762.07it/s]\u001b[A\n",
      "Document:  76%|███████▋  | 2730/3578 [00:06<00:01, 803.37it/s]\u001b[A\n",
      "Document:  79%|███████▊  | 2814/3578 [00:06<00:01, 650.99it/s]\u001b[A\n",
      "Document:  81%|████████  | 2886/3578 [00:07<00:03, 182.41it/s]\u001b[A\n",
      "Document:  82%|████████▏ | 2939/3578 [00:07<00:02, 214.89it/s]\u001b[A\n",
      "Document:  83%|████████▎ | 2987/3578 [00:08<00:02, 251.18it/s]\u001b[A\n",
      "Document:  85%|████████▍ | 3033/3578 [00:08<00:02, 237.26it/s]\u001b[A\n",
      "Document:  86%|████████▌ | 3072/3578 [00:08<00:02, 219.82it/s]\u001b[A\n",
      "Document:  87%|████████▋ | 3105/3578 [00:08<00:02, 194.12it/s]\u001b[A\n",
      "Document:  88%|████████▊ | 3143/3578 [00:08<00:01, 227.50it/s]\u001b[A\n",
      "Document:  89%|████████▉ | 3182/3578 [00:08<00:01, 255.48it/s]\u001b[A\n",
      "Document:  90%|████████▉ | 3215/3578 [00:09<00:02, 149.57it/s]\u001b[A\n",
      "Document:  91%|█████████ | 3240/3578 [00:09<00:03, 86.88it/s] \u001b[A\n",
      "Document:  91%|█████████ | 3259/3578 [00:10<00:04, 77.20it/s]\u001b[A\n",
      "Document:  92%|█████████▏| 3274/3578 [00:10<00:05, 51.22it/s]\u001b[A\n",
      "Document:  92%|█████████▏| 3292/3578 [00:10<00:04, 63.94it/s]\u001b[A\n",
      "Document:  92%|█████████▏| 3305/3578 [00:11<00:04, 67.23it/s]\u001b[A\n",
      "Document:  93%|█████████▎| 3318/3578 [00:11<00:03, 78.27it/s]\u001b[A\n",
      "Document:  93%|█████████▎| 3337/3578 [00:11<00:02, 94.13it/s]\u001b[A\n",
      "Document:  94%|█████████▍| 3361/3578 [00:11<00:01, 113.29it/s]\u001b[A\n",
      "Document:  94%|█████████▍| 3377/3578 [00:11<00:01, 115.85it/s]\u001b[A\n",
      "Document:  95%|█████████▌| 3405/3578 [00:11<00:01, 139.98it/s]\u001b[A\n",
      "Document:  96%|█████████▌| 3424/3578 [00:11<00:01, 149.03it/s]\u001b[A\n",
      "Document:  96%|█████████▌| 3443/3578 [00:11<00:00, 151.56it/s]\u001b[A\n",
      "Document:  97%|█████████▋| 3469/3578 [00:11<00:00, 173.14it/s]\u001b[A\n",
      "Document:  98%|█████████▊| 3505/3578 [00:12<00:00, 188.42it/s]\u001b[A\n",
      "Document:  99%|█████████▊| 3533/3578 [00:12<00:00, 205.06it/s]\u001b[A\n",
      "Document: 100%|█████████▉| 3568/3578 [00:12<00:00, 233.73it/s]\u001b[A\n",
      "Epoch:  67%|██████▋   | 2/3 [00:24<00:12, 12.51s/it]90.62it/s]\u001b[A\n",
      "Document:   0%|          | 0/3578 [00:00<?, ?it/s]\u001b[A\n",
      "Document:   1%|          | 38/3578 [00:00<00:09, 364.22it/s]\u001b[A\n",
      "Document:   2%|▏         | 85/3578 [00:00<00:08, 390.33it/s]\u001b[A\n",
      "Document:   3%|▎         | 118/3578 [00:00<00:09, 368.90it/s]\u001b[A\n",
      "Document:   4%|▍         | 144/3578 [00:00<00:12, 286.10it/s]\u001b[A\n",
      "Document:   5%|▍         | 168/3578 [00:00<00:14, 232.12it/s]\u001b[A\n",
      "Document:   5%|▌         | 190/3578 [00:00<00:16, 201.20it/s]\u001b[A\n",
      "Document:   6%|▌         | 210/3578 [00:00<00:17, 194.06it/s]\u001b[A\n",
      "Document:   6%|▋         | 229/3578 [00:00<00:18, 185.59it/s]\u001b[A\n",
      "Document:   7%|▋         | 257/3578 [00:01<00:16, 206.39it/s]\u001b[A\n",
      "Document:   8%|▊         | 283/3578 [00:01<00:15, 215.64it/s]\u001b[A\n",
      "Document:   9%|▉         | 327/3578 [00:01<00:12, 254.36it/s]\u001b[A\n",
      "Document:  12%|█▏        | 421/3578 [00:01<00:09, 325.12it/s]\u001b[A\n",
      "Document:  14%|█▍        | 511/3578 [00:01<00:07, 402.18it/s]\u001b[A\n",
      "Document:  16%|█▌        | 572/3578 [00:01<00:07, 416.62it/s]\u001b[A\n",
      "Document:  19%|█▉        | 671/3578 [00:01<00:05, 502.05it/s]\u001b[A\n",
      "Document:  22%|██▏       | 776/3578 [00:01<00:04, 583.63it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arguing', 'with', 'one', 'another', 'is', 'a', 'waste', 'of', 'good', 'positive', 'energy', 'since', 'we', 'all', 'need', 'to', 'actively', 'continue', 'blowing', 'the', 'whistle', 'on', 'outright', 'crimes', 'happening', 'every', 'day', 'in', 'our', 'food', 'system', '.', 'think', 'of', 'how', 'many', 'posts', 'you', 'could', 'have', 'sent', 'to', 'your', 'friends', 'educating', 'them', 'on', 'how', 'to', 'make', 'a', 'difference', '.']\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Document:  24%|██▍       | 852/3578 [00:02<00:05, 486.55it/s]\u001b[A\n",
      "Document:  26%|██▌       | 916/3578 [00:02<00:07, 372.93it/s]\u001b[A\n",
      "Document:  27%|██▋       | 968/3578 [00:02<00:08, 310.40it/s]\u001b[A\n",
      "Document:  28%|██▊       | 1011/3578 [00:02<00:09, 270.53it/s]\u001b[A\n",
      "Document:  29%|██▉       | 1048/3578 [00:03<00:12, 197.81it/s]\u001b[A\n",
      "Document:  30%|███       | 1078/3578 [00:03<00:14, 175.80it/s]\u001b[A\n",
      "Document:  31%|███       | 1103/3578 [00:03<00:15, 159.78it/s]\u001b[A\n",
      "Document:  31%|███▏      | 1125/3578 [00:03<00:14, 170.65it/s]\u001b[A\n",
      "Document:  32%|███▏      | 1146/3578 [00:03<00:13, 178.30it/s]\u001b[A\n",
      "Document:  33%|███▎      | 1186/3578 [00:03<00:11, 212.46it/s]\u001b[A\n",
      "Document:  34%|███▍      | 1224/3578 [00:03<00:09, 244.35it/s]\u001b[A\n",
      "Document:  35%|███▌      | 1265/3578 [00:03<00:08, 275.72it/s]\u001b[A\n",
      "Document:  37%|███▋      | 1310/3578 [00:04<00:07, 311.58it/s]\u001b[A\n",
      "Document:  38%|███▊      | 1348/3578 [00:04<00:06, 327.56it/s]\u001b[A\n",
      "Document:  39%|███▊      | 1385/3578 [00:04<00:06, 335.65it/s]\u001b[A\n",
      "Document:  40%|███▉      | 1422/3578 [00:04<00:07, 298.47it/s]\u001b[A\n",
      "Document:  41%|████      | 1455/3578 [00:04<00:15, 140.90it/s]\u001b[A\n",
      "Document:  42%|████▏     | 1504/3578 [00:05<00:11, 179.16it/s]\u001b[A\n",
      "Document:  44%|████▎     | 1559/3578 [00:05<00:08, 224.53it/s]\u001b[A\n",
      "Document:  45%|████▌     | 1627/3578 [00:05<00:06, 280.78it/s]\u001b[A\n",
      "Document:  47%|████▋     | 1692/3578 [00:05<00:05, 338.34it/s]\u001b[A\n",
      "Document:  49%|████▉     | 1764/3578 [00:05<00:04, 399.06it/s]\u001b[A\n",
      "Document:  52%|█████▏    | 1869/3578 [00:05<00:03, 489.40it/s]\u001b[A\n",
      "Document:  54%|█████▍    | 1944/3578 [00:05<00:02, 546.00it/s]\u001b[A\n",
      "Document:  57%|█████▋    | 2037/3578 [00:05<00:02, 622.84it/s]\u001b[A\n",
      "Document:  59%|█████▉    | 2116/3578 [00:05<00:02, 641.35it/s]\u001b[A\n",
      "Document:  62%|██████▏   | 2205/3578 [00:05<00:01, 698.94it/s]\u001b[A\n",
      "Document:  64%|██████▍   | 2285/3578 [00:06<00:01, 677.77it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['are', 'none', '.', 'organic', 'is', 'just', 'a', 'marketing', 'term', '.', 'there', 'are', 'zero', 'nutritional', 'or', 'health', 'advantages', 'to', 'buying', '\"', 'organic', '\"', ',', 'and', 'if', 'the', 'organic', 'choice', 'happens', 'to', 'taste', 'better', 'it', \"'\", 's', 'due', 'to', 'chance', 'and', '/', 'or', 'factors', 'that', 'are', 'tangent', '##ial', 'to', 'it', 'being', 'organic', '.', 'just', 'as', 'denise', 'fu', '##ku', '##da', ',', 'i', 'am', 'surprised', 'at', 'the', 'assumption', 'that', 'the', 'benefit', 'of', 'organic', 'food', 'is', 'that', 'it', 'is', 'more', 'healthy', 'or', 'taste', 'better', '.', 'this', 'is', 'not', 'so', '.', 'the', 'benefit', 'is', 'environmental', '.', 'all', 'those', 'pest', '##icides', 'that', 'non', '-', 'organic', 'food', 'get', 'sprayed', 'with', ',', 'where', 'do', 'they', 'go', '?', 'the', 'less', 'our', 'food', 'gets', 'sprayed', 'with', 'that', 'stuff', ',', 'the', 'better', 'for', 'the', 'environment', 'and', 'our']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Document:  66%|██████▌   | 2365/3578 [00:06<00:01, 706.53it/s]\u001b[A\n",
      "Document:  69%|██████▉   | 2466/3578 [00:06<00:01, 776.10it/s]\u001b[A\n",
      "Document:  71%|███████▏  | 2550/3578 [00:06<00:01, 708.98it/s]\u001b[A\n",
      "Document:  74%|███████▎  | 2638/3578 [00:06<00:01, 747.21it/s]\u001b[A\n",
      "Document:  76%|███████▌  | 2722/3578 [00:06<00:01, 772.62it/s]\u001b[A\n",
      "Document:  78%|███████▊  | 2803/3578 [00:06<00:01, 704.39it/s]\u001b[A\n",
      "Document:  80%|████████  | 2877/3578 [00:07<00:03, 194.55it/s]\u001b[A\n",
      "Document:  82%|████████▏ | 2931/3578 [00:08<00:03, 205.85it/s]\u001b[A\n",
      "Document:  83%|████████▎ | 2976/3578 [00:08<00:02, 237.94it/s]\u001b[A\n",
      "Document:  84%|████████▍ | 3019/3578 [00:08<00:02, 231.03it/s]\u001b[A\n",
      "Document:  85%|████████▌ | 3056/3578 [00:08<00:02, 221.05it/s]\u001b[A\n",
      "Document:  86%|████████▋ | 3088/3578 [00:08<00:02, 208.49it/s]\u001b[A\n",
      "Document:  87%|████████▋ | 3116/3578 [00:08<00:02, 198.97it/s]\u001b[A\n",
      "Document:  89%|████████▊ | 3174/3578 [00:08<00:01, 247.27it/s]\u001b[A\n",
      "Document:  90%|████████▉ | 3209/3578 [00:09<00:02, 164.57it/s]\u001b[A\n",
      "Document:  90%|█████████ | 3236/3578 [00:09<00:03, 103.83it/s]\u001b[A\n",
      "Document:  91%|█████████ | 3257/3578 [00:10<00:04, 77.21it/s] \u001b[A\n",
      "Document:  91%|█████████▏| 3273/3578 [00:10<00:05, 52.53it/s]\u001b[A\n",
      "Document:  92%|█████████▏| 3285/3578 [00:10<00:04, 62.96it/s]\u001b[A\n",
      "Document:  92%|█████████▏| 3297/3578 [00:11<00:03, 71.00it/s]\u001b[A\n",
      "Document:  92%|█████████▏| 3309/3578 [00:11<00:03, 76.74it/s]\u001b[A\n",
      "Document:  93%|█████████▎| 3321/3578 [00:11<00:03, 85.44it/s]\u001b[A\n",
      "Document:  93%|█████████▎| 3339/3578 [00:11<00:02, 100.36it/s]\u001b[A\n",
      "Document:  94%|█████████▍| 3361/3578 [00:11<00:01, 118.47it/s]\u001b[A\n",
      "Document:  94%|█████████▍| 3377/3578 [00:11<00:01, 121.71it/s]\u001b[A\n",
      "Document:  95%|█████████▌| 3405/3578 [00:11<00:01, 146.30it/s]\u001b[A\n",
      "Document:  96%|█████████▌| 3424/3578 [00:11<00:01, 149.33it/s]\u001b[A\n",
      "Document:  96%|█████████▌| 3443/3578 [00:11<00:00, 154.06it/s]\u001b[A\n",
      "Document:  97%|█████████▋| 3472/3578 [00:12<00:00, 179.02it/s]\u001b[A\n",
      "Document:  98%|█████████▊| 3505/3578 [00:12<00:00, 191.06it/s]\u001b[A\n",
      "Document:  99%|█████████▊| 3530/3578 [00:12<00:00, 205.52it/s]\u001b[A\n",
      "Document:  99%|█████████▉| 3560/3578 [00:12<00:00, 222.58it/s]\u001b[A\n",
      "Epoch: 100%|██████████| 3/3 [00:37<00:00, 12.48s/it]88.20it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "pregenerateData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
