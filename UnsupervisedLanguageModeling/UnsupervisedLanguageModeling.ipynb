{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import logging\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from tempfile import TemporaryDirectory\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pytorch_pretrained_bert.modeling import BertForPreTraining\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "InputFeatures = namedtuple(\"InputFeatures\", \"input_ids input_mask segment_ids lm_label_ids is_next\")\n",
    "\n",
    "log_format = '%(asctime)-10s: %(message)s'\n",
    "logging.basicConfig(level=logging.INFO, format=log_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example_to_features(example, tokenizer, max_seq_length):\n",
    "    tokens = example[\"tokens\"]\n",
    "    segment_ids = example[\"segment_ids\"]\n",
    "    is_random_next = example[\"is_random_next\"]\n",
    "    masked_lm_positions = example[\"masked_lm_positions\"]\n",
    "    masked_lm_labels = example[\"masked_lm_labels\"]\n",
    "\n",
    "    assert len(tokens) == len(segment_ids) <= max_seq_length  # The preprocessed data should be already truncated\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    masked_label_ids = tokenizer.convert_tokens_to_ids(masked_lm_labels)\n",
    "\n",
    "    input_array = np.zeros(max_seq_length, dtype=np.int)\n",
    "    input_array[:len(input_ids)] = input_ids\n",
    "\n",
    "    mask_array = np.zeros(max_seq_length, dtype=np.bool)\n",
    "    mask_array[:len(input_ids)] = 1\n",
    "\n",
    "    segment_array = np.zeros(max_seq_length, dtype=np.bool)\n",
    "    segment_array[:len(segment_ids)] = segment_ids\n",
    "\n",
    "    lm_label_array = np.full(max_seq_length, dtype=np.int, fill_value=-1)\n",
    "    lm_label_array[masked_lm_positions] = masked_label_ids\n",
    "\n",
    "    features = InputFeatures(input_ids=input_array,\n",
    "                             input_mask=mask_array,\n",
    "                             segment_ids=segment_array,\n",
    "                             lm_label_ids=lm_label_array,\n",
    "                             is_next=is_random_next)\n",
    "    return features\n",
    "\n",
    "\n",
    "class PregeneratedDataset(Dataset):\n",
    "    def __init__(self, training_path, epoch, tokenizer, num_data_epochs, reduce_memory=False):\n",
    "        self.vocab = tokenizer.vocab\n",
    "        self.tokenizer = tokenizer\n",
    "        self.epoch = epoch\n",
    "        self.data_epoch = epoch % num_data_epochs\n",
    "        data_file = training_path / f\"epoch_{self.data_epoch}.json\"\n",
    "        metrics_file = training_path / f\"epoch_{self.data_epoch}_metrics.json\"\n",
    "        assert data_file.is_file() and metrics_file.is_file()\n",
    "        metrics = json.loads(metrics_file.read_text())\n",
    "        num_samples = metrics['num_training_examples']\n",
    "        seq_len = metrics['max_seq_len']\n",
    "        self.temp_dir = None\n",
    "        self.working_dir = None\n",
    "        if reduce_memory:\n",
    "            self.temp_dir = TemporaryDirectory()\n",
    "            self.working_dir = Path(self.temp_dir.name)\n",
    "            input_ids = np.memmap(filename=self.working_dir/'input_ids.memmap',\n",
    "                                  mode='w+', dtype=np.int32, shape=(num_samples, seq_len))\n",
    "            input_masks = np.memmap(filename=self.working_dir/'input_masks.memmap',\n",
    "                                    shape=(num_samples, seq_len), mode='w+', dtype=np.bool)\n",
    "            segment_ids = np.memmap(filename=self.working_dir/'segment_ids.memmap',\n",
    "                                    shape=(num_samples, seq_len), mode='w+', dtype=np.bool)\n",
    "            lm_label_ids = np.memmap(filename=self.working_dir/'lm_label_ids.memmap',\n",
    "                                     shape=(num_samples, seq_len), mode='w+', dtype=np.int32)\n",
    "            lm_label_ids[:] = -1\n",
    "            is_nexts = np.memmap(filename=self.working_dir/'is_nexts.memmap',\n",
    "                                 shape=(num_samples,), mode='w+', dtype=np.bool)\n",
    "        else:\n",
    "            input_ids = np.zeros(shape=(num_samples, seq_len), dtype=np.int32)\n",
    "            input_masks = np.zeros(shape=(num_samples, seq_len), dtype=np.bool)\n",
    "            segment_ids = np.zeros(shape=(num_samples, seq_len), dtype=np.bool)\n",
    "            lm_label_ids = np.full(shape=(num_samples, seq_len), dtype=np.int32, fill_value=-1)\n",
    "            is_nexts = np.zeros(shape=(num_samples,), dtype=np.bool)\n",
    "        logging.info(f\"Loading training examples for epoch {epoch}\")\n",
    "        with data_file.open() as f:\n",
    "            for i, line in enumerate(tqdm(f, total=num_samples, desc=\"Training examples\")):\n",
    "                line = line.strip()\n",
    "                example = json.loads(line)\n",
    "                features = convert_example_to_features(example, tokenizer, seq_len)\n",
    "                input_ids[i] = features.input_ids\n",
    "                segment_ids[i] = features.segment_ids\n",
    "                input_masks[i] = features.input_mask\n",
    "                lm_label_ids[i] = features.lm_label_ids\n",
    "                is_nexts[i] = features.is_next\n",
    "        assert i == num_samples - 1  # Assert that the sample count metric was true\n",
    "        logging.info(\"Loading complete!\")\n",
    "        self.num_samples = num_samples\n",
    "        self.seq_len = seq_len\n",
    "        self.input_ids = input_ids\n",
    "        self.input_masks = input_masks\n",
    "        self.segment_ids = segment_ids\n",
    "        self.lm_label_ids = lm_label_ids\n",
    "        self.is_nexts = is_nexts\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return (torch.tensor(self.input_ids[item].astype(np.int64)),\n",
    "                torch.tensor(self.input_masks[item].astype(np.int64)),\n",
    "                torch.tensor(self.segment_ids[item].astype(np.int64)),\n",
    "                torch.tensor(self.lm_label_ids[item].astype(np.int64)),\n",
    "                torch.tensor(self.is_nexts[item].astype(np.int64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readConfig():\n",
    "    args = Namespace()\n",
    "    args.pregenerated_data = Path('./pretrained/Current/Data/')\n",
    "    args.bert_model = 'bert-base-uncased'\n",
    "    args.output_dir = Path('./pretrained/Current/Model/')\n",
    "    args.reduce_memory = False\n",
    "    args.do_lower_case = True\n",
    "    args.train_batch_size = 32\n",
    "    args.learning_rate = 3e-5\n",
    "    args.epochs = 1\n",
    "    args.warmup_proportion = 0.1\n",
    "    args.no_cuda = False\n",
    "    args.local_rank = -1\n",
    "    args.seed = 42\n",
    "    args.gradient_accumulation_steps = 1\n",
    "    args.fp16 = False\n",
    "    args.loss_scale = 0\n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsupervisedLM():\n",
    "    args = readConfig()\n",
    "\n",
    "    assert args.pregenerated_data.is_dir(), \\\n",
    "        \"--pregenerated_data should point to the folder of files made by pregenerate_training_data.py!\"\n",
    "\n",
    "    samples_per_epoch = []\n",
    "    for i in range(args.epochs):\n",
    "        epoch_file = args.pregenerated_data / f\"epoch_{i}.json\"\n",
    "        metrics_file = args.pregenerated_data / f\"epoch_{i}_metrics.json\"\n",
    "        if epoch_file.is_file() and metrics_file.is_file():\n",
    "            metrics = json.loads(metrics_file.read_text())\n",
    "            samples_per_epoch.append(metrics['num_training_examples'])\n",
    "        else:\n",
    "            if i == 0:\n",
    "                exit(\"No training data was found!\")\n",
    "            print(f\"Warning! There are fewer epochs of pregenerated data ({i}) than training epochs ({args.epochs}).\")\n",
    "            print(\"This script will loop over the available data, but training diversity may be negatively impacted.\")\n",
    "            num_data_epochs = i\n",
    "            break\n",
    "    else:\n",
    "        num_data_epochs = args.epochs\n",
    "\n",
    "    if args.local_rank == -1 or args.no_cuda:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "        n_gpu = torch.cuda.device_count()\n",
    "    else:\n",
    "        torch.cuda.set_device(args.local_rank)\n",
    "        device = torch.device(\"cuda\", args.local_rank)\n",
    "        n_gpu = 1\n",
    "        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "        torch.distributed.init_process_group(backend='nccl')\n",
    "    logging.info(\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n",
    "        device, n_gpu, bool(args.local_rank != -1), args.fp16))\n",
    "\n",
    "    if args.gradient_accumulation_steps < 1:\n",
    "        raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(\n",
    "                            args.gradient_accumulation_steps))\n",
    "\n",
    "    args.train_batch_size = args.train_batch_size // args.gradient_accumulation_steps\n",
    "\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "    if args.output_dir.is_dir() and list(args.output_dir.iterdir()):\n",
    "        logging.warning(f\"Output directory ({args.output_dir}) already exists and is not empty!\")\n",
    "    args.output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n",
    "\n",
    "    total_train_examples = 0\n",
    "    for i in range(args.epochs):\n",
    "        # The modulo takes into account the fact that we may loop over limited epochs of data\n",
    "        total_train_examples += samples_per_epoch[i % len(samples_per_epoch)]\n",
    "\n",
    "    num_train_optimization_steps = int(\n",
    "        total_train_examples / args.train_batch_size / args.gradient_accumulation_steps)\n",
    "    if args.local_rank != -1:\n",
    "        num_train_optimization_steps = num_train_optimization_steps // torch.distributed.get_world_size()\n",
    "\n",
    "    # Prepare model\n",
    "    model = BertForPreTraining.from_pretrained(args.bert_model)\n",
    "    if args.fp16:\n",
    "        model.half()\n",
    "    model.to(device)\n",
    "    if args.local_rank != -1:\n",
    "        try:\n",
    "            from apex.parallel import DistributedDataParallel as DDP\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
    "        model = DDP(model)\n",
    "    elif n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Prepare optimizer\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex.optimizers import FP16_Optimizer\n",
    "            from apex.optimizers import FusedAdam\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
    "\n",
    "        optimizer = FusedAdam(optimizer_grouped_parameters,\n",
    "                              lr=args.learning_rate,\n",
    "                              bias_correction=False,\n",
    "                              max_grad_norm=1.0)\n",
    "        if args.loss_scale == 0:\n",
    "            optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\n",
    "        else:\n",
    "            optimizer = FP16_Optimizer(optimizer, static_loss_scale=args.loss_scale)\n",
    "        warmup_linear = WarmupLinearSchedule(warmup=args.warmup_proportion,\n",
    "                                             t_total=num_train_optimization_steps)\n",
    "    else:\n",
    "        optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                             lr=args.learning_rate,\n",
    "                             warmup=args.warmup_proportion,\n",
    "                             t_total=num_train_optimization_steps)\n",
    "\n",
    "    global_step = 0\n",
    "    logging.info(\"***** Running training *****\")\n",
    "    logging.info(f\"  Num examples = {total_train_examples}\")\n",
    "    logging.info(\"  Batch size = %d\", args.train_batch_size)\n",
    "    logging.info(\"  Num steps = %d\", num_train_optimization_steps)\n",
    "    model.train()\n",
    "    for epoch in range(args.epochs):\n",
    "        epoch_dataset = PregeneratedDataset(epoch=epoch, training_path=args.pregenerated_data, tokenizer=tokenizer,\n",
    "                                            num_data_epochs=num_data_epochs, reduce_memory=args.reduce_memory)\n",
    "        if args.local_rank == -1:\n",
    "            train_sampler = RandomSampler(epoch_dataset)\n",
    "        else:\n",
    "            train_sampler = DistributedSampler(epoch_dataset)\n",
    "        train_dataloader = DataLoader(epoch_dataset, sampler=train_sampler, batch_size=args.train_batch_size)\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "        with tqdm(total=len(train_dataloader), desc=f\"Epoch {epoch}\") as pbar:\n",
    "            for step, batch in enumerate(train_dataloader):\n",
    "                batch = tuple(t.to(device) for t in batch)\n",
    "                input_ids, input_mask, segment_ids, lm_label_ids, is_next = batch\n",
    "                loss = model(input_ids, segment_ids, input_mask, lm_label_ids, is_next)\n",
    "                if n_gpu > 1:\n",
    "                    loss = loss.mean() # mean() to average on multi-gpu.\n",
    "                if args.gradient_accumulation_steps > 1:\n",
    "                    loss = loss / args.gradient_accumulation_steps\n",
    "                if args.fp16:\n",
    "                    optimizer.backward(loss)\n",
    "                else:\n",
    "                    loss.backward()\n",
    "                tr_loss += loss.item()\n",
    "                nb_tr_examples += input_ids.size(0)\n",
    "                nb_tr_steps += 1\n",
    "                pbar.update(1)\n",
    "                mean_loss = tr_loss * args.gradient_accumulation_steps / nb_tr_steps\n",
    "                pbar.set_postfix_str(f\"Loss: {mean_loss:.5f}\")\n",
    "                if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                    if args.fp16:\n",
    "                        # modify learning rate with special warm up BERT uses\n",
    "                        # if args.fp16 is False, BertAdam is used that handles this automatically\n",
    "                        lr_this_step = args.learning_rate * warmup_linear.get_lr(global_step, args.warmup_proportion)\n",
    "                        for param_group in optimizer.param_groups:\n",
    "                            param_group['lr'] = lr_this_step\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    global_step += 1\n",
    "\n",
    "    # Save a trained model\n",
    "    logging.info(\"** ** * Saving fine-tuned model ** ** * \")\n",
    "    model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "    output_model_file = args.output_dir / \"pytorch_model.bin\"\n",
    "    torch.save(model_to_save.state_dict(), str(output_model_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsupervisedLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
