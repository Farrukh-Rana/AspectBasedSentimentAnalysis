{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE, WEIGHTS_NAME, CONFIG_NAME\n",
    "from pytorch_pretrained_bert.modeling import BertForSequenceClassification, BertConfig\n",
    "from pytorch_pretrained_bert.tokenization import BertTokenizer\n",
    "from pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2018 The Google AI Language Team Authors and The HuggingFace Inc. team.\n",
    "# Copyright (c) 2018, NVIDIA CORPORATION.  All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"BERT finetuning runner.\"\"\"\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "\n",
    "\n",
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\n",
    "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "            lines = []\n",
    "            for line in reader:\n",
    "                if sys.version_info[0] == 2:\n",
    "                    line = list(unicode(cell, 'utf-8') for cell in line)\n",
    "                lines.append(line)\n",
    "            return lines\n",
    "        \n",
    "    @classmethod\n",
    "    def _read_csv(cls, input_file, quotechar=None):\n",
    "        df = pd.read_csv(input_file,sep='|')\n",
    "        relevantDf = df[df.Domain_Relevance == 9]\n",
    "        relevantDf['AspectSentiment'] = relevantDf['Aspect'] + '-' + relevantDf['Sentiment']\n",
    "        sentences = relevantDf['Sentence'].values.tolist()\n",
    "        aspectSentiment = relevantDf['AspectSentiment'].values.tolist()\n",
    "        return [sentences,aspectSentiment]\n",
    "\n",
    "class Sst2Processor(DataProcessor):\n",
    "    \"\"\"Processor for the SST-2 data set (GLUE version).\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_csv(os.path.join(data_dir, \"train/dataframe.csv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_csv(os.path.join(data_dir, \"validation/dataframe.csv\")), \"validation\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        all_labels = []\n",
    "        entities = ['g', 'p', 'f', 'c', 'cg', 'cp', 'cf', 'cc', 'cg', 'gg']\n",
    "        attributes = ['g', 'p', 'll', 'h', 'e', 'c', 's', 'q', 'pp', 't', 'or', 'a', 'l', 'av']\n",
    "        sentiments = ['0','p','n']\n",
    "        for entity in entities:\n",
    "            for attribute in attributes:\n",
    "                for sentiment in sentiments:\n",
    "                    all_labels.append(entity+'-'+attribute+'-'+sentiment)\n",
    "        return all_labels\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for i in range(len(lines[0])):\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = lines[0][i]\n",
    "            label = lines[1][i]\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "        return examples\n",
    "\n",
    "\n",
    "def convert_examples_to_features(examples, label_list, max_seq_length,\n",
    "                                 tokenizer, output_mode):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    label_map = {label : i for i, label in enumerate(label_list)}\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        if ex_index % 1000 == 0:\n",
    "            logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "            # length is less than the specified length.\n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "        else:\n",
    "            # Account for [CLS] and [SEP] with \"- 2\"\n",
    "            if len(tokens_a) > max_seq_length - 2:\n",
    "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids: 0   0   0   0  0     0 0\n",
    "        #\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "        segment_ids = [0] * len(tokens)\n",
    "\n",
    "        if tokens_b:\n",
    "            tokens += tokens_b + [\"[SEP]\"]\n",
    "            segment_ids += [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "\n",
    "        if output_mode == \"classification\":\n",
    "            label_id = label_map[example.label]\n",
    "        elif output_mode == \"regression\":\n",
    "            label_id = float(example.label)\n",
    "        else:\n",
    "            raise KeyError(output_mode)\n",
    "\n",
    "        if ex_index < 5:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid: %s\" % (example.guid))\n",
    "            logger.info(\"tokens: %s\" % \" \".join(\n",
    "                    [str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\n",
    "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "            logger.info(\"label: %s (id = %d)\" % (example.label, label_id))\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_id=label_id))\n",
    "    return features\n",
    "\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "\n",
    "\n",
    "def simple_accuracy(preds, labels):\n",
    "    return (preds == labels).mean()\n",
    "\n",
    "\n",
    "def acc_and_f1(preds, labels, average='binary'):\n",
    "    acc = simple_accuracy(preds, labels)\n",
    "    f1 = f1_score(y_true=labels, y_pred=preds, average=average)\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"acc_and_f1\": (acc + f1) / 2,\n",
    "    }\n",
    "\n",
    "\n",
    "def pearson_and_spearman(preds, labels):\n",
    "    pearson_corr = pearsonr(preds, labels)[0]\n",
    "    spearman_corr = spearmanr(preds, labels)[0]\n",
    "    return {\n",
    "        \"pearson\": pearson_corr,\n",
    "        \"spearmanr\": spearman_corr,\n",
    "        \"corr\": (pearson_corr + spearman_corr) / 2,\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_metrics(task_name, preds, labels):\n",
    "    assert len(preds) == len(labels)\n",
    "    if task_name == \"cola\":\n",
    "        return {\"mcc\": matthews_corrcoef(labels, preds)}\n",
    "    elif task_name == \"sst-2\":\n",
    "        return {\"acc\": acc_and_f1(preds, labels, 'weighted')}\n",
    "    elif task_name == \"mrpc\":\n",
    "        return acc_and_f1(preds, labels)\n",
    "    elif task_name == \"sts-b\":\n",
    "        return pearson_and_spearman(preds, labels)\n",
    "    elif task_name == \"qqp\":\n",
    "        return acc_and_f1(preds, labels)\n",
    "    elif task_name == \"mnli\":\n",
    "        return {\"acc\": simple_accuracy(preds, labels)}\n",
    "    elif task_name == \"mnli-mm\":\n",
    "        return {\"acc\": simple_accuracy(preds, labels)}\n",
    "    elif task_name == \"qnli\":\n",
    "        return {\"acc\": simple_accuracy(preds, labels)}\n",
    "    elif task_name == \"rte\":\n",
    "        return {\"acc\": simple_accuracy(preds, labels)}\n",
    "    elif task_name == \"wnli\":\n",
    "        return {\"acc\": simple_accuracy(preds, labels)}\n",
    "    else:\n",
    "        raise KeyError(task_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readConfig():\n",
    "    args = argparse.Namespace()\n",
    "    args.data_dir = './dataset/Organic/OD-A/train_test_validation V0.2/'\n",
    "    args.bert_model = 'bert-base-uncased'\n",
    "    args.task_name = 'sst-2'\n",
    "    args.output_dir = './models/Organic/Current'\n",
    "    args.cache_dir = './models/cache/'\n",
    "    args.max_seq_length = 32\n",
    "    args.do_lower_case = True\n",
    "    args.train_batch_size = 32\n",
    "    args.eval_batch_size = 32\n",
    "    args.learning_rate = 5e-5\n",
    "    args.num_train_epochs = 5.0\n",
    "    args.warmup_proportion = 0.1\n",
    "    args.no_cuda = False\n",
    "    args.local_rank = -1\n",
    "    args.seed = 42\n",
    "    args.gradient_accumulation_steps = 1.0\n",
    "    args.fp16 = False\n",
    "    args.loss_scale = 0\n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runTraining():\n",
    "    trace = []   \n",
    "    args = readConfig()\n",
    "\n",
    "    processors = {\n",
    "        \"sst-2\": Sst2Processor\n",
    "    }\n",
    "\n",
    "    output_modes = {\n",
    "        \"sst-2\": \"classification\"\n",
    "    }\n",
    "\n",
    "    if args.local_rank == -1 or args.no_cuda:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "        n_gpu = torch.cuda.device_count()\n",
    "    else:\n",
    "        torch.cuda.set_device(args.local_rank)\n",
    "        device = torch.device(\"cuda\", args.local_rank)\n",
    "        n_gpu = 1\n",
    "        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "        torch.distributed.init_process_group(backend='nccl')\n",
    "\n",
    "    logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                        datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                        level = logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
    "\n",
    "    logger.info(\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n",
    "        device, n_gpu, bool(args.local_rank != -1), args.fp16))\n",
    "\n",
    "    if args.gradient_accumulation_steps < 1:\n",
    "        raise ValueError(\"Invalid gradient_accumulation_steps parameter: {}, should be >= 1\".format(\n",
    "                            args.gradient_accumulation_steps))\n",
    "\n",
    "    args.train_batch_size = args.train_batch_size // args.gradient_accumulation_steps\n",
    "\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "    if os.path.exists(args.output_dir) and os.listdir(args.output_dir):\n",
    "        raise ValueError(\"Output directory ({}) already exists and is not empty.\".format(args.output_dir))\n",
    "    if not os.path.exists(args.output_dir):\n",
    "        os.makedirs(args.output_dir)\n",
    "\n",
    "    task_name = args.task_name.lower()\n",
    "\n",
    "    if task_name not in processors:\n",
    "        raise ValueError(\"Task not found: %s\" % (task_name))\n",
    "\n",
    "    processor = processors[task_name]()\n",
    "    output_mode = output_modes[task_name]\n",
    "\n",
    "    label_list = processor.get_labels()\n",
    "    num_labels = len(label_list)\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(args.bert_model, do_lower_case=args.do_lower_case)\n",
    "\n",
    "    train_examples = None\n",
    "    num_train_optimization_steps = None\n",
    "    train_examples = processor.get_train_examples(args.data_dir)\n",
    "    num_train_optimization_steps = int(\n",
    "        len(train_examples) / args.train_batch_size / args.gradient_accumulation_steps) * args.num_train_epochs\n",
    "    if args.local_rank != -1:\n",
    "        num_train_optimization_steps = num_train_optimization_steps // torch.distributed.get_world_size()\n",
    "\n",
    "    # Prepare model\n",
    "    cache_dir = args.cache_dir if args.cache_dir else os.path.join(str(PYTORCH_PRETRAINED_BERT_CACHE), 'distributed_{}'.format(args.local_rank))\n",
    "    model = BertForSequenceClassification.from_pretrained(args.bert_model,cache_dir=cache_dir,num_labels=num_labels)\n",
    "    \n",
    "    if args.fp16:\n",
    "        model.half()\n",
    "    model.to(device)\n",
    "    if args.local_rank != -1:\n",
    "        try:\n",
    "            from apex.parallel import DistributedDataParallel as DDP\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
    "\n",
    "        model = DDP(model)\n",
    "    elif n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Prepare optimizer\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex.optimizers import FP16_Optimizer\n",
    "            from apex.optimizers import FusedAdam\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
    "\n",
    "        optimizer = FusedAdam(optimizer_grouped_parameters,\n",
    "                              lr=args.learning_rate,\n",
    "                              bias_correction=False,\n",
    "                              max_grad_norm=1.0)\n",
    "        if args.loss_scale == 0:\n",
    "            optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\n",
    "        else:\n",
    "            optimizer = FP16_Optimizer(optimizer, static_loss_scale=args.loss_scale)\n",
    "        warmup_linear = WarmupLinearSchedule(warmup=args.warmup_proportion,\n",
    "                                             t_total=num_train_optimization_steps)\n",
    "\n",
    "    else:\n",
    "        optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                             lr=args.learning_rate,\n",
    "                             warmup=args.warmup_proportion,\n",
    "                             t_total=num_train_optimization_steps)\n",
    "\n",
    "    global_step = 0\n",
    "    nb_tr_steps = 0\n",
    "    tr_loss = 0\n",
    "    \n",
    "    train_features = convert_examples_to_features(\n",
    "        train_examples, label_list, args.max_seq_length, tokenizer, output_mode)\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_examples))\n",
    "    logger.info(\"  Batch size = %d\", args.train_batch_size)\n",
    "    logger.info(\"  Num steps = %d\", num_train_optimization_steps)\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "\n",
    "    if output_mode == \"classification\":\n",
    "        all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.long)\n",
    "    elif output_mode == \"regression\":\n",
    "        all_label_ids = torch.tensor([f.label_id for f in train_features], dtype=torch.float)\n",
    "\n",
    "    train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    if args.local_rank == -1:\n",
    "        train_sampler = RandomSampler(train_data)\n",
    "    else:\n",
    "        train_sampler = DistributedSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=int(args.train_batch_size))\n",
    "\n",
    "    model.train()\n",
    "    for epochCount in trange(int(args.num_train_epochs), desc=\"Epoch\"):\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "        trainIterator = tqdm(train_dataloader, desc=\"Iteration\")\n",
    "        for step, batch in enumerate(trainIterator):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, label_ids = batch\n",
    "\n",
    "            # define a new function to compute loss values for both output_modes\n",
    "            logits = model(input_ids, segment_ids, input_mask, labels=None)\n",
    "\n",
    "            if output_mode == \"classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, num_labels), label_ids.view(-1))\n",
    "            elif output_mode == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), label_ids.view(-1))\n",
    "\n",
    "            if n_gpu > 1:\n",
    "                loss = loss.mean() # mean() to average on multi-gpu.\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            if args.fp16:\n",
    "                optimizer.backward(loss)\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            trainIterator.set_postfix(loss=loss.item())\n",
    "            trace.append((global_step,loss.item()))\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                if args.fp16:\n",
    "                    # modify learning rate with special warm up BERT uses\n",
    "                    # if args.fp16 is False, BertAdam is used that handles this automatically\n",
    "                    lr_this_step = args.learning_rate * warmup_linear.get_lr(global_step, args.warmup_proportion)\n",
    "                    for param_group in optimizer.param_groups:\n",
    "                        param_group['lr'] = lr_this_step\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "    if args.local_rank == -1 or torch.distributed.get_rank() == 0:\n",
    "        # Save a trained model, configuration and tokenizer\n",
    "        model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "\n",
    "        # If we save using the predefined names, we can load using `from_pretrained`\n",
    "        output_model_file = os.path.join(args.output_dir, WEIGHTS_NAME)\n",
    "        output_config_file = os.path.join(args.output_dir, CONFIG_NAME)\n",
    "\n",
    "        torch.save(model_to_save.state_dict(), output_model_file)\n",
    "        model_to_save.config.to_json_file(output_config_file)\n",
    "        tokenizer.save_vocabulary(args.output_dir)\n",
    "    \n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runTest():\n",
    "    args = readConfig()\n",
    "\n",
    "    processors = {\n",
    "        \"sst-2\": Sst2Processor\n",
    "    }\n",
    "\n",
    "    output_modes = {\n",
    "        \"sst-2\": \"classification\"\n",
    "    }\n",
    "\n",
    "    if args.local_rank == -1 or args.no_cuda:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\")\n",
    "        n_gpu = torch.cuda.device_count()\n",
    "    else:\n",
    "        torch.cuda.set_device(args.local_rank)\n",
    "        device = torch.device(\"cuda\", args.local_rank)\n",
    "        n_gpu = 1\n",
    "        # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "        torch.distributed.init_process_group(backend='nccl')\n",
    "\n",
    "    logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                        datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                        level = logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
    "\n",
    "    logger.info(\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n",
    "        device, n_gpu, bool(args.local_rank != -1), args.fp16))\n",
    "    \n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "        \n",
    "    task_name = args.task_name.lower()\n",
    "\n",
    "    if task_name not in processors:\n",
    "        raise ValueError(\"Task not found: %s\" % (task_name))\n",
    "\n",
    "    processor = processors[task_name]()\n",
    "    output_mode = output_modes[task_name]\n",
    "\n",
    "    label_list = processor.get_labels()\n",
    "    num_labels = len(label_list)\n",
    "    \n",
    "    # Load a trained model and vocabulary that you have fine-tuned\n",
    "    model = BertForSequenceClassification.from_pretrained(args.output_dir, num_labels=num_labels)\n",
    "    tokenizer = BertTokenizer.from_pretrained(args.output_dir, do_lower_case=args.do_lower_case)\n",
    "    model.to(device)\n",
    "\n",
    "    if args.local_rank == -1 or torch.distributed.get_rank() == 0:\n",
    "        eval_examples = processor.get_dev_examples(args.data_dir)\n",
    "        eval_features = convert_examples_to_features(\n",
    "            eval_examples, label_list, args.max_seq_length, tokenizer, output_mode)\n",
    "        logger.info(\"***** Running evaluation *****\")\n",
    "        logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "        logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
    "        all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "\n",
    "        if output_mode == \"classification\":\n",
    "            all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.long)\n",
    "        elif output_mode == \"regression\":\n",
    "            all_label_ids = torch.tensor([f.label_id for f in eval_features], dtype=torch.float)\n",
    "\n",
    "        eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "        # Run prediction for full data\n",
    "        eval_sampler = SequentialSampler(eval_data)\n",
    "        eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
    "\n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        nb_eval_steps = 0\n",
    "        preds = []\n",
    "\n",
    "        for input_ids, input_mask, segment_ids, label_ids in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = input_ids.to(device)\n",
    "            input_mask = input_mask.to(device)\n",
    "            segment_ids = segment_ids.to(device)\n",
    "            label_ids = label_ids.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_ids, segment_ids, input_mask, labels=None)\n",
    "\n",
    "            # create eval loss and other metric required by the task\n",
    "            if output_mode == \"classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                tmp_eval_loss = loss_fct(logits.view(-1, num_labels), label_ids.view(-1))\n",
    "            elif output_mode == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                tmp_eval_loss = loss_fct(logits.view(-1), label_ids.view(-1))\n",
    "            \n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "            nb_eval_steps += 1\n",
    "            if len(preds) == 0:\n",
    "                preds.append(logits.detach().cpu().numpy())\n",
    "            else:\n",
    "                preds[0] = np.append(\n",
    "                    preds[0], logits.detach().cpu().numpy(), axis=0)\n",
    "\n",
    "        eval_loss = eval_loss / nb_eval_steps\n",
    "        preds = preds[0]\n",
    "        if output_mode == \"classification\":\n",
    "            preds = np.argmax(preds, axis=1)\n",
    "        elif output_mode == \"regression\":\n",
    "            preds = np.squeeze(preds)\n",
    "        result = compute_metrics(task_name, preds, all_label_ids.numpy())\n",
    "\n",
    "        result['eval_loss'] = eval_loss\n",
    "\n",
    "        output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n",
    "        with open(output_eval_file, \"w\") as writer:\n",
    "            logger.info(\"***** Eval results *****\")\n",
    "            for key in sorted(result.keys()):\n",
    "                logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/18/2019 20:00:02 - INFO - __main__ -   device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "05/18/2019 20:00:03 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/farrukh/.pytorch_pretrained_bert/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "/home/farrukh/anaconda3/envs/nlpLab/lib/python3.7/site-packages/ipykernel_launcher.py:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "05/18/2019 20:00:03 - INFO - pytorch_pretrained_bert.modeling -   loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz from cache at ./models/cache/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba\n",
      "05/18/2019 20:00:03 - INFO - pytorch_pretrained_bert.modeling -   extracting archive file ./models/cache/9c41111e2de84547a463fd39217199738d1e3deb72d4fec4399e6e241983c6f0.ae3cef932725ca7a30cdcb93fc6e09150a55e2a130ec7af63975a16c153ae2ba to temp dir /tmp/tmp1b70pc5o\n",
      "05/18/2019 20:00:06 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "05/18/2019 20:00:08 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "05/18/2019 20:00:08 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "05/18/2019 20:00:10 - INFO - __main__ -   Writing example 0 of 4687\n",
      "05/18/2019 20:00:10 - INFO - __main__ -   *** Example ***\n",
      "05/18/2019 20:00:10 - INFO - __main__ -   guid: train-0\n",
      "05/18/2019 20:00:10 - INFO - __main__ -   tokens: [CLS] industrial ##ization is everything about productivity and efficiency . [SEP]\n",
      "05/18/2019 20:00:10 - INFO - __main__ -   input_ids: 101 3919 3989 2003 2673 2055 15836 1998 8122 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/18/2019 20:00:10 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/18/2019 20:00:10 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/18/2019 20:00:10 - INFO - __main__ -   label: cg-pp-p (id = 361)\n",
      "05/18/2019 20:00:10 - INFO - __main__ -   *** Example ***\n",
      "05/18/2019 20:00:10 - INFO - __main__ -   guid: train-1\n",
      "05/18/2019 20:00:10 - INFO - __main__ -   tokens: [CLS] ' organic ' agriculture goes along with that theme , but certified organic is an over - reaction . [SEP]\n",
      "05/18/2019 20:00:10 - INFO - __main__ -   input_ids: 101 1005 7554 1005 5237 3632 2247 2007 2008 4323 1010 2021 7378 7554 2003 2019 2058 1011 4668 1012 102 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/18/2019 20:00:10 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/18/2019 20:00:10 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/18/2019 20:00:10 - INFO - __main__ -   label: g-g-0 (id = 0)\n",
      "05/18/2019 20:00:10 - INFO - __main__ -   *** Example ***\n",
      "05/18/2019 20:00:10 - INFO - __main__ -   guid: train-2\n",
      "05/18/2019 20:00:10 - INFO - __main__ -   tokens: [CLS] it isn ' t innate in chickens to be \" disease prone \" it is how they are raised . [SEP]\n",
      "05/18/2019 20:00:10 - INFO - __main__ -   input_ids: 101 2009 3475 1005 1056 25605 1999 21868 2000 2022 1000 4295 13047 1000 2009 2003 2129 2027 2024 2992 1012 102 0 0 0 0 0 0 0 0 0 0\n",
      "05/18/2019 20:00:10 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0\n",
      "05/18/2019 20:00:10 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/18/2019 20:00:10 - INFO - __main__ -   label: cp-a-n (id = 245)\n",
      "05/18/2019 20:00:10 - INFO - __main__ -   *** Example ***\n",
      "05/18/2019 20:00:10 - INFO - __main__ -   guid: train-3\n",
      "05/18/2019 20:00:10 - INFO - __main__ -   tokens: [CLS] what looks more healthy to you : factory farm ##ed chickens or this free range chickens because of the factory farm ##ed birds are so crowded they are living in [SEP]\n",
      "05/18/2019 20:00:10 - INFO - __main__ -   input_ids: 101 2054 3504 2062 7965 2000 2017 1024 4713 3888 2098 21868 2030 2023 2489 2846 21868 2138 1997 1996 4713 3888 2098 5055 2024 2061 10789 2027 2024 2542 1999 102\n",
      "05/18/2019 20:00:10 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/18/2019 20:00:10 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/18/2019 20:00:10 - INFO - __main__ -   label: cp-a-n (id = 245)\n",
      "05/18/2019 20:00:10 - INFO - __main__ -   *** Example ***\n",
      "05/18/2019 20:00:10 - INFO - __main__ -   guid: train-4\n",
      "05/18/2019 20:00:10 - INFO - __main__ -   tokens: [CLS] we currently only have 8 laying hen ##s but even in winter we ' re getting about 3 eggs a day . [SEP]\n",
      "05/18/2019 20:00:10 - INFO - __main__ -   input_ids: 101 2057 2747 2069 2031 1022 10201 21863 2015 2021 2130 1999 3467 2057 1005 2128 2893 2055 1017 6763 1037 2154 1012 102 0 0 0 0 0 0 0 0\n",
      "05/18/2019 20:00:10 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0\n",
      "05/18/2019 20:00:10 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/18/2019 20:00:10 - INFO - __main__ -   label: cf-g-p (id = 253)\n",
      "05/18/2019 20:00:10 - INFO - __main__ -   Writing example 1000 of 4687\n",
      "05/18/2019 20:00:11 - INFO - __main__ -   Writing example 2000 of 4687\n",
      "05/18/2019 20:00:11 - INFO - __main__ -   Writing example 3000 of 4687\n",
      "05/18/2019 20:00:11 - INFO - __main__ -   Writing example 4000 of 4687\n",
      "05/18/2019 20:00:11 - INFO - __main__ -   ***** Running training *****\n",
      "05/18/2019 20:00:11 - INFO - __main__ -     Num examples = 4687\n",
      "05/18/2019 20:00:11 - INFO - __main__ -     Batch size = 32\n",
      "05/18/2019 20:00:11 - INFO - __main__ -     Num steps = 730\n",
      "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]\n",
      "Iteration:   0%|          | 0/147 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   0%|          | 0/147 [00:01<?, ?it/s, loss=6.09]\u001b[A\n",
      "Iteration:   1%|          | 1/147 [00:01<04:34,  1.88s/it, loss=6.09]\u001b[A\n",
      "Iteration:   1%|          | 1/147 [00:03<04:34,  1.88s/it, loss=6.12]\u001b[A\n",
      "Iteration:   1%|▏         | 2/147 [00:03<04:24,  1.83s/it, loss=6.12]\u001b[A\n",
      "Iteration:   1%|▏         | 2/147 [00:04<04:24,  1.83s/it, loss=6.1] \u001b[A\n",
      "Iteration:   2%|▏         | 3/147 [00:05<04:15,  1.78s/it, loss=6.1]\u001b[A\n",
      "Iteration:   2%|▏         | 3/147 [00:06<04:15,  1.78s/it, loss=5.99]\u001b[A\n",
      "Iteration:   3%|▎         | 4/147 [00:06<04:08,  1.74s/it, loss=5.99]\u001b[A\n",
      "Iteration:   3%|▎         | 4/147 [00:08<04:08,  1.74s/it, loss=6.02]\u001b[A\n",
      "Iteration:   3%|▎         | 5/147 [00:08<04:04,  1.72s/it, loss=6.02]\u001b[A\n",
      "Iteration:   3%|▎         | 5/147 [00:09<04:04,  1.72s/it, loss=6.07]\u001b[A\n",
      "Iteration:   4%|▍         | 6/147 [00:10<03:59,  1.70s/it, loss=6.07]\u001b[A\n",
      "Iteration:   4%|▍         | 6/147 [00:11<03:59,  1.70s/it, loss=6.01]\u001b[A\n",
      "Iteration:   5%|▍         | 7/147 [00:11<03:55,  1.68s/it, loss=6.01]\u001b[A\n",
      "Iteration:   5%|▍         | 7/147 [00:13<03:55,  1.68s/it, loss=5.96]\u001b[A\n",
      "Iteration:   5%|▌         | 8/147 [00:13<03:52,  1.67s/it, loss=5.96]\u001b[A\n",
      "Iteration:   5%|▌         | 8/147 [00:14<03:52,  1.67s/it, loss=5.96]\u001b[A\n",
      "Iteration:   6%|▌         | 9/147 [00:15<03:49,  1.66s/it, loss=5.96]\u001b[A\n",
      "Iteration:   6%|▌         | 9/147 [00:16<03:49,  1.66s/it, loss=6.05]\u001b[A\n",
      "Iteration:   7%|▋         | 10/147 [00:16<03:47,  1.66s/it, loss=6.05]\u001b[A\n",
      "Iteration:   7%|▋         | 10/147 [00:18<03:47,  1.66s/it, loss=5.99]\u001b[A\n",
      "Iteration:   7%|▋         | 11/147 [00:18<03:47,  1.67s/it, loss=5.99]\u001b[A\n",
      "Iteration:   7%|▋         | 11/147 [00:19<03:47,  1.67s/it, loss=5.97]\u001b[A\n",
      "Iteration:   8%|▊         | 12/147 [00:20<03:44,  1.66s/it, loss=5.97]\u001b[A\n",
      "Iteration:   8%|▊         | 12/147 [00:21<03:44,  1.66s/it, loss=5.92]\u001b[A\n",
      "Iteration:   9%|▉         | 13/147 [00:21<03:41,  1.66s/it, loss=5.92]\u001b[A\n",
      "Iteration:   9%|▉         | 13/147 [00:22<03:41,  1.66s/it, loss=5.89]\u001b[A\n",
      "Iteration:  10%|▉         | 14/147 [00:23<03:39,  1.65s/it, loss=5.89]\u001b[A\n",
      "Iteration:  10%|▉         | 14/147 [00:24<03:39,  1.65s/it, loss=5.85]\u001b[A\n",
      "Iteration:  10%|█         | 15/147 [00:25<03:36,  1.64s/it, loss=5.85]\u001b[A\n",
      "Iteration:  10%|█         | 15/147 [00:26<03:36,  1.64s/it, loss=5.9] \u001b[A\n",
      "Iteration:  11%|█         | 16/147 [00:26<03:34,  1.64s/it, loss=5.9]\u001b[A\n",
      "Iteration:  11%|█         | 16/147 [00:27<03:34,  1.64s/it, loss=5.71]\u001b[A\n",
      "Iteration:  12%|█▏        | 17/147 [00:28<03:32,  1.64s/it, loss=5.71]\u001b[A\n",
      "Iteration:  12%|█▏        | 17/147 [00:29<03:32,  1.64s/it, loss=5.72]\u001b[A\n",
      "Iteration:  12%|█▏        | 18/147 [00:29<03:32,  1.65s/it, loss=5.72]\u001b[A\n",
      "Iteration:  12%|█▏        | 18/147 [00:31<03:32,  1.65s/it, loss=5.79]\u001b[A\n",
      "Iteration:  13%|█▎        | 19/147 [00:31<03:30,  1.64s/it, loss=5.79]\u001b[A\n",
      "Iteration:  13%|█▎        | 19/147 [00:32<03:30,  1.64s/it, loss=5.7] \u001b[A\n",
      "Iteration:  14%|█▎        | 20/147 [00:33<03:28,  1.64s/it, loss=5.7]\u001b[A\n",
      "Iteration:  14%|█▎        | 20/147 [00:34<03:28,  1.64s/it, loss=5.72]\u001b[A\n",
      "Iteration:  14%|█▍        | 21/147 [00:34<03:26,  1.64s/it, loss=5.72]\u001b[A\n",
      "Iteration:  14%|█▍        | 21/147 [00:36<03:26,  1.64s/it, loss=5.72]\u001b[A\n",
      "Iteration:  15%|█▍        | 22/147 [00:36<03:24,  1.64s/it, loss=5.72]\u001b[A\n",
      "Iteration:  15%|█▍        | 22/147 [00:37<03:24,  1.64s/it, loss=5.63]\u001b[A\n",
      "Iteration:  16%|█▌        | 23/147 [00:38<03:22,  1.63s/it, loss=5.63]\u001b[A\n",
      "Iteration:  16%|█▌        | 23/147 [00:39<03:22,  1.63s/it, loss=5.64]\u001b[A\n",
      "Iteration:  16%|█▋        | 24/147 [00:39<03:20,  1.63s/it, loss=5.64]\u001b[A\n",
      "Iteration:  16%|█▋        | 24/147 [00:40<03:20,  1.63s/it, loss=5.41]\u001b[A\n",
      "Iteration:  17%|█▋        | 25/147 [00:41<03:19,  1.63s/it, loss=5.41]\u001b[A\n",
      "Iteration:  17%|█▋        | 25/147 [00:42<03:19,  1.63s/it, loss=5.41]\u001b[A\n",
      "Iteration:  18%|█▊        | 26/147 [00:43<03:18,  1.64s/it, loss=5.41]\u001b[A\n",
      "Iteration:  18%|█▊        | 26/147 [00:44<03:18,  1.64s/it, loss=5.38]\u001b[A\n",
      "Iteration:  18%|█▊        | 27/147 [00:44<03:18,  1.65s/it, loss=5.38]\u001b[A\n",
      "Iteration:  18%|█▊        | 27/147 [00:45<03:18,  1.65s/it, loss=5.28]\u001b[A\n",
      "Iteration:  19%|█▉        | 28/147 [00:46<03:16,  1.65s/it, loss=5.28]\u001b[A\n",
      "Iteration:  19%|█▉        | 28/147 [00:47<03:16,  1.65s/it, loss=5.23]\u001b[A\n",
      "Iteration:  20%|█▉        | 29/147 [00:48<03:14,  1.65s/it, loss=5.23]\u001b[A\n",
      "Iteration:  20%|█▉        | 29/147 [00:49<03:14,  1.65s/it, loss=5.13]\u001b[A\n",
      "Iteration:  20%|██        | 30/147 [00:49<03:12,  1.65s/it, loss=5.13]\u001b[A\n",
      "Iteration:  20%|██        | 30/147 [00:50<03:12,  1.65s/it, loss=5.28]\u001b[A\n",
      "Iteration:  21%|██        | 31/147 [00:51<03:11,  1.65s/it, loss=5.28]\u001b[A\n",
      "Iteration:  21%|██        | 31/147 [00:52<03:11,  1.65s/it, loss=5.16]\u001b[A\n",
      "Iteration:  22%|██▏       | 32/147 [00:52<03:09,  1.65s/it, loss=5.16]\u001b[A\n",
      "Iteration:  22%|██▏       | 32/147 [00:54<03:09,  1.65s/it, loss=5.36]\u001b[A\n",
      "Iteration:  22%|██▏       | 33/147 [00:54<03:07,  1.65s/it, loss=5.36]\u001b[A\n",
      "Iteration:  22%|██▏       | 33/147 [00:55<03:07,  1.65s/it, loss=5.39]\u001b[A\n",
      "Iteration:  23%|██▎       | 34/147 [00:56<03:05,  1.64s/it, loss=5.39]\u001b[A\n",
      "Iteration:  23%|██▎       | 34/147 [00:57<03:05,  1.64s/it, loss=5.01]\u001b[A\n",
      "Iteration:  24%|██▍       | 35/147 [00:57<03:04,  1.64s/it, loss=5.01]\u001b[A\n",
      "Iteration:  24%|██▍       | 35/147 [00:59<03:04,  1.64s/it, loss=5.12]\u001b[A\n",
      "Iteration:  24%|██▍       | 36/147 [00:59<03:02,  1.64s/it, loss=5.12]\u001b[A\n",
      "Iteration:  24%|██▍       | 36/147 [01:00<03:02,  1.64s/it, loss=4.67]\u001b[A\n",
      "Iteration:  25%|██▌       | 37/147 [01:01<03:01,  1.65s/it, loss=4.67]\u001b[A\n",
      "Iteration:  25%|██▌       | 37/147 [01:02<03:01,  1.65s/it, loss=5.06]\u001b[A\n",
      "Iteration:  26%|██▌       | 38/147 [01:02<03:00,  1.65s/it, loss=5.06]\u001b[A\n",
      "Iteration:  26%|██▌       | 38/147 [01:04<03:00,  1.65s/it, loss=5.34]\u001b[A\n",
      "Iteration:  27%|██▋       | 39/147 [01:04<02:59,  1.66s/it, loss=5.34]\u001b[A\n",
      "Iteration:  27%|██▋       | 39/147 [01:05<02:59,  1.66s/it, loss=4.62]\u001b[A\n",
      "Iteration:  27%|██▋       | 40/147 [01:06<02:57,  1.66s/it, loss=4.62]\u001b[A\n",
      "Iteration:  27%|██▋       | 40/147 [01:07<02:57,  1.66s/it, loss=5.34]\u001b[A\n",
      "Iteration:  28%|██▊       | 41/147 [01:07<02:55,  1.66s/it, loss=5.34]\u001b[A\n",
      "Iteration:  28%|██▊       | 41/147 [01:09<02:55,  1.66s/it, loss=4.62]\u001b[A\n",
      "Iteration:  29%|██▊       | 42/147 [01:09<02:53,  1.65s/it, loss=4.62]\u001b[A\n",
      "Iteration:  29%|██▊       | 42/147 [01:10<02:53,  1.65s/it, loss=4.98]\u001b[A\n",
      "Iteration:  29%|██▉       | 43/147 [01:11<02:52,  1.66s/it, loss=4.98]\u001b[A\n",
      "Iteration:  29%|██▉       | 43/147 [01:12<02:52,  1.66s/it, loss=5.19]\u001b[A\n",
      "Iteration:  30%|██▉       | 44/147 [01:12<02:51,  1.67s/it, loss=5.19]\u001b[A\n",
      "Iteration:  30%|██▉       | 44/147 [01:14<02:51,  1.67s/it, loss=5.08]\u001b[A\n",
      "Iteration:  31%|███       | 45/147 [01:14<02:49,  1.66s/it, loss=5.08]\u001b[A\n",
      "Iteration:  31%|███       | 45/147 [01:15<02:49,  1.66s/it, loss=4.83]\u001b[A\n",
      "Iteration:  31%|███▏      | 46/147 [01:16<02:47,  1.66s/it, loss=4.83]\u001b[A\n",
      "Iteration:  31%|███▏      | 46/147 [01:17<02:47,  1.66s/it, loss=4.71]\u001b[A\n",
      "Iteration:  32%|███▏      | 47/147 [01:17<02:45,  1.66s/it, loss=4.71]\u001b[A\n",
      "Iteration:  32%|███▏      | 47/147 [01:18<02:45,  1.66s/it, loss=5.04]\u001b[A\n",
      "Iteration:  33%|███▎      | 48/147 [01:19<02:44,  1.66s/it, loss=5.04]\u001b[A\n",
      "Iteration:  33%|███▎      | 48/147 [01:20<02:44,  1.66s/it, loss=4.9] \u001b[A\n",
      "Iteration:  33%|███▎      | 49/147 [01:21<02:42,  1.66s/it, loss=4.9]\u001b[A\n",
      "Iteration:  33%|███▎      | 49/147 [01:22<02:42,  1.66s/it, loss=4.82]\u001b[A\n",
      "Iteration:  34%|███▍      | 50/147 [01:22<02:41,  1.67s/it, loss=4.82]\u001b[A\n",
      "Iteration:  34%|███▍      | 50/147 [01:24<02:41,  1.67s/it, loss=4.97]\u001b[A\n",
      "Iteration:  35%|███▍      | 51/147 [01:24<02:40,  1.68s/it, loss=4.97]\u001b[A\n",
      "Iteration:  35%|███▍      | 51/147 [01:25<02:40,  1.68s/it, loss=4.52]\u001b[A\n",
      "Iteration:  35%|███▌      | 52/147 [01:26<02:39,  1.68s/it, loss=4.52]\u001b[A\n",
      "Iteration:  35%|███▌      | 52/147 [01:27<02:39,  1.68s/it, loss=4.69]\u001b[A\n",
      "Iteration:  36%|███▌      | 53/147 [01:27<02:37,  1.68s/it, loss=4.69]\u001b[A\n",
      "Iteration:  36%|███▌      | 53/147 [01:29<02:37,  1.68s/it, loss=5.22]\u001b[A\n",
      "Iteration:  37%|███▋      | 54/147 [01:29<02:35,  1.67s/it, loss=5.22]\u001b[A\n",
      "Iteration:  37%|███▋      | 54/147 [01:30<02:35,  1.67s/it, loss=4.76]\u001b[A\n",
      "Iteration:  37%|███▋      | 55/147 [01:31<02:34,  1.68s/it, loss=4.76]\u001b[A\n",
      "Iteration:  37%|███▋      | 55/147 [01:32<02:34,  1.68s/it, loss=4.74]\u001b[A\n",
      "Iteration:  38%|███▊      | 56/147 [01:32<02:33,  1.68s/it, loss=4.74]\u001b[A\n",
      "Iteration:  38%|███▊      | 56/147 [01:34<02:33,  1.68s/it, loss=4.89]\u001b[A\n",
      "Iteration:  39%|███▉      | 57/147 [01:34<02:31,  1.68s/it, loss=4.89]\u001b[A\n",
      "Iteration:  39%|███▉      | 57/147 [01:35<02:31,  1.68s/it, loss=4.76]\u001b[A\n",
      "Iteration:  39%|███▉      | 58/147 [01:36<02:29,  1.68s/it, loss=4.76]\u001b[A\n",
      "Iteration:  39%|███▉      | 58/147 [01:37<02:29,  1.68s/it, loss=4.72]\u001b[A\n",
      "Iteration:  40%|████      | 59/147 [01:37<02:27,  1.67s/it, loss=4.72]\u001b[A\n",
      "Iteration:  40%|████      | 59/147 [01:39<02:27,  1.67s/it, loss=4.72]\u001b[A\n",
      "Iteration:  41%|████      | 60/147 [01:39<02:25,  1.67s/it, loss=4.72]\u001b[A\n",
      "Iteration:  41%|████      | 60/147 [01:40<02:25,  1.67s/it, loss=4.84]\u001b[A\n",
      "Iteration:  41%|████▏     | 61/147 [01:41<02:23,  1.67s/it, loss=4.84]\u001b[A\n",
      "Iteration:  41%|████▏     | 61/147 [01:42<02:23,  1.67s/it, loss=4.82]\u001b[A\n",
      "Iteration:  42%|████▏     | 62/147 [01:42<02:22,  1.67s/it, loss=4.82]\u001b[A\n",
      "Iteration:  42%|████▏     | 62/147 [01:44<02:22,  1.67s/it, loss=4.49]\u001b[A\n",
      "Iteration:  43%|████▎     | 63/147 [01:44<02:20,  1.67s/it, loss=4.49]\u001b[A\n",
      "Iteration:  43%|████▎     | 63/147 [01:45<02:20,  1.67s/it, loss=4.48]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  44%|████▎     | 64/147 [01:46<02:18,  1.67s/it, loss=4.48]\u001b[A\n",
      "Iteration:  44%|████▎     | 64/147 [01:47<02:18,  1.67s/it, loss=4.79]\u001b[A\n",
      "Iteration:  44%|████▍     | 65/147 [01:47<02:17,  1.68s/it, loss=4.79]\u001b[A\n",
      "Iteration:  44%|████▍     | 65/147 [01:49<02:17,  1.68s/it, loss=4.55]\u001b[A\n",
      "Iteration:  45%|████▍     | 66/147 [01:49<02:15,  1.67s/it, loss=4.55]\u001b[A\n",
      "Iteration:  45%|████▍     | 66/147 [01:50<02:15,  1.67s/it, loss=4.45]\u001b[A\n",
      "Iteration:  46%|████▌     | 67/147 [01:51<02:13,  1.67s/it, loss=4.45]\u001b[A\n",
      "Iteration:  46%|████▌     | 67/147 [01:52<02:13,  1.67s/it, loss=4.31]\u001b[A\n",
      "Iteration:  46%|████▋     | 68/147 [01:52<02:12,  1.67s/it, loss=4.31]\u001b[A\n",
      "Iteration:  46%|████▋     | 68/147 [01:54<02:12,  1.67s/it, loss=4.53]\u001b[A\n",
      "Iteration:  47%|████▋     | 69/147 [01:54<02:10,  1.67s/it, loss=4.53]\u001b[A\n",
      "Iteration:  47%|████▋     | 69/147 [01:55<02:10,  1.67s/it, loss=4.68]\u001b[A\n",
      "Iteration:  48%|████▊     | 70/147 [01:56<02:08,  1.67s/it, loss=4.68]\u001b[A\n",
      "Iteration:  48%|████▊     | 70/147 [01:57<02:08,  1.67s/it, loss=4.89]\u001b[A\n",
      "Iteration:  48%|████▊     | 71/147 [01:57<02:06,  1.67s/it, loss=4.89]\u001b[A\n",
      "Iteration:  48%|████▊     | 71/147 [01:59<02:06,  1.67s/it, loss=4.78]\u001b[A\n",
      "Iteration:  49%|████▉     | 72/147 [01:59<02:05,  1.67s/it, loss=4.78]\u001b[A\n",
      "Iteration:  49%|████▉     | 72/147 [02:00<02:05,  1.67s/it, loss=4.86]\u001b[A\n",
      "Iteration:  50%|████▉     | 73/147 [02:01<02:04,  1.68s/it, loss=4.86]\u001b[A\n",
      "Iteration:  50%|████▉     | 73/147 [02:02<02:04,  1.68s/it, loss=5.03]\u001b[A\n",
      "Iteration:  50%|█████     | 74/147 [02:03<02:02,  1.68s/it, loss=5.03]\u001b[A\n",
      "Iteration:  50%|█████     | 74/147 [02:04<02:02,  1.68s/it, loss=4.75]\u001b[A\n",
      "Iteration:  51%|█████     | 75/147 [02:04<02:00,  1.68s/it, loss=4.75]\u001b[A\n",
      "Iteration:  51%|█████     | 75/147 [02:05<02:00,  1.68s/it, loss=4.42]\u001b[A\n",
      "Iteration:  52%|█████▏    | 76/147 [02:06<01:59,  1.68s/it, loss=4.42]\u001b[A\n",
      "Iteration:  52%|█████▏    | 76/147 [02:07<01:59,  1.68s/it, loss=4.59]\u001b[A\n",
      "Iteration:  52%|█████▏    | 77/147 [02:08<01:57,  1.68s/it, loss=4.59]\u001b[A\n",
      "Iteration:  52%|█████▏    | 77/147 [02:09<01:57,  1.68s/it, loss=4.46]\u001b[A\n",
      "Iteration:  53%|█████▎    | 78/147 [02:09<01:55,  1.68s/it, loss=4.46]\u001b[A\n",
      "Iteration:  53%|█████▎    | 78/147 [02:10<01:55,  1.68s/it, loss=4.79]\u001b[A\n",
      "Iteration:  54%|█████▎    | 79/147 [02:11<01:54,  1.68s/it, loss=4.79]\u001b[A\n",
      "Iteration:  54%|█████▎    | 79/147 [02:12<01:54,  1.68s/it, loss=4.07]\u001b[A\n",
      "Iteration:  54%|█████▍    | 80/147 [02:13<01:52,  1.68s/it, loss=4.07]\u001b[A\n",
      "Iteration:  54%|█████▍    | 80/147 [02:14<01:52,  1.68s/it, loss=4.26]\u001b[A\n",
      "Iteration:  55%|█████▌    | 81/147 [02:14<01:51,  1.68s/it, loss=4.26]\u001b[A\n",
      "Iteration:  55%|█████▌    | 81/147 [02:16<01:51,  1.68s/it, loss=4.68]\u001b[A\n",
      "Iteration:  56%|█████▌    | 82/147 [02:16<01:49,  1.68s/it, loss=4.68]\u001b[A\n",
      "Iteration:  56%|█████▌    | 82/147 [02:17<01:49,  1.68s/it, loss=5.14]\u001b[A\n",
      "Iteration:  56%|█████▋    | 83/147 [02:18<01:47,  1.69s/it, loss=5.14]\u001b[A\n",
      "Iteration:  56%|█████▋    | 83/147 [02:19<01:47,  1.69s/it, loss=4.01]\u001b[A\n",
      "Iteration:  57%|█████▋    | 84/147 [02:19<01:46,  1.69s/it, loss=4.01]\u001b[A\n",
      "Iteration:  57%|█████▋    | 84/147 [02:21<01:46,  1.69s/it, loss=4.61]\u001b[A\n",
      "Iteration:  58%|█████▊    | 85/147 [02:21<01:44,  1.69s/it, loss=4.61]\u001b[A\n",
      "Iteration:  58%|█████▊    | 85/147 [02:22<01:44,  1.69s/it, loss=4.76]\u001b[A\n",
      "Iteration:  59%|█████▊    | 86/147 [02:23<01:43,  1.69s/it, loss=4.76]\u001b[A\n",
      "Iteration:  59%|█████▊    | 86/147 [02:24<01:43,  1.69s/it, loss=4.69]\u001b[A\n",
      "Iteration:  59%|█████▉    | 87/147 [02:25<01:42,  1.71s/it, loss=4.69]\u001b[A\n",
      "Iteration:  59%|█████▉    | 87/147 [02:26<01:42,  1.71s/it, loss=4.69]\u001b[A\n",
      "Iteration:  60%|█████▉    | 88/147 [02:26<01:42,  1.73s/it, loss=4.69]\u001b[A\n",
      "Iteration:  60%|█████▉    | 88/147 [02:28<01:42,  1.73s/it, loss=4.33]\u001b[A\n",
      "Iteration:  61%|██████    | 89/147 [02:28<01:41,  1.75s/it, loss=4.33]\u001b[A\n",
      "Iteration:  61%|██████    | 89/147 [02:29<01:41,  1.75s/it, loss=4.67]\u001b[A\n",
      "Iteration:  61%|██████    | 90/147 [02:30<01:40,  1.77s/it, loss=4.67]\u001b[A\n",
      "Iteration:  61%|██████    | 90/147 [02:31<01:40,  1.77s/it, loss=4.48]\u001b[A\n",
      "Iteration:  62%|██████▏   | 91/147 [02:32<01:39,  1.79s/it, loss=4.48]\u001b[A\n",
      "Iteration:  62%|██████▏   | 91/147 [02:33<01:39,  1.79s/it, loss=4.34]\u001b[A\n",
      "Iteration:  63%|██████▎   | 92/147 [02:33<01:37,  1.78s/it, loss=4.34]\u001b[A\n",
      "Iteration:  63%|██████▎   | 92/147 [02:35<01:37,  1.78s/it, loss=4.56]\u001b[A\n",
      "Iteration:  63%|██████▎   | 93/147 [02:35<01:36,  1.79s/it, loss=4.56]\u001b[A\n",
      "Iteration:  63%|██████▎   | 93/147 [02:37<01:36,  1.79s/it, loss=4.57]\u001b[A\n",
      "Iteration:  64%|██████▍   | 94/147 [02:37<01:33,  1.77s/it, loss=4.57]\u001b[A\n",
      "Iteration:  64%|██████▍   | 94/147 [02:38<01:33,  1.77s/it, loss=4.59]\u001b[A\n",
      "Iteration:  65%|██████▍   | 95/147 [02:39<01:30,  1.75s/it, loss=4.59]\u001b[A\n",
      "Iteration:  65%|██████▍   | 95/147 [02:40<01:30,  1.75s/it, loss=4.91]\u001b[A\n",
      "Iteration:  65%|██████▌   | 96/147 [02:40<01:28,  1.73s/it, loss=4.91]\u001b[A\n",
      "Iteration:  65%|██████▌   | 96/147 [02:42<01:28,  1.73s/it, loss=4.6] \u001b[A\n",
      "Iteration:  66%|██████▌   | 97/147 [02:42<01:26,  1.73s/it, loss=4.6]\u001b[A\n",
      "Iteration:  66%|██████▌   | 97/147 [02:43<01:26,  1.73s/it, loss=4.58]\u001b[A\n",
      "Iteration:  67%|██████▋   | 98/147 [02:44<01:24,  1.72s/it, loss=4.58]\u001b[A\n",
      "Iteration:  67%|██████▋   | 98/147 [02:45<01:24,  1.72s/it, loss=4.55]\u001b[A\n",
      "Iteration:  67%|██████▋   | 99/147 [02:46<01:22,  1.71s/it, loss=4.55]\u001b[A\n",
      "Iteration:  67%|██████▋   | 99/147 [02:47<01:22,  1.71s/it, loss=4.24]\u001b[A\n",
      "Iteration:  68%|██████▊   | 100/147 [02:47<01:20,  1.71s/it, loss=4.24]\u001b[A\n",
      "Iteration:  68%|██████▊   | 100/147 [02:48<01:20,  1.71s/it, loss=4.47]\u001b[A\n",
      "Iteration:  69%|██████▊   | 101/147 [02:49<01:18,  1.71s/it, loss=4.47]\u001b[A\n",
      "Iteration:  69%|██████▊   | 101/147 [02:50<01:18,  1.71s/it, loss=4.69]\u001b[A\n",
      "Iteration:  69%|██████▉   | 102/147 [02:51<01:16,  1.71s/it, loss=4.69]\u001b[A\n",
      "Iteration:  69%|██████▉   | 102/147 [02:52<01:16,  1.71s/it, loss=4.15]\u001b[A\n",
      "Iteration:  70%|███████   | 103/147 [02:52<01:15,  1.71s/it, loss=4.15]\u001b[A\n",
      "Iteration:  70%|███████   | 103/147 [02:54<01:15,  1.71s/it, loss=3.96]\u001b[A\n",
      "Iteration:  71%|███████   | 104/147 [02:54<01:13,  1.71s/it, loss=3.96]\u001b[A\n",
      "Iteration:  71%|███████   | 104/147 [02:55<01:13,  1.71s/it, loss=4.49]\u001b[A\n",
      "Iteration:  71%|███████▏  | 105/147 [02:56<01:11,  1.71s/it, loss=4.49]\u001b[A\n",
      "Iteration:  71%|███████▏  | 105/147 [02:57<01:11,  1.71s/it, loss=4.37]\u001b[A\n",
      "Iteration:  72%|███████▏  | 106/147 [02:58<01:10,  1.71s/it, loss=4.37]\u001b[A\n",
      "Iteration:  72%|███████▏  | 106/147 [02:59<01:10,  1.71s/it, loss=4.4] \u001b[A\n",
      "Iteration:  73%|███████▎  | 107/147 [02:59<01:08,  1.71s/it, loss=4.4]\u001b[A\n",
      "Iteration:  73%|███████▎  | 107/147 [03:00<01:08,  1.71s/it, loss=4.4]\u001b[A\n",
      "Iteration:  73%|███████▎  | 108/147 [03:01<01:06,  1.71s/it, loss=4.4]\u001b[A\n",
      "Iteration:  73%|███████▎  | 108/147 [03:02<01:06,  1.71s/it, loss=4.73]\u001b[A\n",
      "Iteration:  74%|███████▍  | 109/147 [03:03<01:04,  1.71s/it, loss=4.73]\u001b[A\n",
      "Iteration:  74%|███████▍  | 109/147 [03:04<01:04,  1.71s/it, loss=4.62]\u001b[A\n",
      "Iteration:  75%|███████▍  | 110/147 [03:04<01:03,  1.71s/it, loss=4.62]\u001b[A\n",
      "Iteration:  75%|███████▍  | 110/147 [03:06<01:03,  1.71s/it, loss=3.99]\u001b[A\n",
      "Iteration:  76%|███████▌  | 111/147 [03:06<01:01,  1.71s/it, loss=3.99]\u001b[A\n",
      "Iteration:  76%|███████▌  | 111/147 [03:07<01:01,  1.71s/it, loss=4.46]\u001b[A\n",
      "Iteration:  76%|███████▌  | 112/147 [03:08<00:59,  1.71s/it, loss=4.46]\u001b[A\n",
      "Iteration:  76%|███████▌  | 112/147 [03:09<00:59,  1.71s/it, loss=4.38]\u001b[A\n",
      "Iteration:  77%|███████▋  | 113/147 [03:09<00:58,  1.71s/it, loss=4.38]\u001b[A\n",
      "Iteration:  77%|███████▋  | 113/147 [03:11<00:58,  1.71s/it, loss=4.65]\u001b[A\n",
      "Iteration:  78%|███████▊  | 114/147 [03:11<00:56,  1.71s/it, loss=4.65]\u001b[A\n",
      "Iteration:  78%|███████▊  | 114/147 [03:12<00:56,  1.71s/it, loss=4.22]\u001b[A\n",
      "Iteration:  78%|███████▊  | 115/147 [03:13<00:54,  1.71s/it, loss=4.22]\u001b[A\n",
      "Iteration:  78%|███████▊  | 115/147 [03:14<00:54,  1.71s/it, loss=3.66]\u001b[A\n",
      "Iteration:  79%|███████▉  | 116/147 [03:15<00:52,  1.71s/it, loss=3.66]\u001b[A\n",
      "Iteration:  79%|███████▉  | 116/147 [03:16<00:52,  1.71s/it, loss=4.3] \u001b[A\n",
      "Iteration:  80%|███████▉  | 117/147 [03:16<00:51,  1.71s/it, loss=4.3]\u001b[A\n",
      "Iteration:  80%|███████▉  | 117/147 [03:18<00:51,  1.71s/it, loss=4.37]\u001b[A\n",
      "Iteration:  80%|████████  | 118/147 [03:18<00:49,  1.72s/it, loss=4.37]\u001b[A\n",
      "Iteration:  80%|████████  | 118/147 [03:19<00:49,  1.72s/it, loss=5.1] \u001b[A\n",
      "Iteration:  81%|████████  | 119/147 [03:20<00:48,  1.72s/it, loss=5.1]\u001b[A\n",
      "Iteration:  81%|████████  | 119/147 [03:21<00:48,  1.72s/it, loss=4.24]\u001b[A\n",
      "Iteration:  82%|████████▏ | 120/147 [03:21<00:46,  1.72s/it, loss=4.24]\u001b[A\n",
      "Iteration:  82%|████████▏ | 120/147 [03:23<00:46,  1.72s/it, loss=4.27]\u001b[A\n",
      "Iteration:  82%|████████▏ | 121/147 [03:23<00:44,  1.72s/it, loss=4.27]\u001b[A\n",
      "Iteration:  82%|████████▏ | 121/147 [03:24<00:44,  1.72s/it, loss=4.36]\u001b[A\n",
      "Iteration:  83%|████████▎ | 122/147 [03:25<00:42,  1.72s/it, loss=4.36]\u001b[A\n",
      "Iteration:  83%|████████▎ | 122/147 [03:26<00:42,  1.72s/it, loss=3.94]\u001b[A\n",
      "Iteration:  84%|████████▎ | 123/147 [03:27<00:41,  1.72s/it, loss=3.94]\u001b[A\n",
      "Iteration:  84%|████████▎ | 123/147 [03:28<00:41,  1.72s/it, loss=4.2] \u001b[A\n",
      "Iteration:  84%|████████▍ | 124/147 [03:28<00:39,  1.72s/it, loss=4.2]\u001b[A\n",
      "Iteration:  84%|████████▍ | 124/147 [03:30<00:39,  1.72s/it, loss=4.15]\u001b[A\n",
      "Iteration:  85%|████████▌ | 125/147 [03:30<00:37,  1.72s/it, loss=4.15]\u001b[A\n",
      "Iteration:  85%|████████▌ | 125/147 [03:31<00:37,  1.72s/it, loss=4.14]\u001b[A\n",
      "Iteration:  86%|████████▌ | 126/147 [03:32<00:36,  1.72s/it, loss=4.14]\u001b[A\n",
      "Iteration:  86%|████████▌ | 126/147 [03:33<00:36,  1.72s/it, loss=4.59]\u001b[A\n",
      "Iteration:  86%|████████▋ | 127/147 [03:34<00:34,  1.72s/it, loss=4.59]\u001b[A\n",
      "Iteration:  86%|████████▋ | 127/147 [03:35<00:34,  1.72s/it, loss=4.27]\u001b[A\n",
      "Iteration:  87%|████████▋ | 128/147 [03:35<00:32,  1.72s/it, loss=4.27]\u001b[A\n",
      "Iteration:  87%|████████▋ | 128/147 [03:36<00:32,  1.72s/it, loss=4.58]\u001b[A\n",
      "Iteration:  88%|████████▊ | 129/147 [03:37<00:30,  1.72s/it, loss=4.58]\u001b[A\n",
      "Iteration:  88%|████████▊ | 129/147 [03:38<00:30,  1.72s/it, loss=4.16]\u001b[A\n",
      "Iteration:  88%|████████▊ | 130/147 [03:39<00:29,  1.72s/it, loss=4.16]\u001b[A\n",
      "Iteration:  88%|████████▊ | 130/147 [03:40<00:29,  1.72s/it, loss=4.92]\u001b[A\n",
      "Iteration:  89%|████████▉ | 131/147 [03:40<00:27,  1.72s/it, loss=4.92]\u001b[A\n",
      "Iteration:  89%|████████▉ | 131/147 [03:42<00:27,  1.72s/it, loss=3.87]\u001b[A\n",
      "Iteration:  90%|████████▉ | 132/147 [03:42<00:25,  1.72s/it, loss=3.87]\u001b[A\n",
      "Iteration:  90%|████████▉ | 132/147 [03:43<00:25,  1.72s/it, loss=4.22]\u001b[A\n",
      "Iteration:  90%|█████████ | 133/147 [03:44<00:24,  1.72s/it, loss=4.22]\u001b[A\n",
      "Iteration:  90%|█████████ | 133/147 [03:45<00:24,  1.72s/it, loss=4.37]\u001b[A\n",
      "Iteration:  91%|█████████ | 134/147 [03:46<00:22,  1.72s/it, loss=4.37]\u001b[A\n",
      "Iteration:  91%|█████████ | 134/147 [03:47<00:22,  1.72s/it, loss=4.43]\u001b[A\n",
      "Iteration:  92%|█████████▏| 135/147 [03:47<00:20,  1.72s/it, loss=4.43]\u001b[A\n",
      "Iteration:  92%|█████████▏| 135/147 [03:48<00:20,  1.72s/it, loss=4.06]\u001b[A\n",
      "Iteration:  93%|█████████▎| 136/147 [03:49<00:18,  1.72s/it, loss=4.06]\u001b[A\n",
      "Iteration:  93%|█████████▎| 136/147 [03:50<00:18,  1.72s/it, loss=4.7] \u001b[A\n",
      "Iteration:  93%|█████████▎| 137/147 [03:51<00:17,  1.72s/it, loss=4.7]\u001b[A\n",
      "Iteration:  93%|█████████▎| 137/147 [03:52<00:17,  1.72s/it, loss=4.1]\u001b[A\n",
      "Iteration:  94%|█████████▍| 138/147 [03:52<00:15,  1.72s/it, loss=4.1]\u001b[A\n",
      "Iteration:  94%|█████████▍| 138/147 [03:54<00:15,  1.72s/it, loss=4.19]\u001b[A\n",
      "Iteration:  95%|█████████▍| 139/147 [03:54<00:13,  1.73s/it, loss=4.19]\u001b[A\n",
      "Iteration:  95%|█████████▍| 139/147 [03:55<00:13,  1.73s/it, loss=3.8] \u001b[A\n",
      "Iteration:  95%|█████████▌| 140/147 [03:56<00:12,  1.73s/it, loss=3.8]\u001b[A\n",
      "Iteration:  95%|█████████▌| 140/147 [03:57<00:12,  1.73s/it, loss=4.47]\u001b[A\n",
      "Iteration:  96%|█████████▌| 141/147 [03:58<00:10,  1.73s/it, loss=4.47]\u001b[A\n",
      "Iteration:  96%|█████████▌| 141/147 [03:59<00:10,  1.73s/it, loss=4.75]\u001b[A\n",
      "Iteration:  97%|█████████▋| 142/147 [03:59<00:08,  1.74s/it, loss=4.75]\u001b[A\n",
      "Iteration:  97%|█████████▋| 142/147 [04:01<00:08,  1.74s/it, loss=4.59]\u001b[A\n",
      "Iteration:  97%|█████████▋| 143/147 [04:01<00:06,  1.74s/it, loss=4.59]\u001b[A\n",
      "Iteration:  97%|█████████▋| 143/147 [04:02<00:06,  1.74s/it, loss=4.14]\u001b[A\n",
      "Iteration:  98%|█████████▊| 144/147 [04:03<00:05,  1.73s/it, loss=4.14]\u001b[A\n",
      "Iteration:  98%|█████████▊| 144/147 [04:04<00:05,  1.73s/it, loss=4.58]\u001b[A\n",
      "Iteration:  99%|█████████▊| 145/147 [04:05<00:03,  1.73s/it, loss=4.58]\u001b[A\n",
      "Iteration:  99%|█████████▊| 145/147 [04:06<00:03,  1.73s/it, loss=3.96]\u001b[A\n",
      "Iteration:  99%|█████████▉| 146/147 [04:06<00:01,  1.73s/it, loss=3.96]\u001b[A\n",
      "Iteration:  99%|█████████▉| 146/147 [04:07<00:01,  1.73s/it, loss=4.64]\u001b[A\n",
      "Epoch:  20%|██        | 1/5 [04:07<16:31, 247.92s/it]55s/it, loss=4.64]\u001b[A\n",
      "Iteration:   0%|          | 0/147 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   0%|          | 0/147 [00:01<?, ?it/s, loss=4.1]\u001b[A\n",
      "Iteration:   1%|          | 1/147 [00:01<04:24,  1.81s/it, loss=4.1]\u001b[A\n",
      "Iteration:   1%|          | 1/147 [00:03<04:24,  1.81s/it, loss=4.1]\u001b[A\n",
      "Iteration:   1%|▏         | 2/147 [00:03<04:23,  1.82s/it, loss=4.1]\u001b[A\n",
      "Iteration:   1%|▏         | 2/147 [00:04<04:23,  1.82s/it, loss=3.49]\u001b[A\n",
      "Iteration:   2%|▏         | 3/147 [00:05<04:22,  1.82s/it, loss=3.49]\u001b[A\n",
      "Iteration:   2%|▏         | 3/147 [00:06<04:22,  1.82s/it, loss=4.37]\u001b[A\n",
      "Iteration:   3%|▎         | 4/147 [00:07<04:21,  1.83s/it, loss=4.37]\u001b[A\n",
      "Iteration:   3%|▎         | 4/147 [00:08<04:21,  1.83s/it, loss=4.34]\u001b[A\n",
      "Iteration:   3%|▎         | 5/147 [00:09<04:17,  1.82s/it, loss=4.34]\u001b[A\n",
      "Iteration:   3%|▎         | 5/147 [00:10<04:17,  1.82s/it, loss=4.12]\u001b[A\n",
      "Iteration:   4%|▍         | 6/147 [00:10<04:15,  1.81s/it, loss=4.12]\u001b[A\n",
      "Iteration:   4%|▍         | 6/147 [00:12<04:15,  1.81s/it, loss=4.48]\u001b[A\n",
      "Iteration:   5%|▍         | 7/147 [00:12<04:15,  1.82s/it, loss=4.48]\u001b[A\n",
      "Iteration:   5%|▍         | 7/147 [00:14<04:15,  1.82s/it, loss=3.86]\u001b[A\n",
      "Iteration:   5%|▌         | 8/147 [00:14<04:11,  1.81s/it, loss=3.86]\u001b[A\n",
      "Iteration:   5%|▌         | 8/147 [00:15<04:11,  1.81s/it, loss=3.46]\u001b[A\n",
      "Iteration:   6%|▌         | 9/147 [00:16<04:08,  1.80s/it, loss=3.46]\u001b[A\n",
      "Iteration:   6%|▌         | 9/147 [00:17<04:08,  1.80s/it, loss=4.28]\u001b[A\n",
      "Iteration:   7%|▋         | 10/147 [00:18<04:05,  1.79s/it, loss=4.28]\u001b[A\n",
      "Iteration:   7%|▋         | 10/147 [00:19<04:05,  1.79s/it, loss=4.39]\u001b[A\n",
      "Iteration:   7%|▋         | 11/147 [00:19<04:02,  1.78s/it, loss=4.39]\u001b[A\n",
      "Iteration:   7%|▋         | 11/147 [00:21<04:02,  1.78s/it, loss=4]   \u001b[A\n",
      "Iteration:   8%|▊         | 12/147 [00:21<03:59,  1.77s/it, loss=4]\u001b[A\n",
      "Iteration:   8%|▊         | 12/147 [00:22<03:59,  1.77s/it, loss=4.71]\u001b[A\n",
      "Iteration:   9%|▉         | 13/147 [00:23<03:57,  1.77s/it, loss=4.71]\u001b[A\n",
      "Iteration:   9%|▉         | 13/147 [00:24<03:57,  1.77s/it, loss=3.81]\u001b[A\n",
      "Iteration:  10%|▉         | 14/147 [00:25<03:55,  1.77s/it, loss=3.81]\u001b[A\n",
      "Iteration:  10%|▉         | 14/147 [00:26<03:55,  1.77s/it, loss=3.97]\u001b[A\n",
      "Iteration:  10%|█         | 15/147 [00:26<03:53,  1.77s/it, loss=3.97]\u001b[A\n",
      "Iteration:  10%|█         | 15/147 [00:28<03:53,  1.77s/it, loss=4.12]\u001b[A\n",
      "Iteration:  11%|█         | 16/147 [00:28<03:51,  1.77s/it, loss=4.12]\u001b[A\n",
      "Iteration:  11%|█         | 16/147 [00:29<03:51,  1.77s/it, loss=3.76]\u001b[A\n",
      "Iteration:  12%|█▏        | 17/147 [00:30<03:49,  1.77s/it, loss=3.76]\u001b[A\n",
      "Iteration:  12%|█▏        | 17/147 [00:31<03:49,  1.77s/it, loss=3.83]\u001b[A\n",
      "Iteration:  12%|█▏        | 18/147 [00:32<03:47,  1.76s/it, loss=3.83]\u001b[A\n",
      "Iteration:  12%|█▏        | 18/147 [00:33<03:47,  1.76s/it, loss=3.81]\u001b[A\n",
      "Iteration:  13%|█▎        | 19/147 [00:33<03:45,  1.76s/it, loss=3.81]\u001b[A\n",
      "Iteration:  13%|█▎        | 19/147 [00:35<03:45,  1.76s/it, loss=3.73]\u001b[A\n",
      "Iteration:  14%|█▎        | 20/147 [00:35<03:44,  1.76s/it, loss=3.73]\u001b[A\n",
      "Iteration:  14%|█▎        | 20/147 [00:36<03:44,  1.76s/it, loss=3.59]\u001b[A\n",
      "Iteration:  14%|█▍        | 21/147 [00:37<03:42,  1.76s/it, loss=3.59]\u001b[A\n",
      "Iteration:  14%|█▍        | 21/147 [00:38<03:42,  1.76s/it, loss=3.6] \u001b[A\n",
      "Iteration:  15%|█▍        | 22/147 [00:39<03:40,  1.76s/it, loss=3.6]\u001b[A\n",
      "Iteration:  15%|█▍        | 22/147 [00:40<03:40,  1.76s/it, loss=3.58]\u001b[A\n",
      "Iteration:  16%|█▌        | 23/147 [00:40<03:38,  1.76s/it, loss=3.58]\u001b[A\n",
      "Iteration:  16%|█▌        | 23/147 [00:42<03:38,  1.76s/it, loss=4.21]\u001b[A\n",
      "Iteration:  16%|█▋        | 24/147 [00:42<03:36,  1.76s/it, loss=4.21]\u001b[A\n",
      "Iteration:  16%|█▋        | 24/147 [00:44<03:36,  1.76s/it, loss=4.18]\u001b[A\n",
      "Iteration:  17%|█▋        | 25/147 [00:44<03:35,  1.76s/it, loss=4.18]\u001b[A\n",
      "Iteration:  17%|█▋        | 25/147 [00:45<03:35,  1.76s/it, loss=3.29]\u001b[A\n",
      "Iteration:  18%|█▊        | 26/147 [00:46<03:33,  1.77s/it, loss=3.29]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  18%|█▊        | 26/147 [00:47<03:33,  1.77s/it, loss=3.69]\u001b[A\n",
      "Iteration:  18%|█▊        | 27/147 [00:48<03:32,  1.77s/it, loss=3.69]\u001b[A\n",
      "Iteration:  18%|█▊        | 27/147 [00:49<03:32,  1.77s/it, loss=3.64]\u001b[A\n",
      "Iteration:  19%|█▉        | 28/147 [00:49<03:29,  1.76s/it, loss=3.64]\u001b[A\n",
      "Iteration:  19%|█▉        | 28/147 [00:51<03:29,  1.76s/it, loss=4.2] \u001b[A\n",
      "Iteration:  20%|█▉        | 29/147 [00:51<03:27,  1.76s/it, loss=4.2]\u001b[A\n",
      "Iteration:  20%|█▉        | 29/147 [00:52<03:27,  1.76s/it, loss=4.15]\u001b[A\n",
      "Iteration:  20%|██        | 30/147 [00:53<03:25,  1.76s/it, loss=4.15]\u001b[A\n",
      "Iteration:  20%|██        | 30/147 [00:54<03:25,  1.76s/it, loss=3.74]\u001b[A\n",
      "Iteration:  21%|██        | 31/147 [00:55<03:23,  1.76s/it, loss=3.74]\u001b[A\n",
      "Iteration:  21%|██        | 31/147 [00:56<03:23,  1.76s/it, loss=3.86]\u001b[A\n",
      "Iteration:  22%|██▏       | 32/147 [00:56<03:21,  1.75s/it, loss=3.86]\u001b[A\n",
      "Iteration:  22%|██▏       | 32/147 [00:58<03:21,  1.75s/it, loss=3.66]\u001b[A\n",
      "Iteration:  22%|██▏       | 33/147 [00:58<03:19,  1.75s/it, loss=3.66]\u001b[A\n",
      "Iteration:  22%|██▏       | 33/147 [00:59<03:19,  1.75s/it, loss=3.91]\u001b[A\n",
      "Iteration:  23%|██▎       | 34/147 [01:00<03:18,  1.75s/it, loss=3.91]\u001b[A\n",
      "Iteration:  23%|██▎       | 34/147 [01:01<03:18,  1.75s/it, loss=3.53]\u001b[A\n",
      "Iteration:  24%|██▍       | 35/147 [01:02<03:16,  1.75s/it, loss=3.53]\u001b[A\n",
      "Iteration:  24%|██▍       | 35/147 [01:03<03:16,  1.75s/it, loss=3.94]\u001b[A\n",
      "Iteration:  24%|██▍       | 36/147 [01:03<03:14,  1.75s/it, loss=3.94]\u001b[A\n",
      "Iteration:  24%|██▍       | 36/147 [01:05<03:14,  1.75s/it, loss=3.66]\u001b[A\n",
      "Iteration:  25%|██▌       | 37/147 [01:05<03:12,  1.75s/it, loss=3.66]\u001b[A\n",
      "Iteration:  25%|██▌       | 37/147 [01:06<03:12,  1.75s/it, loss=4.11]\u001b[A\n",
      "Iteration:  26%|██▌       | 38/147 [01:07<03:11,  1.75s/it, loss=4.11]\u001b[A\n",
      "Iteration:  26%|██▌       | 38/147 [01:08<03:11,  1.75s/it, loss=4.37]\u001b[A\n",
      "Iteration:  27%|██▋       | 39/147 [01:09<03:09,  1.75s/it, loss=4.37]\u001b[A\n",
      "Iteration:  27%|██▋       | 39/147 [01:10<03:09,  1.75s/it, loss=3.81]\u001b[A\n",
      "Iteration:  27%|██▋       | 40/147 [01:10<03:07,  1.75s/it, loss=3.81]\u001b[A\n",
      "Iteration:  27%|██▋       | 40/147 [01:12<03:07,  1.75s/it, loss=3.69]\u001b[A\n",
      "Iteration:  28%|██▊       | 41/147 [01:12<03:06,  1.76s/it, loss=3.69]\u001b[A\n",
      "Iteration:  28%|██▊       | 41/147 [01:13<03:06,  1.76s/it, loss=3.94]\u001b[A\n",
      "Iteration:  29%|██▊       | 42/147 [01:14<03:04,  1.76s/it, loss=3.94]\u001b[A\n",
      "Iteration:  29%|██▊       | 42/147 [01:15<03:04,  1.76s/it, loss=3.58]\u001b[A\n",
      "Iteration:  29%|██▉       | 43/147 [01:16<03:03,  1.77s/it, loss=3.58]\u001b[A\n",
      "Iteration:  29%|██▉       | 43/147 [01:17<03:03,  1.77s/it, loss=3.9] \u001b[A\n",
      "Iteration:  30%|██▉       | 44/147 [01:17<03:01,  1.76s/it, loss=3.9]\u001b[A\n",
      "Iteration:  30%|██▉       | 44/147 [01:19<03:01,  1.76s/it, loss=4.11]\u001b[A\n",
      "Iteration:  31%|███       | 45/147 [01:19<02:59,  1.76s/it, loss=4.11]\u001b[A\n",
      "Iteration:  31%|███       | 45/147 [01:20<02:59,  1.76s/it, loss=3.96]\u001b[A\n",
      "Iteration:  31%|███▏      | 46/147 [01:21<02:57,  1.76s/it, loss=3.96]\u001b[A\n",
      "Iteration:  31%|███▏      | 46/147 [01:22<02:57,  1.76s/it, loss=3.58]\u001b[A\n",
      "Iteration:  32%|███▏      | 47/147 [01:23<02:55,  1.75s/it, loss=3.58]\u001b[A\n",
      "Iteration:  32%|███▏      | 47/147 [01:24<02:55,  1.75s/it, loss=3.8] \u001b[A\n",
      "Iteration:  33%|███▎      | 48/147 [01:24<02:53,  1.76s/it, loss=3.8]\u001b[A\n",
      "Iteration:  33%|███▎      | 48/147 [01:26<02:53,  1.76s/it, loss=3.79]\u001b[A\n",
      "Iteration:  33%|███▎      | 49/147 [01:26<02:52,  1.76s/it, loss=3.79]\u001b[A\n",
      "Iteration:  33%|███▎      | 49/147 [01:27<02:52,  1.76s/it, loss=4.41]\u001b[A\n",
      "Iteration:  34%|███▍      | 50/147 [01:28<02:50,  1.76s/it, loss=4.41]\u001b[A\n",
      "Iteration:  34%|███▍      | 50/147 [01:29<02:50,  1.76s/it, loss=4.68]\u001b[A\n",
      "Iteration:  35%|███▍      | 51/147 [01:30<02:48,  1.76s/it, loss=4.68]\u001b[A\n",
      "Iteration:  35%|███▍      | 51/147 [01:31<02:48,  1.76s/it, loss=3.93]\u001b[A\n",
      "Iteration:  35%|███▌      | 52/147 [01:31<02:46,  1.76s/it, loss=3.93]\u001b[A\n",
      "Iteration:  35%|███▌      | 52/147 [01:33<02:46,  1.76s/it, loss=3.8] \u001b[A\n",
      "Iteration:  36%|███▌      | 53/147 [01:33<02:45,  1.76s/it, loss=3.8]\u001b[A\n",
      "Iteration:  36%|███▌      | 53/147 [01:34<02:45,  1.76s/it, loss=3.88]\u001b[A\n",
      "Iteration:  37%|███▋      | 54/147 [01:35<02:43,  1.75s/it, loss=3.88]\u001b[A\n",
      "Iteration:  37%|███▋      | 54/147 [01:36<02:43,  1.75s/it, loss=4.37]\u001b[A\n",
      "Iteration:  37%|███▋      | 55/147 [01:37<02:41,  1.76s/it, loss=4.37]\u001b[A\n",
      "Iteration:  37%|███▋      | 55/147 [01:38<02:41,  1.76s/it, loss=3.95]\u001b[A\n",
      "Iteration:  38%|███▊      | 56/147 [01:38<02:39,  1.76s/it, loss=3.95]\u001b[A\n",
      "Iteration:  38%|███▊      | 56/147 [01:40<02:39,  1.76s/it, loss=3.86]\u001b[A\n",
      "Iteration:  39%|███▉      | 57/147 [01:40<02:38,  1.76s/it, loss=3.86]\u001b[A\n",
      "Iteration:  39%|███▉      | 57/147 [01:42<02:38,  1.76s/it, loss=3.88]\u001b[A\n",
      "Iteration:  39%|███▉      | 58/147 [01:42<02:37,  1.77s/it, loss=3.88]\u001b[A\n",
      "Iteration:  39%|███▉      | 58/147 [01:43<02:37,  1.77s/it, loss=3.77]\u001b[A\n",
      "Iteration:  40%|████      | 59/147 [01:44<02:35,  1.77s/it, loss=3.77]\u001b[A\n",
      "Iteration:  40%|████      | 59/147 [01:45<02:35,  1.77s/it, loss=3.94]\u001b[A\n",
      "Iteration:  41%|████      | 60/147 [01:46<02:34,  1.77s/it, loss=3.94]\u001b[A\n",
      "Iteration:  41%|████      | 60/147 [01:47<02:34,  1.77s/it, loss=4.45]\u001b[A\n",
      "Iteration:  41%|████▏     | 61/147 [01:47<02:32,  1.77s/it, loss=4.45]\u001b[A\n",
      "Iteration:  41%|████▏     | 61/147 [01:49<02:32,  1.77s/it, loss=3.65]\u001b[A\n",
      "Iteration:  42%|████▏     | 62/147 [01:49<02:30,  1.77s/it, loss=3.65]\u001b[A\n",
      "Iteration:  42%|████▏     | 62/147 [01:50<02:30,  1.77s/it, loss=3.82]\u001b[A\n",
      "Iteration:  43%|████▎     | 63/147 [01:51<02:29,  1.78s/it, loss=3.82]\u001b[A\n",
      "Iteration:  43%|████▎     | 63/147 [01:52<02:29,  1.78s/it, loss=4.17]\u001b[A\n",
      "Iteration:  44%|████▎     | 64/147 [01:53<02:27,  1.78s/it, loss=4.17]\u001b[A\n",
      "Iteration:  44%|████▎     | 64/147 [01:54<02:27,  1.78s/it, loss=3.51]\u001b[A\n",
      "Iteration:  44%|████▍     | 65/147 [01:54<02:25,  1.78s/it, loss=3.51]\u001b[A\n",
      "Iteration:  44%|████▍     | 65/147 [01:56<02:25,  1.78s/it, loss=3.83]\u001b[A\n",
      "Iteration:  45%|████▍     | 66/147 [01:56<02:23,  1.78s/it, loss=3.83]\u001b[A\n",
      "Iteration:  45%|████▍     | 66/147 [01:58<02:23,  1.78s/it, loss=4.36]\u001b[A\n",
      "Iteration:  46%|████▌     | 67/147 [01:58<02:22,  1.78s/it, loss=4.36]\u001b[A\n",
      "Iteration:  46%|████▌     | 67/147 [01:59<02:22,  1.78s/it, loss=3.78]\u001b[A\n",
      "Iteration:  46%|████▋     | 68/147 [02:00<02:20,  1.78s/it, loss=3.78]\u001b[A\n",
      "Iteration:  46%|████▋     | 68/147 [02:01<02:20,  1.78s/it, loss=3.62]\u001b[A\n",
      "Iteration:  47%|████▋     | 69/147 [02:02<02:18,  1.78s/it, loss=3.62]\u001b[A\n",
      "Iteration:  47%|████▋     | 69/147 [02:03<02:18,  1.78s/it, loss=4.03]\u001b[A\n",
      "Iteration:  48%|████▊     | 70/147 [02:03<02:17,  1.79s/it, loss=4.03]\u001b[A\n",
      "Iteration:  48%|████▊     | 70/147 [02:05<02:17,  1.79s/it, loss=3.11]\u001b[A\n",
      "Iteration:  48%|████▊     | 71/147 [02:05<02:16,  1.79s/it, loss=3.11]\u001b[A\n",
      "Iteration:  48%|████▊     | 71/147 [02:07<02:16,  1.79s/it, loss=3.98]\u001b[A\n",
      "Iteration:  49%|████▉     | 72/147 [02:07<02:14,  1.79s/it, loss=3.98]\u001b[A\n",
      "Iteration:  49%|████▉     | 72/147 [02:08<02:14,  1.79s/it, loss=3.64]\u001b[A\n",
      "Iteration:  50%|████▉     | 73/147 [02:09<02:12,  1.79s/it, loss=3.64]\u001b[A\n",
      "Iteration:  50%|████▉     | 73/147 [02:10<02:12,  1.79s/it, loss=4]   \u001b[A\n",
      "Iteration:  50%|█████     | 74/147 [02:11<02:10,  1.79s/it, loss=4]\u001b[A\n",
      "Iteration:  50%|█████     | 74/147 [02:12<02:10,  1.79s/it, loss=3.92]\u001b[A\n",
      "Iteration:  51%|█████     | 75/147 [02:12<02:09,  1.80s/it, loss=3.92]\u001b[A\n",
      "Iteration:  51%|█████     | 75/147 [02:14<02:09,  1.80s/it, loss=3.01]\u001b[A\n",
      "Iteration:  52%|█████▏    | 76/147 [02:14<02:07,  1.80s/it, loss=3.01]\u001b[A\n",
      "Iteration:  52%|█████▏    | 76/147 [02:15<02:07,  1.80s/it, loss=3.79]\u001b[A\n",
      "Iteration:  52%|█████▏    | 77/147 [02:16<02:05,  1.80s/it, loss=3.79]\u001b[A\n",
      "Iteration:  52%|█████▏    | 77/147 [02:17<02:05,  1.80s/it, loss=3.88]\u001b[A\n",
      "Iteration:  53%|█████▎    | 78/147 [02:18<02:04,  1.80s/it, loss=3.88]\u001b[A\n",
      "Iteration:  53%|█████▎    | 78/147 [02:19<02:04,  1.80s/it, loss=4.16]\u001b[A\n",
      "Iteration:  54%|█████▎    | 79/147 [02:20<02:02,  1.80s/it, loss=4.16]\u001b[A\n",
      "Iteration:  54%|█████▎    | 79/147 [02:21<02:02,  1.80s/it, loss=4.5] \u001b[A\n",
      "Iteration:  54%|█████▍    | 80/147 [02:21<02:00,  1.80s/it, loss=4.5]\u001b[A\n",
      "Iteration:  54%|█████▍    | 80/147 [02:23<02:00,  1.80s/it, loss=3.61]\u001b[A\n",
      "Iteration:  55%|█████▌    | 81/147 [02:23<01:58,  1.80s/it, loss=3.61]\u001b[A\n",
      "Iteration:  55%|█████▌    | 81/147 [02:24<01:58,  1.80s/it, loss=4.32]\u001b[A\n",
      "Iteration:  56%|█████▌    | 82/147 [02:25<01:56,  1.80s/it, loss=4.32]\u001b[A\n",
      "Iteration:  56%|█████▌    | 82/147 [02:26<01:56,  1.80s/it, loss=3.7] \u001b[A\n",
      "Iteration:  56%|█████▋    | 83/147 [02:27<01:55,  1.80s/it, loss=3.7]\u001b[A\n",
      "Iteration:  56%|█████▋    | 83/147 [02:28<01:55,  1.80s/it, loss=3.58]\u001b[A\n",
      "Iteration:  57%|█████▋    | 84/147 [02:29<01:53,  1.80s/it, loss=3.58]\u001b[A\n",
      "Iteration:  57%|█████▋    | 84/147 [02:30<01:53,  1.80s/it, loss=3.5] \u001b[A\n",
      "Iteration:  58%|█████▊    | 85/147 [02:30<01:51,  1.80s/it, loss=3.5]\u001b[A\n",
      "Iteration:  58%|█████▊    | 85/147 [02:32<01:51,  1.80s/it, loss=4.26]\u001b[A\n",
      "Iteration:  59%|█████▊    | 86/147 [02:32<01:49,  1.80s/it, loss=4.26]\u001b[A\n",
      "Iteration:  59%|█████▊    | 86/147 [02:33<01:49,  1.80s/it, loss=3.19]\u001b[A\n",
      "Iteration:  59%|█████▉    | 87/147 [02:34<01:48,  1.80s/it, loss=3.19]\u001b[A\n",
      "Iteration:  59%|█████▉    | 87/147 [02:35<01:48,  1.80s/it, loss=4.16]\u001b[A\n",
      "Iteration:  60%|█████▉    | 88/147 [02:36<01:46,  1.80s/it, loss=4.16]\u001b[A\n",
      "Iteration:  60%|█████▉    | 88/147 [02:37<01:46,  1.80s/it, loss=4.06]\u001b[A\n",
      "Iteration:  61%|██████    | 89/147 [02:38<01:44,  1.80s/it, loss=4.06]\u001b[A\n",
      "Iteration:  61%|██████    | 89/147 [02:39<01:44,  1.80s/it, loss=3.64]\u001b[A\n",
      "Iteration:  61%|██████    | 90/147 [02:39<01:42,  1.80s/it, loss=3.64]\u001b[A\n",
      "Iteration:  61%|██████    | 90/147 [02:41<01:42,  1.80s/it, loss=3.66]\u001b[A\n",
      "Iteration:  62%|██████▏   | 91/147 [02:41<01:40,  1.80s/it, loss=3.66]\u001b[A\n",
      "Iteration:  62%|██████▏   | 91/147 [02:43<01:40,  1.80s/it, loss=3.83]\u001b[A\n",
      "Iteration:  63%|██████▎   | 92/147 [02:43<01:39,  1.80s/it, loss=3.83]\u001b[A\n",
      "Iteration:  63%|██████▎   | 92/147 [02:44<01:39,  1.80s/it, loss=3.93]\u001b[A\n",
      "Iteration:  63%|██████▎   | 93/147 [02:45<01:37,  1.80s/it, loss=3.93]\u001b[A\n",
      "Iteration:  63%|██████▎   | 93/147 [02:46<01:37,  1.80s/it, loss=4.08]\u001b[A\n",
      "Iteration:  64%|██████▍   | 94/147 [02:47<01:35,  1.80s/it, loss=4.08]\u001b[A\n",
      "Iteration:  64%|██████▍   | 94/147 [02:48<01:35,  1.80s/it, loss=4.29]\u001b[A\n",
      "Iteration:  65%|██████▍   | 95/147 [02:48<01:33,  1.80s/it, loss=4.29]\u001b[A\n",
      "Iteration:  65%|██████▍   | 95/147 [02:50<01:33,  1.80s/it, loss=4]   \u001b[A\n",
      "Iteration:  65%|██████▌   | 96/147 [02:50<01:31,  1.80s/it, loss=4]\u001b[A\n",
      "Iteration:  65%|██████▌   | 96/147 [02:52<01:31,  1.80s/it, loss=3.74]\u001b[A\n",
      "Iteration:  66%|██████▌   | 97/147 [02:52<01:30,  1.80s/it, loss=3.74]\u001b[A\n",
      "Iteration:  66%|██████▌   | 97/147 [02:53<01:30,  1.80s/it, loss=3.61]\u001b[A\n",
      "Iteration:  67%|██████▋   | 98/147 [02:54<01:28,  1.80s/it, loss=3.61]\u001b[A\n",
      "Iteration:  67%|██████▋   | 98/147 [02:55<01:28,  1.80s/it, loss=4.17]\u001b[A\n",
      "Iteration:  67%|██████▋   | 99/147 [02:56<01:26,  1.80s/it, loss=4.17]\u001b[A\n",
      "Iteration:  67%|██████▋   | 99/147 [02:57<01:26,  1.80s/it, loss=4]   \u001b[A\n",
      "Iteration:  68%|██████▊   | 100/147 [02:57<01:24,  1.80s/it, loss=4]\u001b[A\n",
      "Iteration:  68%|██████▊   | 100/147 [02:59<01:24,  1.80s/it, loss=3.95]\u001b[A\n",
      "Iteration:  69%|██████▊   | 101/147 [02:59<01:22,  1.80s/it, loss=3.95]\u001b[A\n",
      "Iteration:  69%|██████▊   | 101/147 [03:01<01:22,  1.80s/it, loss=3.54]\u001b[A\n",
      "Iteration:  69%|██████▉   | 102/147 [03:01<01:21,  1.80s/it, loss=3.54]\u001b[A\n",
      "Iteration:  69%|██████▉   | 102/147 [03:02<01:21,  1.80s/it, loss=3.89]\u001b[A\n",
      "Iteration:  70%|███████   | 103/147 [03:03<01:19,  1.80s/it, loss=3.89]\u001b[A\n",
      "Iteration:  70%|███████   | 103/147 [03:04<01:19,  1.80s/it, loss=3.87]\u001b[A\n",
      "Iteration:  71%|███████   | 104/147 [03:05<01:17,  1.80s/it, loss=3.87]\u001b[A\n",
      "Iteration:  71%|███████   | 104/147 [03:06<01:17,  1.80s/it, loss=3.55]\u001b[A\n",
      "Iteration:  71%|███████▏  | 105/147 [03:06<01:15,  1.80s/it, loss=3.55]\u001b[A\n",
      "Iteration:  71%|███████▏  | 105/147 [03:08<01:15,  1.80s/it, loss=4.29]\u001b[A\n",
      "Iteration:  72%|███████▏  | 106/147 [03:08<01:13,  1.80s/it, loss=4.29]\u001b[A\n",
      "Iteration:  72%|███████▏  | 106/147 [03:10<01:13,  1.80s/it, loss=3.67]\u001b[A\n",
      "Iteration:  73%|███████▎  | 107/147 [03:10<01:12,  1.81s/it, loss=3.67]\u001b[A\n",
      "Iteration:  73%|███████▎  | 107/147 [03:11<01:12,  1.81s/it, loss=4.14]\u001b[A\n",
      "Iteration:  73%|███████▎  | 108/147 [03:12<01:11,  1.84s/it, loss=4.14]\u001b[A\n",
      "Iteration:  73%|███████▎  | 108/147 [03:13<01:11,  1.84s/it, loss=3.5] \u001b[A\n",
      "Iteration:  74%|███████▍  | 109/147 [03:14<01:10,  1.85s/it, loss=3.5]\u001b[A\n",
      "Iteration:  74%|███████▍  | 109/147 [03:15<01:10,  1.85s/it, loss=3.76]\u001b[A\n",
      "Iteration:  75%|███████▍  | 110/147 [03:16<01:07,  1.84s/it, loss=3.76]\u001b[A\n",
      "Iteration:  75%|███████▍  | 110/147 [03:17<01:07,  1.84s/it, loss=3.4] \u001b[A\n",
      "Iteration:  76%|███████▌  | 111/147 [03:17<01:05,  1.83s/it, loss=3.4]\u001b[A\n",
      "Iteration:  76%|███████▌  | 111/147 [03:19<01:05,  1.83s/it, loss=3.84]\u001b[A\n",
      "Iteration:  76%|███████▌  | 112/147 [03:19<01:04,  1.83s/it, loss=3.84]\u001b[A\n",
      "Iteration:  76%|███████▌  | 112/147 [03:21<01:04,  1.83s/it, loss=3.46]\u001b[A\n",
      "Iteration:  77%|███████▋  | 113/147 [03:21<01:02,  1.83s/it, loss=3.46]\u001b[A\n",
      "Iteration:  77%|███████▋  | 113/147 [03:22<01:02,  1.83s/it, loss=4.21]\u001b[A\n",
      "Iteration:  78%|███████▊  | 114/147 [03:23<01:00,  1.82s/it, loss=4.21]\u001b[A\n",
      "Iteration:  78%|███████▊  | 114/147 [03:24<01:00,  1.82s/it, loss=2.93]\u001b[A\n",
      "Iteration:  78%|███████▊  | 115/147 [03:25<00:58,  1.82s/it, loss=2.93]\u001b[A\n",
      "Iteration:  78%|███████▊  | 115/147 [03:26<00:58,  1.82s/it, loss=3.79]\u001b[A\n",
      "Iteration:  79%|███████▉  | 116/147 [03:27<00:56,  1.82s/it, loss=3.79]\u001b[A\n",
      "Iteration:  79%|███████▉  | 116/147 [03:28<00:56,  1.82s/it, loss=3.47]\u001b[A\n",
      "Iteration:  80%|███████▉  | 117/147 [03:28<00:54,  1.83s/it, loss=3.47]\u001b[A\n",
      "Iteration:  80%|███████▉  | 117/147 [03:30<00:54,  1.83s/it, loss=3.61]\u001b[A\n",
      "Iteration:  80%|████████  | 118/147 [03:30<00:53,  1.85s/it, loss=3.61]\u001b[A\n",
      "Iteration:  80%|████████  | 118/147 [03:32<00:53,  1.85s/it, loss=3.48]\u001b[A\n",
      "Iteration:  81%|████████  | 119/147 [03:32<00:51,  1.84s/it, loss=3.48]\u001b[A\n",
      "Iteration:  81%|████████  | 119/147 [03:33<00:51,  1.84s/it, loss=3.42]\u001b[A\n",
      "Iteration:  82%|████████▏ | 120/147 [03:34<00:49,  1.84s/it, loss=3.42]\u001b[A\n",
      "Iteration:  82%|████████▏ | 120/147 [03:35<00:49,  1.84s/it, loss=3.92]\u001b[A\n",
      "Iteration:  82%|████████▏ | 121/147 [03:36<00:47,  1.83s/it, loss=3.92]\u001b[A\n",
      "Iteration:  82%|████████▏ | 121/147 [03:37<00:47,  1.83s/it, loss=3.26]\u001b[A\n",
      "Iteration:  83%|████████▎ | 122/147 [03:38<00:45,  1.82s/it, loss=3.26]\u001b[A\n",
      "Iteration:  83%|████████▎ | 122/147 [03:39<00:45,  1.82s/it, loss=3.4] \u001b[A\n",
      "Iteration:  84%|████████▎ | 123/147 [03:39<00:43,  1.82s/it, loss=3.4]\u001b[A\n",
      "Iteration:  84%|████████▎ | 123/147 [03:41<00:43,  1.82s/it, loss=3.96]\u001b[A\n",
      "Iteration:  84%|████████▍ | 124/147 [03:41<00:41,  1.82s/it, loss=3.96]\u001b[A\n",
      "Iteration:  84%|████████▍ | 124/147 [03:43<00:41,  1.82s/it, loss=3.8] \u001b[A\n",
      "Iteration:  85%|████████▌ | 125/147 [03:43<00:39,  1.82s/it, loss=3.8]\u001b[A\n",
      "Iteration:  85%|████████▌ | 125/147 [03:44<00:39,  1.82s/it, loss=3.72]\u001b[A\n",
      "Iteration:  86%|████████▌ | 126/147 [03:45<00:38,  1.81s/it, loss=3.72]\u001b[A\n",
      "Iteration:  86%|████████▌ | 126/147 [03:46<00:38,  1.81s/it, loss=3.48]\u001b[A\n",
      "Iteration:  86%|████████▋ | 127/147 [03:47<00:36,  1.81s/it, loss=3.48]\u001b[A\n",
      "Iteration:  86%|████████▋ | 127/147 [03:48<00:36,  1.81s/it, loss=3.3] \u001b[A\n",
      "Iteration:  87%|████████▋ | 128/147 [03:48<00:34,  1.81s/it, loss=3.3]\u001b[A\n",
      "Iteration:  87%|████████▋ | 128/147 [03:50<00:34,  1.81s/it, loss=3.42]\u001b[A\n",
      "Iteration:  88%|████████▊ | 129/147 [03:50<00:32,  1.82s/it, loss=3.42]\u001b[A\n",
      "Iteration:  88%|████████▊ | 129/147 [03:52<00:32,  1.82s/it, loss=3.79]\u001b[A\n",
      "Iteration:  88%|████████▊ | 130/147 [03:52<00:30,  1.82s/it, loss=3.79]\u001b[A\n",
      "Iteration:  88%|████████▊ | 130/147 [03:53<00:30,  1.82s/it, loss=3.5] \u001b[A\n",
      "Iteration:  89%|████████▉ | 131/147 [03:54<00:29,  1.82s/it, loss=3.5]\u001b[A\n",
      "Iteration:  89%|████████▉ | 131/147 [03:55<00:29,  1.82s/it, loss=4.09]\u001b[A\n",
      "Iteration:  90%|████████▉ | 132/147 [03:56<00:27,  1.82s/it, loss=4.09]\u001b[A\n",
      "Iteration:  90%|████████▉ | 132/147 [03:57<00:27,  1.82s/it, loss=4.08]\u001b[A\n",
      "Iteration:  90%|█████████ | 133/147 [03:58<00:25,  1.82s/it, loss=4.08]\u001b[A\n",
      "Iteration:  90%|█████████ | 133/147 [03:59<00:25,  1.82s/it, loss=4.09]\u001b[A\n",
      "Iteration:  91%|█████████ | 134/147 [03:59<00:23,  1.82s/it, loss=4.09]\u001b[A\n",
      "Iteration:  91%|█████████ | 134/147 [04:01<00:23,  1.82s/it, loss=4.26]\u001b[A\n",
      "Iteration:  92%|█████████▏| 135/147 [04:01<00:21,  1.82s/it, loss=4.26]\u001b[A\n",
      "Iteration:  92%|█████████▏| 135/147 [04:02<00:21,  1.82s/it, loss=3.5] \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  93%|█████████▎| 136/147 [04:03<00:19,  1.81s/it, loss=3.5]\u001b[A\n",
      "Iteration:  93%|█████████▎| 136/147 [04:04<00:19,  1.81s/it, loss=4.11]\u001b[A\n",
      "Iteration:  93%|█████████▎| 137/147 [04:05<00:18,  1.81s/it, loss=4.11]\u001b[A\n",
      "Iteration:  93%|█████████▎| 137/147 [04:06<00:18,  1.81s/it, loss=3.26]\u001b[A\n",
      "Iteration:  94%|█████████▍| 138/147 [04:07<00:16,  1.81s/it, loss=3.26]\u001b[A\n",
      "Iteration:  94%|█████████▍| 138/147 [04:08<00:16,  1.81s/it, loss=3.4] \u001b[A\n",
      "Iteration:  95%|█████████▍| 139/147 [04:08<00:14,  1.82s/it, loss=3.4]\u001b[A\n",
      "Iteration:  95%|█████████▍| 139/147 [04:10<00:14,  1.82s/it, loss=3.47]\u001b[A\n",
      "Iteration:  95%|█████████▌| 140/147 [04:10<00:12,  1.85s/it, loss=3.47]\u001b[A\n",
      "Iteration:  95%|█████████▌| 140/147 [04:12<00:12,  1.85s/it, loss=3.04]\u001b[A\n",
      "Iteration:  96%|█████████▌| 141/147 [04:12<00:11,  1.84s/it, loss=3.04]\u001b[A\n",
      "Iteration:  96%|█████████▌| 141/147 [04:13<00:11,  1.84s/it, loss=4.21]\u001b[A\n",
      "Iteration:  97%|█████████▋| 142/147 [04:14<00:09,  1.83s/it, loss=4.21]\u001b[A\n",
      "Iteration:  97%|█████████▋| 142/147 [04:15<00:09,  1.83s/it, loss=3.96]\u001b[A\n",
      "Iteration:  97%|█████████▋| 143/147 [04:16<00:07,  1.82s/it, loss=3.96]\u001b[A\n",
      "Iteration:  97%|█████████▋| 143/147 [04:17<00:07,  1.82s/it, loss=3.69]\u001b[A\n",
      "Iteration:  98%|█████████▊| 144/147 [04:18<00:05,  1.82s/it, loss=3.69]\u001b[A\n",
      "Iteration:  98%|█████████▊| 144/147 [04:19<00:05,  1.82s/it, loss=3.9] \u001b[A\n",
      "Iteration:  99%|█████████▊| 145/147 [04:19<00:03,  1.81s/it, loss=3.9]\u001b[A\n",
      "Iteration:  99%|█████████▊| 145/147 [04:21<00:03,  1.81s/it, loss=3.38]\u001b[A\n",
      "Iteration:  99%|█████████▉| 146/147 [04:21<00:01,  1.81s/it, loss=3.38]\u001b[A\n",
      "Iteration:  99%|█████████▉| 146/147 [04:22<00:01,  1.81s/it, loss=3.88]\u001b[A\n",
      "Epoch:  40%|████      | 2/5 [08:30<12:37, 252.41s/it]62s/it, loss=3.88]\u001b[A\n",
      "Iteration:   0%|          | 0/147 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   0%|          | 0/147 [00:01<?, ?it/s, loss=3.37]\u001b[A\n",
      "Iteration:   1%|          | 1/147 [00:01<04:22,  1.80s/it, loss=3.37]\u001b[A\n",
      "Iteration:   1%|          | 1/147 [00:03<04:22,  1.80s/it, loss=3.57]\u001b[A\n",
      "Iteration:   1%|▏         | 2/147 [00:03<04:20,  1.80s/it, loss=3.57]\u001b[A\n",
      "Iteration:   1%|▏         | 2/147 [00:04<04:20,  1.80s/it, loss=3.12]\u001b[A\n",
      "Iteration:   2%|▏         | 3/147 [00:05<04:18,  1.80s/it, loss=3.12]\u001b[A\n",
      "Iteration:   2%|▏         | 3/147 [00:06<04:18,  1.80s/it, loss=3.54]\u001b[A\n",
      "Iteration:   3%|▎         | 4/147 [00:07<04:17,  1.80s/it, loss=3.54]\u001b[A\n",
      "Iteration:   3%|▎         | 4/147 [00:08<04:17,  1.80s/it, loss=3.42]\u001b[A\n",
      "Iteration:   3%|▎         | 5/147 [00:09<04:15,  1.80s/it, loss=3.42]\u001b[A\n",
      "Iteration:   3%|▎         | 5/147 [00:10<04:15,  1.80s/it, loss=3.56]\u001b[A\n",
      "Iteration:   4%|▍         | 6/147 [00:10<04:14,  1.80s/it, loss=3.56]\u001b[A\n",
      "Iteration:   4%|▍         | 6/147 [00:12<04:14,  1.80s/it, loss=3.28]\u001b[A\n",
      "Iteration:   5%|▍         | 7/147 [00:12<04:12,  1.80s/it, loss=3.28]\u001b[A\n",
      "Iteration:   5%|▍         | 7/147 [00:13<04:12,  1.80s/it, loss=3.92]\u001b[A\n",
      "Iteration:   5%|▌         | 8/147 [00:14<04:10,  1.80s/it, loss=3.92]\u001b[A\n",
      "Iteration:   5%|▌         | 8/147 [00:15<04:10,  1.80s/it, loss=3.81]\u001b[A\n",
      "Iteration:   6%|▌         | 9/147 [00:16<04:09,  1.81s/it, loss=3.81]\u001b[A\n",
      "Iteration:   6%|▌         | 9/147 [00:17<04:09,  1.81s/it, loss=3.49]\u001b[A\n",
      "Iteration:   7%|▋         | 10/147 [00:18<04:07,  1.81s/it, loss=3.49]\u001b[A\n",
      "Iteration:   7%|▋         | 10/147 [00:19<04:07,  1.81s/it, loss=3.15]\u001b[A\n",
      "Iteration:   7%|▋         | 11/147 [00:19<04:05,  1.80s/it, loss=3.15]\u001b[A\n",
      "Iteration:   7%|▋         | 11/147 [00:21<04:05,  1.80s/it, loss=3.05]\u001b[A\n",
      "Iteration:   8%|▊         | 12/147 [00:21<04:03,  1.80s/it, loss=3.05]\u001b[A\n",
      "Iteration:   8%|▊         | 12/147 [00:22<04:03,  1.80s/it, loss=3.06]\u001b[A\n",
      "Iteration:   9%|▉         | 13/147 [00:23<04:01,  1.80s/it, loss=3.06]\u001b[A\n",
      "Iteration:   9%|▉         | 13/147 [00:24<04:01,  1.80s/it, loss=2.88]\u001b[A\n",
      "Iteration:  10%|▉         | 14/147 [00:25<03:59,  1.80s/it, loss=2.88]\u001b[A\n",
      "Iteration:  10%|▉         | 14/147 [00:26<03:59,  1.80s/it, loss=3.06]\u001b[A\n",
      "Iteration:  10%|█         | 15/147 [00:27<03:57,  1.80s/it, loss=3.06]\u001b[A\n",
      "Iteration:  10%|█         | 15/147 [00:28<03:57,  1.80s/it, loss=3.44]\u001b[A\n",
      "Iteration:  11%|█         | 16/147 [00:28<03:55,  1.80s/it, loss=3.44]\u001b[A\n",
      "Iteration:  11%|█         | 16/147 [00:30<03:55,  1.80s/it, loss=3.76]\u001b[A\n",
      "Iteration:  12%|█▏        | 17/147 [00:30<03:54,  1.80s/it, loss=3.76]\u001b[A\n",
      "Iteration:  12%|█▏        | 17/147 [00:31<03:54,  1.80s/it, loss=3.07]\u001b[A\n",
      "Iteration:  12%|█▏        | 18/147 [00:32<03:52,  1.81s/it, loss=3.07]\u001b[A\n",
      "Iteration:  12%|█▏        | 18/147 [00:33<03:52,  1.81s/it, loss=3.37]\u001b[A\n",
      "Iteration:  13%|█▎        | 19/147 [00:34<03:50,  1.80s/it, loss=3.37]\u001b[A\n",
      "Iteration:  13%|█▎        | 19/147 [00:35<03:50,  1.80s/it, loss=3.65]\u001b[A\n",
      "Iteration:  14%|█▎        | 20/147 [00:36<03:49,  1.80s/it, loss=3.65]\u001b[A\n",
      "Iteration:  14%|█▎        | 20/147 [00:37<03:49,  1.80s/it, loss=3.57]\u001b[A\n",
      "Iteration:  14%|█▍        | 21/147 [00:37<03:47,  1.80s/it, loss=3.57]\u001b[A\n",
      "Iteration:  14%|█▍        | 21/147 [00:39<03:47,  1.80s/it, loss=3.17]\u001b[A\n",
      "Iteration:  15%|█▍        | 22/147 [00:39<03:45,  1.81s/it, loss=3.17]\u001b[A\n",
      "Iteration:  15%|█▍        | 22/147 [00:41<03:45,  1.81s/it, loss=3.71]\u001b[A\n",
      "Iteration:  16%|█▌        | 23/147 [00:41<03:44,  1.81s/it, loss=3.71]\u001b[A\n",
      "Iteration:  16%|█▌        | 23/147 [00:42<03:44,  1.81s/it, loss=2.58]\u001b[A\n",
      "Iteration:  16%|█▋        | 24/147 [00:43<03:42,  1.81s/it, loss=2.58]\u001b[A\n",
      "Iteration:  16%|█▋        | 24/147 [00:44<03:42,  1.81s/it, loss=3.24]\u001b[A\n",
      "Iteration:  17%|█▋        | 25/147 [00:45<03:40,  1.81s/it, loss=3.24]\u001b[A\n",
      "Iteration:  17%|█▋        | 25/147 [00:46<03:40,  1.81s/it, loss=3.33]\u001b[A\n",
      "Iteration:  18%|█▊        | 26/147 [00:46<03:39,  1.81s/it, loss=3.33]\u001b[A\n",
      "Iteration:  18%|█▊        | 26/147 [00:48<03:39,  1.81s/it, loss=3.38]\u001b[A\n",
      "Iteration:  18%|█▊        | 27/147 [00:48<03:36,  1.81s/it, loss=3.38]\u001b[A\n",
      "Iteration:  18%|█▊        | 27/147 [00:50<03:36,  1.81s/it, loss=2.71]\u001b[A\n",
      "Iteration:  19%|█▉        | 28/147 [00:50<03:35,  1.81s/it, loss=2.71]\u001b[A\n",
      "Iteration:  19%|█▉        | 28/147 [00:51<03:35,  1.81s/it, loss=2.67]\u001b[A\n",
      "Iteration:  20%|█▉        | 29/147 [00:52<03:33,  1.81s/it, loss=2.67]\u001b[A\n",
      "Iteration:  20%|█▉        | 29/147 [00:53<03:33,  1.81s/it, loss=3.22]\u001b[A\n",
      "Iteration:  20%|██        | 30/147 [00:54<03:31,  1.81s/it, loss=3.22]\u001b[A\n",
      "Iteration:  20%|██        | 30/147 [00:55<03:31,  1.81s/it, loss=2.96]\u001b[A\n",
      "Iteration:  21%|██        | 31/147 [00:55<03:29,  1.81s/it, loss=2.96]\u001b[A\n",
      "Iteration:  21%|██        | 31/147 [00:57<03:29,  1.81s/it, loss=3.79]\u001b[A\n",
      "Iteration:  22%|██▏       | 32/147 [00:57<03:27,  1.81s/it, loss=3.79]\u001b[A\n",
      "Iteration:  22%|██▏       | 32/147 [00:59<03:27,  1.81s/it, loss=3.71]\u001b[A\n",
      "Iteration:  22%|██▏       | 33/147 [00:59<03:25,  1.81s/it, loss=3.71]\u001b[A\n",
      "Iteration:  22%|██▏       | 33/147 [01:00<03:25,  1.81s/it, loss=3.5] \u001b[A\n",
      "Iteration:  23%|██▎       | 34/147 [01:01<03:24,  1.81s/it, loss=3.5]\u001b[A\n",
      "Iteration:  23%|██▎       | 34/147 [01:02<03:24,  1.81s/it, loss=3.22]\u001b[A\n",
      "Iteration:  24%|██▍       | 35/147 [01:03<03:22,  1.81s/it, loss=3.22]\u001b[A\n",
      "Iteration:  24%|██▍       | 35/147 [01:04<03:22,  1.81s/it, loss=3.25]\u001b[A\n",
      "Iteration:  24%|██▍       | 36/147 [01:04<03:20,  1.81s/it, loss=3.25]\u001b[A\n",
      "Iteration:  24%|██▍       | 36/147 [01:06<03:20,  1.81s/it, loss=3.2] \u001b[A\n",
      "Iteration:  25%|██▌       | 37/147 [01:06<03:18,  1.81s/it, loss=3.2]\u001b[A\n",
      "Iteration:  25%|██▌       | 37/147 [01:08<03:18,  1.81s/it, loss=3.3]\u001b[A\n",
      "Iteration:  26%|██▌       | 38/147 [01:08<03:16,  1.81s/it, loss=3.3]\u001b[A\n",
      "Iteration:  26%|██▌       | 38/147 [01:09<03:16,  1.81s/it, loss=3.16]\u001b[A\n",
      "Iteration:  27%|██▋       | 39/147 [01:10<03:15,  1.81s/it, loss=3.16]\u001b[A\n",
      "Iteration:  27%|██▋       | 39/147 [01:11<03:15,  1.81s/it, loss=3.11]\u001b[A\n",
      "Iteration:  27%|██▋       | 40/147 [01:12<03:13,  1.81s/it, loss=3.11]\u001b[A\n",
      "Iteration:  27%|██▋       | 40/147 [01:13<03:13,  1.81s/it, loss=3.06]\u001b[A\n",
      "Iteration:  28%|██▊       | 41/147 [01:14<03:11,  1.81s/it, loss=3.06]\u001b[A\n",
      "Iteration:  28%|██▊       | 41/147 [01:15<03:11,  1.81s/it, loss=3.72]\u001b[A\n",
      "Iteration:  29%|██▊       | 42/147 [01:15<03:09,  1.81s/it, loss=3.72]\u001b[A\n",
      "Iteration:  29%|██▊       | 42/147 [01:17<03:09,  1.81s/it, loss=2.62]\u001b[A\n",
      "Iteration:  29%|██▉       | 43/147 [01:17<03:08,  1.82s/it, loss=2.62]\u001b[A\n",
      "Iteration:  29%|██▉       | 43/147 [01:19<03:08,  1.82s/it, loss=3.08]\u001b[A\n",
      "Iteration:  30%|██▉       | 44/147 [01:19<03:08,  1.83s/it, loss=3.08]\u001b[A\n",
      "Iteration:  30%|██▉       | 44/147 [01:20<03:08,  1.83s/it, loss=3.4] \u001b[A\n",
      "Iteration:  31%|███       | 45/147 [01:21<03:09,  1.85s/it, loss=3.4]\u001b[A\n",
      "Iteration:  31%|███       | 45/147 [01:22<03:09,  1.85s/it, loss=3.28]\u001b[A\n",
      "Iteration:  31%|███▏      | 46/147 [01:23<03:09,  1.88s/it, loss=3.28]\u001b[A\n",
      "Iteration:  31%|███▏      | 46/147 [01:24<03:09,  1.88s/it, loss=3.04]\u001b[A\n",
      "Iteration:  32%|███▏      | 47/147 [01:25<03:08,  1.88s/it, loss=3.04]\u001b[A\n",
      "Iteration:  32%|███▏      | 47/147 [01:26<03:08,  1.88s/it, loss=3.43]\u001b[A\n",
      "Iteration:  33%|███▎      | 48/147 [01:27<03:07,  1.90s/it, loss=3.43]\u001b[A\n",
      "Iteration:  33%|███▎      | 48/147 [01:28<03:07,  1.90s/it, loss=3.46]\u001b[A\n",
      "Iteration:  33%|███▎      | 49/147 [01:29<03:05,  1.89s/it, loss=3.46]\u001b[A\n",
      "Iteration:  33%|███▎      | 49/147 [01:30<03:05,  1.89s/it, loss=3.27]\u001b[A\n",
      "Iteration:  34%|███▍      | 50/147 [01:30<03:03,  1.89s/it, loss=3.27]\u001b[A\n",
      "Iteration:  34%|███▍      | 50/147 [01:32<03:03,  1.89s/it, loss=2.79]\u001b[A\n",
      "Iteration:  35%|███▍      | 51/147 [01:32<03:01,  1.89s/it, loss=2.79]\u001b[A\n",
      "Iteration:  35%|███▍      | 51/147 [01:34<03:01,  1.89s/it, loss=3.46]\u001b[A\n",
      "Iteration:  35%|███▌      | 52/147 [01:34<03:01,  1.91s/it, loss=3.46]\u001b[A\n",
      "Iteration:  35%|███▌      | 52/147 [01:36<03:01,  1.91s/it, loss=2.98]\u001b[A\n",
      "Iteration:  36%|███▌      | 53/147 [01:36<03:00,  1.93s/it, loss=2.98]\u001b[A\n",
      "Iteration:  36%|███▌      | 53/147 [01:38<03:00,  1.93s/it, loss=3.22]\u001b[A\n",
      "Iteration:  37%|███▋      | 54/147 [01:38<02:58,  1.92s/it, loss=3.22]\u001b[A\n",
      "Iteration:  37%|███▋      | 54/147 [01:40<02:58,  1.92s/it, loss=3.44]\u001b[A\n",
      "Iteration:  37%|███▋      | 55/147 [01:40<02:58,  1.94s/it, loss=3.44]\u001b[A\n",
      "Iteration:  37%|███▋      | 55/147 [01:42<02:58,  1.94s/it, loss=3.11]\u001b[A\n",
      "Iteration:  38%|███▊      | 56/147 [01:42<03:00,  1.98s/it, loss=3.11]\u001b[A\n",
      "Iteration:  38%|███▊      | 56/147 [01:44<03:00,  1.98s/it, loss=3.27]\u001b[A\n",
      "Iteration:  39%|███▉      | 57/147 [01:44<03:00,  2.01s/it, loss=3.27]\u001b[A\n",
      "Iteration:  39%|███▉      | 57/147 [01:46<03:00,  2.01s/it, loss=3.91]\u001b[A\n",
      "Iteration:  39%|███▉      | 58/147 [01:46<02:58,  2.01s/it, loss=3.91]\u001b[A\n",
      "Iteration:  39%|███▉      | 58/147 [01:48<02:58,  2.01s/it, loss=2.88]\u001b[A\n",
      "Iteration:  40%|████      | 59/147 [01:48<02:54,  1.98s/it, loss=2.88]\u001b[A\n",
      "Iteration:  40%|████      | 59/147 [01:50<02:54,  1.98s/it, loss=2.78]\u001b[A\n",
      "Iteration:  41%|████      | 60/147 [01:50<02:49,  1.95s/it, loss=2.78]\u001b[A\n",
      "Iteration:  41%|████      | 60/147 [01:52<02:49,  1.95s/it, loss=3.47]\u001b[A\n",
      "Iteration:  41%|████▏     | 61/147 [01:52<02:49,  1.97s/it, loss=3.47]\u001b[A\n",
      "Iteration:  41%|████▏     | 61/147 [01:54<02:49,  1.97s/it, loss=2.95]\u001b[A\n",
      "Iteration:  42%|████▏     | 62/147 [01:54<02:48,  1.98s/it, loss=2.95]\u001b[A\n",
      "Iteration:  42%|████▏     | 62/147 [01:56<02:48,  1.98s/it, loss=3.32]\u001b[A\n",
      "Iteration:  43%|████▎     | 63/147 [01:56<02:44,  1.95s/it, loss=3.32]\u001b[A\n",
      "Iteration:  43%|████▎     | 63/147 [01:57<02:44,  1.95s/it, loss=2.9] \u001b[A\n",
      "Iteration:  44%|████▎     | 64/147 [01:58<02:40,  1.93s/it, loss=2.9]\u001b[A\n",
      "Iteration:  44%|████▎     | 64/147 [01:59<02:40,  1.93s/it, loss=3.27]\u001b[A\n",
      "Iteration:  44%|████▍     | 65/147 [02:00<02:36,  1.91s/it, loss=3.27]\u001b[A\n",
      "Iteration:  44%|████▍     | 65/147 [02:01<02:36,  1.91s/it, loss=3.61]\u001b[A\n",
      "Iteration:  45%|████▍     | 66/147 [02:02<02:34,  1.90s/it, loss=3.61]\u001b[A\n",
      "Iteration:  45%|████▍     | 66/147 [02:03<02:34,  1.90s/it, loss=3.12]\u001b[A\n",
      "Iteration:  46%|████▌     | 67/147 [02:04<02:31,  1.89s/it, loss=3.12]\u001b[A\n",
      "Iteration:  46%|████▌     | 67/147 [02:05<02:31,  1.89s/it, loss=3.17]\u001b[A\n",
      "Iteration:  46%|████▋     | 68/147 [02:05<02:29,  1.89s/it, loss=3.17]\u001b[A\n",
      "Iteration:  46%|████▋     | 68/147 [02:07<02:29,  1.89s/it, loss=3.31]\u001b[A\n",
      "Iteration:  47%|████▋     | 69/147 [02:07<02:26,  1.88s/it, loss=3.31]\u001b[A\n",
      "Iteration:  47%|████▋     | 69/147 [02:09<02:26,  1.88s/it, loss=3.35]\u001b[A\n",
      "Iteration:  48%|████▊     | 70/147 [02:09<02:27,  1.91s/it, loss=3.35]\u001b[A\n",
      "Iteration:  48%|████▊     | 70/147 [02:11<02:27,  1.91s/it, loss=3.31]\u001b[A\n",
      "Iteration:  48%|████▊     | 71/147 [02:11<02:27,  1.95s/it, loss=3.31]\u001b[A\n",
      "Iteration:  48%|████▊     | 71/147 [02:13<02:27,  1.95s/it, loss=3.18]\u001b[A\n",
      "Iteration:  49%|████▉     | 72/147 [02:13<02:26,  1.95s/it, loss=3.18]\u001b[A\n",
      "Iteration:  49%|████▉     | 72/147 [02:15<02:26,  1.95s/it, loss=2.87]\u001b[A\n",
      "Iteration:  50%|████▉     | 73/147 [02:15<02:23,  1.95s/it, loss=2.87]\u001b[A\n",
      "Iteration:  50%|████▉     | 73/147 [02:17<02:23,  1.95s/it, loss=3.11]\u001b[A\n",
      "Iteration:  50%|█████     | 74/147 [02:17<02:21,  1.94s/it, loss=3.11]\u001b[A\n",
      "Iteration:  50%|█████     | 74/147 [02:19<02:21,  1.94s/it, loss=3.31]\u001b[A\n",
      "Iteration:  51%|█████     | 75/147 [02:19<02:20,  1.95s/it, loss=3.31]\u001b[A\n",
      "Iteration:  51%|█████     | 75/147 [02:20<02:20,  1.95s/it, loss=2.83]\u001b[A\n",
      "Iteration:  52%|█████▏    | 76/147 [02:21<02:16,  1.92s/it, loss=2.83]\u001b[A\n",
      "Iteration:  52%|█████▏    | 76/147 [02:22<02:16,  1.92s/it, loss=3.24]\u001b[A\n",
      "Iteration:  52%|█████▏    | 77/147 [02:23<02:16,  1.95s/it, loss=3.24]\u001b[A\n",
      "Iteration:  52%|█████▏    | 77/147 [02:24<02:16,  1.95s/it, loss=2.55]\u001b[A\n",
      "Iteration:  53%|█████▎    | 78/147 [02:25<02:15,  1.96s/it, loss=2.55]\u001b[A\n",
      "Iteration:  53%|█████▎    | 78/147 [02:26<02:15,  1.96s/it, loss=3.26]\u001b[A\n",
      "Iteration:  54%|█████▎    | 79/147 [02:27<02:12,  1.94s/it, loss=3.26]\u001b[A\n",
      "Iteration:  54%|█████▎    | 79/147 [02:28<02:12,  1.94s/it, loss=3.11]\u001b[A\n",
      "Iteration:  54%|█████▍    | 80/147 [02:29<02:08,  1.92s/it, loss=3.11]\u001b[A\n",
      "Iteration:  54%|█████▍    | 80/147 [02:30<02:08,  1.92s/it, loss=3.12]\u001b[A\n",
      "Iteration:  55%|█████▌    | 81/147 [02:31<02:08,  1.95s/it, loss=3.12]\u001b[A\n",
      "Iteration:  55%|█████▌    | 81/147 [02:32<02:08,  1.95s/it, loss=2.91]\u001b[A\n",
      "Iteration:  56%|█████▌    | 82/147 [02:33<02:07,  1.96s/it, loss=2.91]\u001b[A\n",
      "Iteration:  56%|█████▌    | 82/147 [02:34<02:07,  1.96s/it, loss=3.55]\u001b[A\n",
      "Iteration:  56%|█████▋    | 83/147 [02:35<02:06,  1.98s/it, loss=3.55]\u001b[A\n",
      "Iteration:  56%|█████▋    | 83/147 [02:36<02:06,  1.98s/it, loss=3.23]\u001b[A\n",
      "Iteration:  57%|█████▋    | 84/147 [02:37<02:04,  1.98s/it, loss=3.23]\u001b[A\n",
      "Iteration:  57%|█████▋    | 84/147 [02:38<02:04,  1.98s/it, loss=3.42]\u001b[A\n",
      "Iteration:  58%|█████▊    | 85/147 [02:39<02:02,  1.97s/it, loss=3.42]\u001b[A\n",
      "Iteration:  58%|█████▊    | 85/147 [02:40<02:02,  1.97s/it, loss=3.5] \u001b[A\n",
      "Iteration:  59%|█████▊    | 86/147 [02:41<01:59,  1.96s/it, loss=3.5]\u001b[A\n",
      "Iteration:  59%|█████▊    | 86/147 [02:42<01:59,  1.96s/it, loss=3.07]\u001b[A\n",
      "Iteration:  59%|█████▉    | 87/147 [02:42<01:56,  1.93s/it, loss=3.07]\u001b[A\n",
      "Iteration:  59%|█████▉    | 87/147 [02:44<01:56,  1.93s/it, loss=2.75]\u001b[A\n",
      "Iteration:  60%|█████▉    | 88/147 [02:44<01:52,  1.91s/it, loss=2.75]\u001b[A\n",
      "Iteration:  60%|█████▉    | 88/147 [02:46<01:52,  1.91s/it, loss=3.13]\u001b[A\n",
      "Iteration:  61%|██████    | 89/147 [02:46<01:49,  1.90s/it, loss=3.13]\u001b[A\n",
      "Iteration:  61%|██████    | 89/147 [02:48<01:49,  1.90s/it, loss=3.66]\u001b[A\n",
      "Iteration:  61%|██████    | 90/147 [02:48<01:47,  1.88s/it, loss=3.66]\u001b[A\n",
      "Iteration:  61%|██████    | 90/147 [02:49<01:47,  1.88s/it, loss=3.19]\u001b[A\n",
      "Iteration:  62%|██████▏   | 91/147 [02:50<01:44,  1.87s/it, loss=3.19]\u001b[A\n",
      "Iteration:  62%|██████▏   | 91/147 [02:51<01:44,  1.87s/it, loss=3.3] \u001b[A\n",
      "Iteration:  63%|██████▎   | 92/147 [02:52<01:42,  1.87s/it, loss=3.3]\u001b[A\n",
      "Iteration:  63%|██████▎   | 92/147 [02:53<01:42,  1.87s/it, loss=3.16]\u001b[A\n",
      "Iteration:  63%|██████▎   | 93/147 [02:54<01:40,  1.86s/it, loss=3.16]\u001b[A\n",
      "Iteration:  63%|██████▎   | 93/147 [02:55<01:40,  1.86s/it, loss=3.13]\u001b[A\n",
      "Iteration:  64%|██████▍   | 94/147 [02:55<01:38,  1.86s/it, loss=3.13]\u001b[A\n",
      "Iteration:  64%|██████▍   | 94/147 [02:57<01:38,  1.86s/it, loss=2.83]\u001b[A\n",
      "Iteration:  65%|██████▍   | 95/147 [02:57<01:36,  1.86s/it, loss=2.83]\u001b[A\n",
      "Iteration:  65%|██████▍   | 95/147 [02:59<01:36,  1.86s/it, loss=4.16]\u001b[A\n",
      "Iteration:  65%|██████▌   | 96/147 [02:59<01:34,  1.86s/it, loss=4.16]\u001b[A\n",
      "Iteration:  65%|██████▌   | 96/147 [03:01<01:34,  1.86s/it, loss=2.86]\u001b[A\n",
      "Iteration:  66%|██████▌   | 97/147 [03:01<01:32,  1.86s/it, loss=2.86]\u001b[A\n",
      "Iteration:  66%|██████▌   | 97/147 [03:02<01:32,  1.86s/it, loss=2.55]\u001b[A\n",
      "Iteration:  67%|██████▋   | 98/147 [03:03<01:30,  1.86s/it, loss=2.55]\u001b[A\n",
      "Iteration:  67%|██████▋   | 98/147 [03:04<01:30,  1.86s/it, loss=3.4] \u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  67%|██████▋   | 99/147 [03:05<01:29,  1.86s/it, loss=3.4]\u001b[A\n",
      "Iteration:  67%|██████▋   | 99/147 [03:06<01:29,  1.86s/it, loss=3.69]\u001b[A\n",
      "Iteration:  68%|██████▊   | 100/147 [03:07<01:27,  1.85s/it, loss=3.69]\u001b[A\n",
      "Iteration:  68%|██████▊   | 100/147 [03:08<01:27,  1.85s/it, loss=3.26]\u001b[A\n",
      "Iteration:  69%|██████▊   | 101/147 [03:08<01:25,  1.85s/it, loss=3.26]\u001b[A\n",
      "Iteration:  69%|██████▊   | 101/147 [03:10<01:25,  1.85s/it, loss=3.85]\u001b[A\n",
      "Iteration:  69%|██████▉   | 102/147 [03:10<01:23,  1.85s/it, loss=3.85]\u001b[A\n",
      "Iteration:  69%|██████▉   | 102/147 [03:12<01:23,  1.85s/it, loss=3.46]\u001b[A\n",
      "Iteration:  70%|███████   | 103/147 [03:12<01:21,  1.85s/it, loss=3.46]\u001b[A\n",
      "Iteration:  70%|███████   | 103/147 [03:13<01:21,  1.85s/it, loss=3.51]\u001b[A\n",
      "Iteration:  71%|███████   | 104/147 [03:14<01:19,  1.85s/it, loss=3.51]\u001b[A\n",
      "Iteration:  71%|███████   | 104/147 [03:15<01:19,  1.85s/it, loss=3.4] \u001b[A\n",
      "Iteration:  71%|███████▏  | 105/147 [03:16<01:17,  1.85s/it, loss=3.4]\u001b[A\n",
      "Iteration:  71%|███████▏  | 105/147 [03:17<01:17,  1.85s/it, loss=3.14]\u001b[A\n",
      "Iteration:  72%|███████▏  | 106/147 [03:18<01:15,  1.85s/it, loss=3.14]\u001b[A\n",
      "Iteration:  72%|███████▏  | 106/147 [03:19<01:15,  1.85s/it, loss=3.2] \u001b[A\n",
      "Iteration:  73%|███████▎  | 107/147 [03:19<01:13,  1.85s/it, loss=3.2]\u001b[A\n",
      "Iteration:  73%|███████▎  | 107/147 [03:21<01:13,  1.85s/it, loss=3.05]\u001b[A\n",
      "Iteration:  73%|███████▎  | 108/147 [03:21<01:12,  1.85s/it, loss=3.05]\u001b[A\n",
      "Iteration:  73%|███████▎  | 108/147 [03:23<01:12,  1.85s/it, loss=3.08]\u001b[A\n",
      "Iteration:  74%|███████▍  | 109/147 [03:23<01:10,  1.85s/it, loss=3.08]\u001b[A\n",
      "Iteration:  74%|███████▍  | 109/147 [03:25<01:10,  1.85s/it, loss=3.41]\u001b[A\n",
      "Iteration:  75%|███████▍  | 110/147 [03:25<01:08,  1.85s/it, loss=3.41]\u001b[A\n",
      "Iteration:  75%|███████▍  | 110/147 [03:26<01:08,  1.85s/it, loss=2.7] \u001b[A\n",
      "Iteration:  76%|███████▌  | 111/147 [03:27<01:06,  1.85s/it, loss=2.7]\u001b[A\n",
      "Iteration:  76%|███████▌  | 111/147 [03:28<01:06,  1.85s/it, loss=3.3]\u001b[A\n",
      "Iteration:  76%|███████▌  | 112/147 [03:29<01:04,  1.85s/it, loss=3.3]\u001b[A\n",
      "Iteration:  76%|███████▌  | 112/147 [03:30<01:04,  1.85s/it, loss=3.51]\u001b[A\n",
      "Iteration:  77%|███████▋  | 113/147 [03:31<01:03,  1.86s/it, loss=3.51]\u001b[A\n",
      "Iteration:  77%|███████▋  | 113/147 [03:32<01:03,  1.86s/it, loss=3.16]\u001b[A\n",
      "Iteration:  78%|███████▊  | 114/147 [03:32<01:01,  1.86s/it, loss=3.16]\u001b[A\n",
      "Iteration:  78%|███████▊  | 114/147 [03:34<01:01,  1.86s/it, loss=3.35]\u001b[A\n",
      "Iteration:  78%|███████▊  | 115/147 [03:34<00:59,  1.85s/it, loss=3.35]\u001b[A\n",
      "Iteration:  78%|███████▊  | 115/147 [03:36<00:59,  1.85s/it, loss=3.26]\u001b[A\n",
      "Iteration:  79%|███████▉  | 116/147 [03:36<00:57,  1.85s/it, loss=3.26]\u001b[A\n",
      "Iteration:  79%|███████▉  | 116/147 [03:38<00:57,  1.85s/it, loss=3.85]\u001b[A\n",
      "Iteration:  80%|███████▉  | 117/147 [03:38<00:55,  1.85s/it, loss=3.85]\u001b[A\n",
      "Iteration:  80%|███████▉  | 117/147 [03:39<00:55,  1.85s/it, loss=3.41]\u001b[A\n",
      "Iteration:  80%|████████  | 118/147 [03:40<00:53,  1.85s/it, loss=3.41]\u001b[A\n",
      "Iteration:  80%|████████  | 118/147 [03:41<00:53,  1.85s/it, loss=3.15]\u001b[A\n",
      "Iteration:  81%|████████  | 119/147 [03:42<00:51,  1.85s/it, loss=3.15]\u001b[A\n",
      "Iteration:  81%|████████  | 119/147 [03:43<00:51,  1.85s/it, loss=3.2] \u001b[A\n",
      "Iteration:  82%|████████▏ | 120/147 [03:44<00:50,  1.85s/it, loss=3.2]\u001b[A\n",
      "Iteration:  82%|████████▏ | 120/147 [03:45<00:50,  1.85s/it, loss=3.69]\u001b[A\n",
      "Iteration:  82%|████████▏ | 121/147 [03:45<00:48,  1.86s/it, loss=3.69]\u001b[A\n",
      "Iteration:  82%|████████▏ | 121/147 [03:47<00:48,  1.86s/it, loss=3.3] \u001b[A\n",
      "Iteration:  83%|████████▎ | 122/147 [03:47<00:46,  1.85s/it, loss=3.3]\u001b[A\n",
      "Iteration:  83%|████████▎ | 122/147 [03:49<00:46,  1.85s/it, loss=3.45]\u001b[A\n",
      "Iteration:  84%|████████▎ | 123/147 [03:49<00:44,  1.85s/it, loss=3.45]\u001b[A\n",
      "Iteration:  84%|████████▎ | 123/147 [03:50<00:44,  1.85s/it, loss=3.18]\u001b[A\n",
      "Iteration:  84%|████████▍ | 124/147 [03:51<00:42,  1.85s/it, loss=3.18]\u001b[A\n",
      "Iteration:  84%|████████▍ | 124/147 [03:52<00:42,  1.85s/it, loss=2.83]\u001b[A\n",
      "Iteration:  85%|████████▌ | 125/147 [03:53<00:40,  1.86s/it, loss=2.83]\u001b[A\n",
      "Iteration:  85%|████████▌ | 125/147 [03:54<00:40,  1.86s/it, loss=3.02]\u001b[A\n",
      "Iteration:  86%|████████▌ | 126/147 [03:55<00:38,  1.85s/it, loss=3.02]\u001b[A\n",
      "Iteration:  86%|████████▌ | 126/147 [03:56<00:38,  1.85s/it, loss=3.16]\u001b[A\n",
      "Iteration:  86%|████████▋ | 127/147 [03:57<00:36,  1.85s/it, loss=3.16]\u001b[A\n",
      "Iteration:  86%|████████▋ | 127/147 [03:58<00:36,  1.85s/it, loss=3.14]\u001b[A\n",
      "Iteration:  87%|████████▋ | 128/147 [03:58<00:35,  1.85s/it, loss=3.14]\u001b[A\n",
      "Iteration:  87%|████████▋ | 128/147 [04:00<00:35,  1.85s/it, loss=3.35]\u001b[A\n",
      "Iteration:  88%|████████▊ | 129/147 [04:00<00:33,  1.85s/it, loss=3.35]\u001b[A\n",
      "Iteration:  88%|████████▊ | 129/147 [04:02<00:33,  1.85s/it, loss=3.37]\u001b[A\n",
      "Iteration:  88%|████████▊ | 130/147 [04:02<00:31,  1.85s/it, loss=3.37]\u001b[A\n",
      "Iteration:  88%|████████▊ | 130/147 [04:03<00:31,  1.85s/it, loss=3.01]\u001b[A\n",
      "Iteration:  89%|████████▉ | 131/147 [04:04<00:29,  1.85s/it, loss=3.01]\u001b[A\n",
      "Iteration:  89%|████████▉ | 131/147 [04:05<00:29,  1.85s/it, loss=3.67]\u001b[A\n",
      "Iteration:  90%|████████▉ | 132/147 [04:06<00:27,  1.85s/it, loss=3.67]\u001b[A\n",
      "Iteration:  90%|████████▉ | 132/147 [04:07<00:27,  1.85s/it, loss=2.81]\u001b[A\n",
      "Iteration:  90%|█████████ | 133/147 [04:08<00:25,  1.85s/it, loss=2.81]\u001b[A\n",
      "Iteration:  90%|█████████ | 133/147 [04:09<00:25,  1.85s/it, loss=2.63]\u001b[A\n",
      "Iteration:  91%|█████████ | 134/147 [04:09<00:24,  1.85s/it, loss=2.63]\u001b[A\n",
      "Iteration:  91%|█████████ | 134/147 [04:11<00:24,  1.85s/it, loss=3.22]\u001b[A\n",
      "Iteration:  92%|█████████▏| 135/147 [04:11<00:22,  1.85s/it, loss=3.22]\u001b[A\n",
      "Iteration:  92%|█████████▏| 135/147 [04:13<00:22,  1.85s/it, loss=2.65]\u001b[A\n",
      "Iteration:  93%|█████████▎| 136/147 [04:13<00:20,  1.85s/it, loss=2.65]\u001b[A\n",
      "Iteration:  93%|█████████▎| 136/147 [04:15<00:20,  1.85s/it, loss=3.77]\u001b[A\n",
      "Iteration:  93%|█████████▎| 137/147 [04:15<00:18,  1.85s/it, loss=3.77]\u001b[A\n",
      "Iteration:  93%|█████████▎| 137/147 [04:16<00:18,  1.85s/it, loss=3.13]\u001b[A\n",
      "Iteration:  94%|█████████▍| 138/147 [04:17<00:16,  1.85s/it, loss=3.13]\u001b[A\n",
      "Iteration:  94%|█████████▍| 138/147 [04:18<00:16,  1.85s/it, loss=2.66]\u001b[A\n",
      "Iteration:  95%|█████████▍| 139/147 [04:19<00:14,  1.85s/it, loss=2.66]\u001b[A\n",
      "Iteration:  95%|█████████▍| 139/147 [04:20<00:14,  1.85s/it, loss=2.63]\u001b[A\n",
      "Iteration:  95%|█████████▌| 140/147 [04:21<00:13,  1.86s/it, loss=2.63]\u001b[A\n",
      "Iteration:  95%|█████████▌| 140/147 [04:22<00:13,  1.86s/it, loss=3.3] \u001b[A\n",
      "Iteration:  96%|█████████▌| 141/147 [04:23<00:11,  1.90s/it, loss=3.3]\u001b[A\n",
      "Iteration:  96%|█████████▌| 141/147 [04:24<00:11,  1.90s/it, loss=2.79]\u001b[A\n",
      "Iteration:  97%|█████████▋| 142/147 [04:24<00:09,  1.90s/it, loss=2.79]\u001b[A\n",
      "Iteration:  97%|█████████▋| 142/147 [04:26<00:09,  1.90s/it, loss=3.17]\u001b[A\n",
      "Iteration:  97%|█████████▋| 143/147 [04:26<00:07,  1.90s/it, loss=3.17]\u001b[A\n",
      "Iteration:  97%|█████████▋| 143/147 [04:28<00:07,  1.90s/it, loss=2.96]\u001b[A\n",
      "Iteration:  98%|█████████▊| 144/147 [04:28<00:05,  1.91s/it, loss=2.96]\u001b[A\n",
      "Iteration:  98%|█████████▊| 144/147 [04:30<00:05,  1.91s/it, loss=3.16]\u001b[A\n",
      "Iteration:  99%|█████████▊| 145/147 [04:30<00:03,  1.90s/it, loss=3.16]\u001b[A\n",
      "Iteration:  99%|█████████▊| 145/147 [04:32<00:03,  1.90s/it, loss=3.29]\u001b[A\n",
      "Iteration:  99%|█████████▉| 146/147 [04:32<00:01,  1.89s/it, loss=3.29]\u001b[A\n",
      "Iteration:  99%|█████████▉| 146/147 [04:33<00:01,  1.89s/it, loss=3.55]\u001b[A\n",
      "Epoch:  60%|██████    | 3/5 [13:04<08:37, 258.81s/it]68s/it, loss=3.55]\u001b[A\n",
      "Iteration:   0%|          | 0/147 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   0%|          | 0/147 [00:01<?, ?it/s, loss=3.08]\u001b[A\n",
      "Iteration:   1%|          | 1/147 [00:01<04:29,  1.85s/it, loss=3.08]\u001b[A\n",
      "Iteration:   1%|          | 1/147 [00:03<04:29,  1.85s/it, loss=3.09]\u001b[A\n",
      "Iteration:   1%|▏         | 2/147 [00:03<04:28,  1.85s/it, loss=3.09]\u001b[A\n",
      "Iteration:   1%|▏         | 2/147 [00:05<04:28,  1.85s/it, loss=3.14]\u001b[A\n",
      "Iteration:   2%|▏         | 3/147 [00:05<04:26,  1.85s/it, loss=3.14]\u001b[A\n",
      "Iteration:   2%|▏         | 3/147 [00:06<04:26,  1.85s/it, loss=2.49]\u001b[A\n",
      "Iteration:   3%|▎         | 4/147 [00:07<04:25,  1.85s/it, loss=2.49]\u001b[A\n",
      "Iteration:   3%|▎         | 4/147 [00:08<04:25,  1.85s/it, loss=3.04]\u001b[A\n",
      "Iteration:   3%|▎         | 5/147 [00:09<04:23,  1.85s/it, loss=3.04]\u001b[A\n",
      "Iteration:   3%|▎         | 5/147 [00:10<04:23,  1.85s/it, loss=3.1] \u001b[A\n",
      "Iteration:   4%|▍         | 6/147 [00:11<04:21,  1.85s/it, loss=3.1]\u001b[A\n",
      "Iteration:   4%|▍         | 6/147 [00:12<04:21,  1.85s/it, loss=3]  \u001b[A\n",
      "Iteration:   5%|▍         | 7/147 [00:12<04:19,  1.85s/it, loss=3]\u001b[A\n",
      "Iteration:   5%|▍         | 7/147 [00:14<04:19,  1.85s/it, loss=2.71]\u001b[A\n",
      "Iteration:   5%|▌         | 8/147 [00:14<04:16,  1.85s/it, loss=2.71]\u001b[A\n",
      "Iteration:   5%|▌         | 8/147 [00:16<04:16,  1.85s/it, loss=2.36]\u001b[A\n",
      "Iteration:   6%|▌         | 9/147 [00:16<04:14,  1.85s/it, loss=2.36]\u001b[A\n",
      "Iteration:   6%|▌         | 9/147 [00:18<04:14,  1.85s/it, loss=2.69]\u001b[A\n",
      "Iteration:   7%|▋         | 10/147 [00:18<04:13,  1.85s/it, loss=2.69]\u001b[A\n",
      "Iteration:   7%|▋         | 10/147 [00:19<04:13,  1.85s/it, loss=2.91]\u001b[A\n",
      "Iteration:   7%|▋         | 11/147 [00:20<04:10,  1.85s/it, loss=2.91]\u001b[A\n",
      "Iteration:   7%|▋         | 11/147 [00:21<04:10,  1.85s/it, loss=2.72]\u001b[A\n",
      "Iteration:   8%|▊         | 12/147 [00:22<04:09,  1.85s/it, loss=2.72]\u001b[A\n",
      "Iteration:   8%|▊         | 12/147 [00:23<04:09,  1.85s/it, loss=2.63]\u001b[A\n",
      "Iteration:   9%|▉         | 13/147 [00:24<04:07,  1.84s/it, loss=2.63]\u001b[A\n",
      "Iteration:   9%|▉         | 13/147 [00:25<04:07,  1.84s/it, loss=2.04]\u001b[A\n",
      "Iteration:  10%|▉         | 14/147 [00:25<04:05,  1.84s/it, loss=2.04]\u001b[A\n",
      "Iteration:  10%|▉         | 14/147 [00:27<04:05,  1.84s/it, loss=2.81]\u001b[A\n",
      "Iteration:  10%|█         | 15/147 [00:27<04:03,  1.84s/it, loss=2.81]\u001b[A\n",
      "Iteration:  10%|█         | 15/147 [00:29<04:03,  1.84s/it, loss=2.64]\u001b[A\n",
      "Iteration:  11%|█         | 16/147 [00:29<04:01,  1.84s/it, loss=2.64]\u001b[A\n",
      "Iteration:  11%|█         | 16/147 [00:30<04:01,  1.84s/it, loss=2.29]\u001b[A\n",
      "Iteration:  12%|█▏        | 17/147 [00:31<03:59,  1.84s/it, loss=2.29]\u001b[A\n",
      "Iteration:  12%|█▏        | 17/147 [00:32<03:59,  1.84s/it, loss=2.2] \u001b[A\n",
      "Iteration:  12%|█▏        | 18/147 [00:33<03:57,  1.84s/it, loss=2.2]\u001b[A\n",
      "Iteration:  12%|█▏        | 18/147 [00:34<03:57,  1.84s/it, loss=2.83]\u001b[A\n",
      "Iteration:  13%|█▎        | 19/147 [00:35<03:55,  1.84s/it, loss=2.83]\u001b[A\n",
      "Iteration:  13%|█▎        | 19/147 [00:36<03:55,  1.84s/it, loss=2.7] \u001b[A\n",
      "Iteration:  14%|█▎        | 20/147 [00:36<03:54,  1.84s/it, loss=2.7]\u001b[A\n",
      "Iteration:  14%|█▎        | 20/147 [00:38<03:54,  1.84s/it, loss=2.67]\u001b[A\n",
      "Iteration:  14%|█▍        | 21/147 [00:38<03:52,  1.84s/it, loss=2.67]\u001b[A\n",
      "Iteration:  14%|█▍        | 21/147 [00:40<03:52,  1.84s/it, loss=2.45]\u001b[A\n",
      "Iteration:  15%|█▍        | 22/147 [00:40<03:50,  1.84s/it, loss=2.45]\u001b[A\n",
      "Iteration:  15%|█▍        | 22/147 [00:41<03:50,  1.84s/it, loss=2.94]\u001b[A\n",
      "Iteration:  16%|█▌        | 23/147 [00:42<03:48,  1.84s/it, loss=2.94]\u001b[A\n",
      "Iteration:  16%|█▌        | 23/147 [00:43<03:48,  1.84s/it, loss=2.9] \u001b[A\n",
      "Iteration:  16%|█▋        | 24/147 [00:44<03:47,  1.85s/it, loss=2.9]\u001b[A\n",
      "Iteration:  16%|█▋        | 24/147 [00:45<03:47,  1.85s/it, loss=3.29]\u001b[A\n",
      "Iteration:  17%|█▋        | 25/147 [00:46<03:45,  1.85s/it, loss=3.29]\u001b[A\n",
      "Iteration:  17%|█▋        | 25/147 [00:47<03:45,  1.85s/it, loss=2.82]\u001b[A\n",
      "Iteration:  18%|█▊        | 26/147 [00:48<03:43,  1.84s/it, loss=2.82]\u001b[A\n",
      "Iteration:  18%|█▊        | 26/147 [00:49<03:43,  1.84s/it, loss=2.32]\u001b[A\n",
      "Iteration:  18%|█▊        | 27/147 [00:49<03:41,  1.84s/it, loss=2.32]\u001b[A\n",
      "Iteration:  18%|█▊        | 27/147 [00:51<03:41,  1.84s/it, loss=3.01]\u001b[A\n",
      "Iteration:  19%|█▉        | 28/147 [00:51<03:39,  1.85s/it, loss=3.01]\u001b[A\n",
      "Iteration:  19%|█▉        | 28/147 [00:53<03:39,  1.85s/it, loss=2.66]\u001b[A\n",
      "Iteration:  20%|█▉        | 29/147 [00:53<03:37,  1.85s/it, loss=2.66]\u001b[A\n",
      "Iteration:  20%|█▉        | 29/147 [00:54<03:37,  1.85s/it, loss=2.98]\u001b[A\n",
      "Iteration:  20%|██        | 30/147 [00:55<03:36,  1.85s/it, loss=2.98]\u001b[A\n",
      "Iteration:  20%|██        | 30/147 [00:56<03:36,  1.85s/it, loss=3.31]\u001b[A\n",
      "Iteration:  21%|██        | 31/147 [00:57<03:34,  1.85s/it, loss=3.31]\u001b[A\n",
      "Iteration:  21%|██        | 31/147 [00:58<03:34,  1.85s/it, loss=2.82]\u001b[A\n",
      "Iteration:  22%|██▏       | 32/147 [00:59<03:32,  1.85s/it, loss=2.82]\u001b[A\n",
      "Iteration:  22%|██▏       | 32/147 [01:00<03:32,  1.85s/it, loss=2.47]\u001b[A\n",
      "Iteration:  22%|██▏       | 33/147 [01:01<03:32,  1.87s/it, loss=2.47]\u001b[A\n",
      "Iteration:  22%|██▏       | 33/147 [01:02<03:32,  1.87s/it, loss=2.81]\u001b[A\n",
      "Iteration:  23%|██▎       | 34/147 [01:02<03:32,  1.88s/it, loss=2.81]\u001b[A\n",
      "Iteration:  23%|██▎       | 34/147 [01:04<03:32,  1.88s/it, loss=3.07]\u001b[A\n",
      "Iteration:  24%|██▍       | 35/147 [01:04<03:30,  1.88s/it, loss=3.07]\u001b[A\n",
      "Iteration:  24%|██▍       | 35/147 [01:06<03:30,  1.88s/it, loss=3.17]\u001b[A\n",
      "Iteration:  24%|██▍       | 36/147 [01:06<03:28,  1.87s/it, loss=3.17]\u001b[A\n",
      "Iteration:  24%|██▍       | 36/147 [01:08<03:28,  1.87s/it, loss=2.77]\u001b[A\n",
      "Iteration:  25%|██▌       | 37/147 [01:08<03:25,  1.87s/it, loss=2.77]\u001b[A\n",
      "Iteration:  25%|██▌       | 37/147 [01:09<03:25,  1.87s/it, loss=2.66]\u001b[A\n",
      "Iteration:  26%|██▌       | 38/147 [01:10<03:23,  1.86s/it, loss=2.66]\u001b[A\n",
      "Iteration:  26%|██▌       | 38/147 [01:11<03:23,  1.86s/it, loss=2.87]\u001b[A\n",
      "Iteration:  27%|██▋       | 39/147 [01:12<03:21,  1.86s/it, loss=2.87]\u001b[A\n",
      "Iteration:  27%|██▋       | 39/147 [01:13<03:21,  1.86s/it, loss=2.92]\u001b[A\n",
      "Iteration:  27%|██▋       | 40/147 [01:14<03:19,  1.86s/it, loss=2.92]\u001b[A\n",
      "Iteration:  27%|██▋       | 40/147 [01:15<03:19,  1.86s/it, loss=2.54]\u001b[A\n",
      "Iteration:  28%|██▊       | 41/147 [01:15<03:16,  1.86s/it, loss=2.54]\u001b[A\n",
      "Iteration:  28%|██▊       | 41/147 [01:17<03:16,  1.86s/it, loss=2.61]\u001b[A\n",
      "Iteration:  29%|██▊       | 42/147 [01:17<03:15,  1.86s/it, loss=2.61]\u001b[A\n",
      "Iteration:  29%|██▊       | 42/147 [01:19<03:15,  1.86s/it, loss=2.73]\u001b[A\n",
      "Iteration:  29%|██▉       | 43/147 [01:19<03:13,  1.86s/it, loss=2.73]\u001b[A\n",
      "Iteration:  29%|██▉       | 43/147 [01:21<03:13,  1.86s/it, loss=2.6] \u001b[A\n",
      "Iteration:  30%|██▉       | 44/147 [01:21<03:11,  1.86s/it, loss=2.6]\u001b[A\n",
      "Iteration:  30%|██▉       | 44/147 [01:22<03:11,  1.86s/it, loss=2.69]\u001b[A\n",
      "Iteration:  31%|███       | 45/147 [01:23<03:09,  1.86s/it, loss=2.69]\u001b[A\n",
      "Iteration:  31%|███       | 45/147 [01:24<03:09,  1.86s/it, loss=3.41]\u001b[A\n",
      "Iteration:  31%|███▏      | 46/147 [01:25<03:07,  1.86s/it, loss=3.41]\u001b[A\n",
      "Iteration:  31%|███▏      | 46/147 [01:26<03:07,  1.86s/it, loss=3.5] \u001b[A\n",
      "Iteration:  32%|███▏      | 47/147 [01:27<03:05,  1.86s/it, loss=3.5]\u001b[A\n",
      "Iteration:  32%|███▏      | 47/147 [01:28<03:05,  1.86s/it, loss=3.03]\u001b[A\n",
      "Iteration:  33%|███▎      | 48/147 [01:28<03:03,  1.86s/it, loss=3.03]\u001b[A\n",
      "Iteration:  33%|███▎      | 48/147 [01:30<03:03,  1.86s/it, loss=3.31]\u001b[A\n",
      "Iteration:  33%|███▎      | 49/147 [01:30<03:01,  1.86s/it, loss=3.31]\u001b[A\n",
      "Iteration:  33%|███▎      | 49/147 [01:32<03:01,  1.86s/it, loss=2.8] \u001b[A\n",
      "Iteration:  34%|███▍      | 50/147 [01:32<03:00,  1.86s/it, loss=2.8]\u001b[A\n",
      "Iteration:  34%|███▍      | 50/147 [01:34<03:00,  1.86s/it, loss=2.95]\u001b[A\n",
      "Iteration:  35%|███▍      | 51/147 [01:34<02:58,  1.86s/it, loss=2.95]\u001b[A\n",
      "Iteration:  35%|███▍      | 51/147 [01:35<02:58,  1.86s/it, loss=2.57]\u001b[A\n",
      "Iteration:  35%|███▌      | 52/147 [01:36<02:56,  1.86s/it, loss=2.57]\u001b[A\n",
      "Iteration:  35%|███▌      | 52/147 [01:37<02:56,  1.86s/it, loss=2.69]\u001b[A\n",
      "Iteration:  36%|███▌      | 53/147 [01:38<02:54,  1.86s/it, loss=2.69]\u001b[A\n",
      "Iteration:  36%|███▌      | 53/147 [01:39<02:54,  1.86s/it, loss=2.75]\u001b[A\n",
      "Iteration:  37%|███▋      | 54/147 [01:40<02:52,  1.86s/it, loss=2.75]\u001b[A\n",
      "Iteration:  37%|███▋      | 54/147 [01:41<02:52,  1.86s/it, loss=3.13]\u001b[A\n",
      "Iteration:  37%|███▋      | 55/147 [01:41<02:50,  1.86s/it, loss=3.13]\u001b[A\n",
      "Iteration:  37%|███▋      | 55/147 [01:43<02:50,  1.86s/it, loss=2.8] \u001b[A\n",
      "Iteration:  38%|███▊      | 56/147 [01:43<02:49,  1.86s/it, loss=2.8]\u001b[A\n",
      "Iteration:  38%|███▊      | 56/147 [01:45<02:49,  1.86s/it, loss=2.91]\u001b[A\n",
      "Iteration:  39%|███▉      | 57/147 [01:45<02:47,  1.86s/it, loss=2.91]\u001b[A\n",
      "Iteration:  39%|███▉      | 57/147 [01:47<02:47,  1.86s/it, loss=2.61]\u001b[A\n",
      "Iteration:  39%|███▉      | 58/147 [01:47<02:45,  1.86s/it, loss=2.61]\u001b[A\n",
      "Iteration:  39%|███▉      | 58/147 [01:48<02:45,  1.86s/it, loss=3.11]\u001b[A\n",
      "Iteration:  40%|████      | 59/147 [01:49<02:43,  1.85s/it, loss=3.11]\u001b[A\n",
      "Iteration:  40%|████      | 59/147 [01:50<02:43,  1.85s/it, loss=2.53]\u001b[A\n",
      "Iteration:  41%|████      | 60/147 [01:51<02:41,  1.86s/it, loss=2.53]\u001b[A\n",
      "Iteration:  41%|████      | 60/147 [01:52<02:41,  1.86s/it, loss=2.77]\u001b[A\n",
      "Iteration:  41%|████▏     | 61/147 [01:53<02:39,  1.86s/it, loss=2.77]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  41%|████▏     | 61/147 [01:54<02:39,  1.86s/it, loss=3.48]\u001b[A\n",
      "Iteration:  42%|████▏     | 62/147 [01:54<02:37,  1.86s/it, loss=3.48]\u001b[A\n",
      "Iteration:  42%|████▏     | 62/147 [01:56<02:37,  1.86s/it, loss=3.13]\u001b[A\n",
      "Iteration:  43%|████▎     | 63/147 [01:56<02:35,  1.86s/it, loss=3.13]\u001b[A\n",
      "Iteration:  43%|████▎     | 63/147 [01:58<02:35,  1.86s/it, loss=3.55]\u001b[A\n",
      "Iteration:  44%|████▎     | 64/147 [01:58<02:33,  1.85s/it, loss=3.55]\u001b[A\n",
      "Iteration:  44%|████▎     | 64/147 [01:59<02:33,  1.85s/it, loss=3.14]\u001b[A\n",
      "Iteration:  44%|████▍     | 65/147 [02:00<02:31,  1.85s/it, loss=3.14]\u001b[A\n",
      "Iteration:  44%|████▍     | 65/147 [02:01<02:31,  1.85s/it, loss=2.47]\u001b[A\n",
      "Iteration:  45%|████▍     | 66/147 [02:02<02:29,  1.85s/it, loss=2.47]\u001b[A\n",
      "Iteration:  45%|████▍     | 66/147 [02:03<02:29,  1.85s/it, loss=2.61]\u001b[A\n",
      "Iteration:  46%|████▌     | 67/147 [02:04<02:27,  1.85s/it, loss=2.61]\u001b[A\n",
      "Iteration:  46%|████▌     | 67/147 [02:05<02:27,  1.85s/it, loss=2.84]\u001b[A\n",
      "Iteration:  46%|████▋     | 68/147 [02:06<02:25,  1.85s/it, loss=2.84]\u001b[A\n",
      "Iteration:  46%|████▋     | 68/147 [02:07<02:25,  1.85s/it, loss=3.05]\u001b[A\n",
      "Iteration:  47%|████▋     | 69/147 [02:07<02:24,  1.85s/it, loss=3.05]\u001b[A\n",
      "Iteration:  47%|████▋     | 69/147 [02:09<02:24,  1.85s/it, loss=2.3] \u001b[A\n",
      "Iteration:  48%|████▊     | 70/147 [02:09<02:22,  1.85s/it, loss=2.3]\u001b[A\n",
      "Iteration:  48%|████▊     | 70/147 [02:11<02:22,  1.85s/it, loss=2.66]\u001b[A\n",
      "Iteration:  48%|████▊     | 71/147 [02:11<02:20,  1.85s/it, loss=2.66]\u001b[A\n",
      "Iteration:  48%|████▊     | 71/147 [02:12<02:20,  1.85s/it, loss=2.75]\u001b[A\n",
      "Iteration:  49%|████▉     | 72/147 [02:13<02:18,  1.85s/it, loss=2.75]\u001b[A\n",
      "Iteration:  49%|████▉     | 72/147 [02:14<02:18,  1.85s/it, loss=2.99]\u001b[A\n",
      "Iteration:  50%|████▉     | 73/147 [02:15<02:16,  1.85s/it, loss=2.99]\u001b[A\n",
      "Iteration:  50%|████▉     | 73/147 [02:16<02:16,  1.85s/it, loss=2.6] \u001b[A\n",
      "Iteration:  50%|█████     | 74/147 [02:17<02:15,  1.86s/it, loss=2.6]\u001b[A\n",
      "Iteration:  50%|█████     | 74/147 [02:18<02:15,  1.86s/it, loss=2.91]\u001b[A\n",
      "Iteration:  51%|█████     | 75/147 [02:19<02:13,  1.86s/it, loss=2.91]\u001b[A\n",
      "Iteration:  51%|█████     | 75/147 [02:20<02:13,  1.86s/it, loss=2.62]\u001b[A\n",
      "Iteration:  52%|█████▏    | 76/147 [02:20<02:11,  1.86s/it, loss=2.62]\u001b[A\n",
      "Iteration:  52%|█████▏    | 76/147 [02:22<02:11,  1.86s/it, loss=2.65]\u001b[A\n",
      "Iteration:  52%|█████▏    | 77/147 [02:22<02:10,  1.87s/it, loss=2.65]\u001b[A\n",
      "Iteration:  52%|█████▏    | 77/147 [02:24<02:10,  1.87s/it, loss=2.82]\u001b[A\n",
      "Iteration:  53%|█████▎    | 78/147 [02:24<02:11,  1.90s/it, loss=2.82]\u001b[A\n",
      "Iteration:  53%|█████▎    | 78/147 [02:26<02:11,  1.90s/it, loss=2.97]\u001b[A\n",
      "Iteration:  54%|█████▎    | 79/147 [02:26<02:10,  1.92s/it, loss=2.97]\u001b[A\n",
      "Iteration:  54%|█████▎    | 79/147 [02:28<02:10,  1.92s/it, loss=2.61]\u001b[A\n",
      "Iteration:  54%|█████▍    | 80/147 [02:28<02:07,  1.90s/it, loss=2.61]\u001b[A\n",
      "Iteration:  54%|█████▍    | 80/147 [02:29<02:07,  1.90s/it, loss=2.69]\u001b[A\n",
      "Iteration:  55%|█████▌    | 81/147 [02:30<02:05,  1.90s/it, loss=2.69]\u001b[A\n",
      "Iteration:  55%|█████▌    | 81/147 [02:31<02:05,  1.90s/it, loss=2.46]\u001b[A\n",
      "Iteration:  56%|█████▌    | 82/147 [02:32<02:04,  1.91s/it, loss=2.46]\u001b[A\n",
      "Iteration:  56%|█████▌    | 82/147 [02:33<02:04,  1.91s/it, loss=2.5] \u001b[A\n",
      "Iteration:  56%|█████▋    | 83/147 [02:34<02:03,  1.93s/it, loss=2.5]\u001b[A\n",
      "Iteration:  56%|█████▋    | 83/147 [02:35<02:03,  1.93s/it, loss=2.65]\u001b[A\n",
      "Iteration:  57%|█████▋    | 84/147 [02:36<02:00,  1.92s/it, loss=2.65]\u001b[A\n",
      "Iteration:  57%|█████▋    | 84/147 [02:37<02:00,  1.92s/it, loss=2.6] \u001b[A\n",
      "Iteration:  58%|█████▊    | 85/147 [02:38<01:58,  1.90s/it, loss=2.6]\u001b[A\n",
      "Iteration:  58%|█████▊    | 85/147 [02:39<01:58,  1.90s/it, loss=2.74]\u001b[A\n",
      "Iteration:  59%|█████▊    | 86/147 [02:39<01:55,  1.89s/it, loss=2.74]\u001b[A\n",
      "Iteration:  59%|█████▊    | 86/147 [02:41<01:55,  1.89s/it, loss=2.78]\u001b[A\n",
      "Iteration:  59%|█████▉    | 87/147 [02:41<01:52,  1.88s/it, loss=2.78]\u001b[A\n",
      "Iteration:  59%|█████▉    | 87/147 [02:43<01:52,  1.88s/it, loss=2.77]\u001b[A\n",
      "Iteration:  60%|█████▉    | 88/147 [02:43<01:50,  1.87s/it, loss=2.77]\u001b[A\n",
      "Iteration:  60%|█████▉    | 88/147 [02:45<01:50,  1.87s/it, loss=2.45]\u001b[A\n",
      "Iteration:  61%|██████    | 89/147 [02:45<01:48,  1.88s/it, loss=2.45]\u001b[A\n",
      "Iteration:  61%|██████    | 89/147 [02:46<01:48,  1.88s/it, loss=2.87]\u001b[A\n",
      "Iteration:  61%|██████    | 90/147 [02:47<01:47,  1.88s/it, loss=2.87]\u001b[A\n",
      "Iteration:  61%|██████    | 90/147 [02:48<01:47,  1.88s/it, loss=3.41]\u001b[A\n",
      "Iteration:  62%|██████▏   | 91/147 [02:49<01:44,  1.87s/it, loss=3.41]\u001b[A\n",
      "Iteration:  62%|██████▏   | 91/147 [02:50<01:44,  1.87s/it, loss=2.8] \u001b[A\n",
      "Iteration:  63%|██████▎   | 92/147 [02:51<01:42,  1.87s/it, loss=2.8]\u001b[A\n",
      "Iteration:  63%|██████▎   | 92/147 [02:52<01:42,  1.87s/it, loss=3.39]\u001b[A\n",
      "Iteration:  63%|██████▎   | 93/147 [02:53<01:41,  1.87s/it, loss=3.39]\u001b[A\n",
      "Iteration:  63%|██████▎   | 93/147 [02:54<01:41,  1.87s/it, loss=1.97]\u001b[A\n",
      "Iteration:  64%|██████▍   | 94/147 [02:54<01:39,  1.88s/it, loss=1.97]\u001b[A\n",
      "Iteration:  64%|██████▍   | 94/147 [02:56<01:39,  1.88s/it, loss=2.92]\u001b[A\n",
      "Iteration:  65%|██████▍   | 95/147 [02:56<01:37,  1.88s/it, loss=2.92]\u001b[A\n",
      "Iteration:  65%|██████▍   | 95/147 [02:58<01:37,  1.88s/it, loss=2.78]\u001b[A\n",
      "Iteration:  65%|██████▌   | 96/147 [02:58<01:35,  1.87s/it, loss=2.78]\u001b[A\n",
      "Iteration:  65%|██████▌   | 96/147 [03:00<01:35,  1.87s/it, loss=2.69]\u001b[A\n",
      "Iteration:  66%|██████▌   | 97/147 [03:00<01:33,  1.87s/it, loss=2.69]\u001b[A\n",
      "Iteration:  66%|██████▌   | 97/147 [03:01<01:33,  1.87s/it, loss=3.36]\u001b[A\n",
      "Iteration:  67%|██████▋   | 98/147 [03:02<01:31,  1.86s/it, loss=3.36]\u001b[A\n",
      "Iteration:  67%|██████▋   | 98/147 [03:03<01:31,  1.86s/it, loss=3.6] \u001b[A\n",
      "Iteration:  67%|██████▋   | 99/147 [03:04<01:29,  1.87s/it, loss=3.6]\u001b[A\n",
      "Iteration:  67%|██████▋   | 99/147 [03:05<01:29,  1.87s/it, loss=2.98]\u001b[A\n",
      "Iteration:  68%|██████▊   | 100/147 [03:06<01:27,  1.87s/it, loss=2.98]\u001b[A\n",
      "Iteration:  68%|██████▊   | 100/147 [03:07<01:27,  1.87s/it, loss=2.83]\u001b[A\n",
      "Iteration:  69%|██████▊   | 101/147 [03:07<01:25,  1.87s/it, loss=2.83]\u001b[A\n",
      "Iteration:  69%|██████▊   | 101/147 [03:09<01:25,  1.87s/it, loss=3.02]\u001b[A\n",
      "Iteration:  69%|██████▉   | 102/147 [03:09<01:23,  1.87s/it, loss=3.02]\u001b[A\n",
      "Iteration:  69%|██████▉   | 102/147 [03:11<01:23,  1.87s/it, loss=3.02]\u001b[A\n",
      "Iteration:  70%|███████   | 103/147 [03:11<01:22,  1.87s/it, loss=3.02]\u001b[A\n",
      "Iteration:  70%|███████   | 103/147 [03:13<01:22,  1.87s/it, loss=2.71]\u001b[A\n",
      "Iteration:  71%|███████   | 104/147 [03:13<01:20,  1.86s/it, loss=2.71]\u001b[A\n",
      "Iteration:  71%|███████   | 104/147 [03:14<01:20,  1.86s/it, loss=2.99]\u001b[A\n",
      "Iteration:  71%|███████▏  | 105/147 [03:15<01:18,  1.86s/it, loss=2.99]\u001b[A\n",
      "Iteration:  71%|███████▏  | 105/147 [03:16<01:18,  1.86s/it, loss=2.62]\u001b[A\n",
      "Iteration:  72%|███████▏  | 106/147 [03:17<01:16,  1.87s/it, loss=2.62]\u001b[A\n",
      "Iteration:  72%|███████▏  | 106/147 [03:18<01:16,  1.87s/it, loss=3.23]\u001b[A\n",
      "Iteration:  73%|███████▎  | 107/147 [03:19<01:14,  1.87s/it, loss=3.23]\u001b[A\n",
      "Iteration:  73%|███████▎  | 107/147 [03:20<01:14,  1.87s/it, loss=2.67]\u001b[A\n",
      "Iteration:  73%|███████▎  | 108/147 [03:21<01:12,  1.87s/it, loss=2.67]\u001b[A\n",
      "Iteration:  73%|███████▎  | 108/147 [03:22<01:12,  1.87s/it, loss=2.57]\u001b[A\n",
      "Iteration:  74%|███████▍  | 109/147 [03:22<01:10,  1.87s/it, loss=2.57]\u001b[A\n",
      "Iteration:  74%|███████▍  | 109/147 [03:24<01:10,  1.87s/it, loss=2.99]\u001b[A\n",
      "Iteration:  75%|███████▍  | 110/147 [03:24<01:09,  1.87s/it, loss=2.99]\u001b[A\n",
      "Iteration:  75%|███████▍  | 110/147 [03:26<01:09,  1.87s/it, loss=2.59]\u001b[A\n",
      "Iteration:  76%|███████▌  | 111/147 [03:26<01:07,  1.87s/it, loss=2.59]\u001b[A\n",
      "Iteration:  76%|███████▌  | 111/147 [03:28<01:07,  1.87s/it, loss=2.36]\u001b[A\n",
      "Iteration:  76%|███████▌  | 112/147 [03:28<01:05,  1.87s/it, loss=2.36]\u001b[A\n",
      "Iteration:  76%|███████▌  | 112/147 [03:29<01:05,  1.87s/it, loss=2.48]\u001b[A\n",
      "Iteration:  77%|███████▋  | 113/147 [03:30<01:03,  1.87s/it, loss=2.48]\u001b[A\n",
      "Iteration:  77%|███████▋  | 113/147 [03:31<01:03,  1.87s/it, loss=2.67]\u001b[A\n",
      "Iteration:  78%|███████▊  | 114/147 [03:32<01:01,  1.87s/it, loss=2.67]\u001b[A\n",
      "Iteration:  78%|███████▊  | 114/147 [03:33<01:01,  1.87s/it, loss=3.24]\u001b[A\n",
      "Iteration:  78%|███████▊  | 115/147 [03:34<00:59,  1.87s/it, loss=3.24]\u001b[A\n",
      "Iteration:  78%|███████▊  | 115/147 [03:35<00:59,  1.87s/it, loss=2.41]\u001b[A\n",
      "Iteration:  79%|███████▉  | 116/147 [03:36<00:58,  1.87s/it, loss=2.41]\u001b[A\n",
      "Iteration:  79%|███████▉  | 116/147 [03:37<00:58,  1.87s/it, loss=3.07]\u001b[A\n",
      "Iteration:  80%|███████▉  | 117/147 [03:37<00:56,  1.87s/it, loss=3.07]\u001b[A\n",
      "Iteration:  80%|███████▉  | 117/147 [03:39<00:56,  1.87s/it, loss=2.67]\u001b[A\n",
      "Iteration:  80%|████████  | 118/147 [03:39<00:54,  1.86s/it, loss=2.67]\u001b[A\n",
      "Iteration:  80%|████████  | 118/147 [03:41<00:54,  1.86s/it, loss=2.8] \u001b[A\n",
      "Iteration:  81%|████████  | 119/147 [03:41<00:51,  1.86s/it, loss=2.8]\u001b[A\n",
      "Iteration:  81%|████████  | 119/147 [03:42<00:51,  1.86s/it, loss=2.32]\u001b[A\n",
      "Iteration:  82%|████████▏ | 120/147 [03:43<00:50,  1.85s/it, loss=2.32]\u001b[A\n",
      "Iteration:  82%|████████▏ | 120/147 [03:44<00:50,  1.85s/it, loss=2.39]\u001b[A\n",
      "Iteration:  82%|████████▏ | 121/147 [03:45<00:48,  1.86s/it, loss=2.39]\u001b[A\n",
      "Iteration:  82%|████████▏ | 121/147 [03:46<00:48,  1.86s/it, loss=2.24]\u001b[A\n",
      "Iteration:  83%|████████▎ | 122/147 [03:47<00:46,  1.86s/it, loss=2.24]\u001b[A\n",
      "Iteration:  83%|████████▎ | 122/147 [03:48<00:46,  1.86s/it, loss=3.05]\u001b[A\n",
      "Iteration:  84%|████████▎ | 123/147 [03:49<00:44,  1.86s/it, loss=3.05]\u001b[A\n",
      "Iteration:  84%|████████▎ | 123/147 [03:50<00:44,  1.86s/it, loss=2.83]\u001b[A\n",
      "Iteration:  84%|████████▍ | 124/147 [03:50<00:42,  1.86s/it, loss=2.83]\u001b[A\n",
      "Iteration:  84%|████████▍ | 124/147 [03:52<00:42,  1.86s/it, loss=2.63]\u001b[A\n",
      "Iteration:  85%|████████▌ | 125/147 [03:52<00:40,  1.86s/it, loss=2.63]\u001b[A\n",
      "Iteration:  85%|████████▌ | 125/147 [03:54<00:40,  1.86s/it, loss=2.66]\u001b[A\n",
      "Iteration:  86%|████████▌ | 126/147 [03:54<00:39,  1.86s/it, loss=2.66]\u001b[A\n",
      "Iteration:  86%|████████▌ | 126/147 [03:55<00:39,  1.86s/it, loss=2.99]\u001b[A\n",
      "Iteration:  86%|████████▋ | 127/147 [03:56<00:37,  1.86s/it, loss=2.99]\u001b[A\n",
      "Iteration:  86%|████████▋ | 127/147 [03:57<00:37,  1.86s/it, loss=2.65]\u001b[A\n",
      "Iteration:  87%|████████▋ | 128/147 [03:58<00:35,  1.86s/it, loss=2.65]\u001b[A\n",
      "Iteration:  87%|████████▋ | 128/147 [03:59<00:35,  1.86s/it, loss=2.99]\u001b[A\n",
      "Iteration:  88%|████████▊ | 129/147 [04:00<00:33,  1.86s/it, loss=2.99]\u001b[A\n",
      "Iteration:  88%|████████▊ | 129/147 [04:01<00:33,  1.86s/it, loss=3.46]\u001b[A\n",
      "Iteration:  88%|████████▊ | 130/147 [04:02<00:31,  1.85s/it, loss=3.46]\u001b[A\n",
      "Iteration:  88%|████████▊ | 130/147 [04:03<00:31,  1.85s/it, loss=3.02]\u001b[A\n",
      "Iteration:  89%|████████▉ | 131/147 [04:03<00:29,  1.85s/it, loss=3.02]\u001b[A\n",
      "Iteration:  89%|████████▉ | 131/147 [04:05<00:29,  1.85s/it, loss=2.59]\u001b[A\n",
      "Iteration:  90%|████████▉ | 132/147 [04:05<00:27,  1.85s/it, loss=2.59]\u001b[A\n",
      "Iteration:  90%|████████▉ | 132/147 [04:07<00:27,  1.85s/it, loss=2.94]\u001b[A\n",
      "Iteration:  90%|█████████ | 133/147 [04:07<00:25,  1.85s/it, loss=2.94]\u001b[A\n",
      "Iteration:  90%|█████████ | 133/147 [04:08<00:25,  1.85s/it, loss=3.1] \u001b[A\n",
      "Iteration:  91%|█████████ | 134/147 [04:09<00:24,  1.86s/it, loss=3.1]\u001b[A\n",
      "Iteration:  91%|█████████ | 134/147 [04:10<00:24,  1.86s/it, loss=2.53]\u001b[A\n",
      "Iteration:  92%|█████████▏| 135/147 [04:11<00:22,  1.88s/it, loss=2.53]\u001b[A\n",
      "Iteration:  92%|█████████▏| 135/147 [04:12<00:22,  1.88s/it, loss=2.27]\u001b[A\n",
      "Iteration:  93%|█████████▎| 136/147 [04:13<00:20,  1.89s/it, loss=2.27]\u001b[A\n",
      "Iteration:  93%|█████████▎| 136/147 [04:14<00:20,  1.89s/it, loss=2.6] \u001b[A\n",
      "Iteration:  93%|█████████▎| 137/147 [04:15<00:18,  1.89s/it, loss=2.6]\u001b[A\n",
      "Iteration:  93%|█████████▎| 137/147 [04:16<00:18,  1.89s/it, loss=2.44]\u001b[A\n",
      "Iteration:  94%|█████████▍| 138/147 [04:17<00:16,  1.88s/it, loss=2.44]\u001b[A\n",
      "Iteration:  94%|█████████▍| 138/147 [04:18<00:16,  1.88s/it, loss=2.43]\u001b[A\n",
      "Iteration:  95%|█████████▍| 139/147 [04:18<00:15,  1.88s/it, loss=2.43]\u001b[A\n",
      "Iteration:  95%|█████████▍| 139/147 [04:20<00:15,  1.88s/it, loss=2.93]\u001b[A\n",
      "Iteration:  95%|█████████▌| 140/147 [04:20<00:13,  1.88s/it, loss=2.93]\u001b[A\n",
      "Iteration:  95%|█████████▌| 140/147 [04:22<00:13,  1.88s/it, loss=2.43]\u001b[A\n",
      "Iteration:  96%|█████████▌| 141/147 [04:22<00:11,  1.87s/it, loss=2.43]\u001b[A\n",
      "Iteration:  96%|█████████▌| 141/147 [04:24<00:11,  1.87s/it, loss=2.38]\u001b[A\n",
      "Iteration:  97%|█████████▋| 142/147 [04:24<00:09,  1.87s/it, loss=2.38]\u001b[A\n",
      "Iteration:  97%|█████████▋| 142/147 [04:25<00:09,  1.87s/it, loss=2.68]\u001b[A\n",
      "Iteration:  97%|█████████▋| 143/147 [04:26<00:07,  1.89s/it, loss=2.68]\u001b[A\n",
      "Iteration:  97%|█████████▋| 143/147 [04:27<00:07,  1.89s/it, loss=2.94]\u001b[A\n",
      "Iteration:  98%|█████████▊| 144/147 [04:28<00:05,  1.92s/it, loss=2.94]\u001b[A\n",
      "Iteration:  98%|█████████▊| 144/147 [04:29<00:05,  1.92s/it, loss=3.11]\u001b[A\n",
      "Iteration:  99%|█████████▊| 145/147 [04:30<00:03,  1.92s/it, loss=3.11]\u001b[A\n",
      "Iteration:  99%|█████████▊| 145/147 [04:31<00:03,  1.92s/it, loss=2.48]\u001b[A\n",
      "Iteration:  99%|█████████▉| 146/147 [04:32<00:01,  1.90s/it, loss=2.48]\u001b[A\n",
      "Iteration:  99%|█████████▉| 146/147 [04:32<00:01,  1.90s/it, loss=2.98]\u001b[A\n",
      "Epoch:  80%|████████  | 4/5 [17:38<04:23, 263.20s/it]69s/it, loss=2.98]\u001b[A\n",
      "Iteration:   0%|          | 0/147 [00:00<?, ?it/s]\u001b[A\n",
      "Iteration:   0%|          | 0/147 [00:01<?, ?it/s, loss=2.59]\u001b[A\n",
      "Iteration:   1%|          | 1/147 [00:01<04:44,  1.95s/it, loss=2.59]\u001b[A\n",
      "Iteration:   1%|          | 1/147 [00:03<04:44,  1.95s/it, loss=3.22]\u001b[A\n",
      "Iteration:   1%|▏         | 2/147 [00:03<04:44,  1.96s/it, loss=3.22]\u001b[A\n",
      "Iteration:   1%|▏         | 2/147 [00:05<04:44,  1.96s/it, loss=2.42]\u001b[A\n",
      "Iteration:   2%|▏         | 3/147 [00:06<04:46,  1.99s/it, loss=2.42]\u001b[A\n",
      "Iteration:   2%|▏         | 3/147 [00:07<04:46,  1.99s/it, loss=2.58]\u001b[A\n",
      "Iteration:   3%|▎         | 4/147 [00:07<04:38,  1.95s/it, loss=2.58]\u001b[A\n",
      "Iteration:   3%|▎         | 4/147 [00:09<04:38,  1.95s/it, loss=2.14]\u001b[A\n",
      "Iteration:   3%|▎         | 5/147 [00:09<04:31,  1.91s/it, loss=2.14]\u001b[A\n",
      "Iteration:   3%|▎         | 5/147 [00:11<04:31,  1.91s/it, loss=2.39]\u001b[A\n",
      "Iteration:   4%|▍         | 6/147 [00:11<04:25,  1.88s/it, loss=2.39]\u001b[A\n",
      "Iteration:   4%|▍         | 6/147 [00:12<04:25,  1.88s/it, loss=2.59]\u001b[A\n",
      "Iteration:   5%|▍         | 7/147 [00:13<04:20,  1.86s/it, loss=2.59]\u001b[A\n",
      "Iteration:   5%|▍         | 7/147 [00:14<04:20,  1.86s/it, loss=2.15]\u001b[A\n",
      "Iteration:   5%|▌         | 8/147 [00:15<04:16,  1.84s/it, loss=2.15]\u001b[A\n",
      "Iteration:   5%|▌         | 8/147 [00:16<04:16,  1.84s/it, loss=2.24]\u001b[A\n",
      "Iteration:   6%|▌         | 9/147 [00:16<04:13,  1.83s/it, loss=2.24]\u001b[A\n",
      "Iteration:   6%|▌         | 9/147 [00:18<04:13,  1.83s/it, loss=2.12]\u001b[A\n",
      "Iteration:   7%|▋         | 10/147 [00:19<04:45,  2.08s/it, loss=2.12]\u001b[A\n",
      "Iteration:   7%|▋         | 10/147 [00:21<04:45,  2.08s/it, loss=1.72]\u001b[A\n",
      "Iteration:   7%|▋         | 11/147 [00:23<05:47,  2.55s/it, loss=1.72]\u001b[A\n",
      "Iteration:   7%|▋         | 11/147 [00:25<05:47,  2.55s/it, loss=2.14]\u001b[A\n",
      "Iteration:   8%|▊         | 12/147 [00:26<06:25,  2.85s/it, loss=2.14]\u001b[A\n",
      "Iteration:   8%|▊         | 12/147 [00:28<06:25,  2.85s/it, loss=2.32]\u001b[A\n",
      "Iteration:   9%|▉         | 13/147 [00:28<05:40,  2.54s/it, loss=2.32]\u001b[A\n",
      "Iteration:   9%|▉         | 13/147 [00:29<05:40,  2.54s/it, loss=2.78]\u001b[A\n",
      "Iteration:  10%|▉         | 14/147 [00:30<05:08,  2.32s/it, loss=2.78]\u001b[A\n",
      "Iteration:  10%|▉         | 14/147 [00:31<05:08,  2.32s/it, loss=2.89]\u001b[A\n",
      "Iteration:  10%|█         | 15/147 [00:32<04:50,  2.20s/it, loss=2.89]\u001b[A\n",
      "Iteration:  10%|█         | 15/147 [00:33<04:50,  2.20s/it, loss=2.48]\u001b[A\n",
      "Iteration:  11%|█         | 16/147 [00:34<04:33,  2.09s/it, loss=2.48]\u001b[A\n",
      "Iteration:  11%|█         | 16/147 [00:35<04:33,  2.09s/it, loss=2.55]\u001b[A\n",
      "Iteration:  12%|█▏        | 17/147 [00:35<04:21,  2.01s/it, loss=2.55]\u001b[A\n",
      "Iteration:  12%|█▏        | 17/147 [00:37<04:21,  2.01s/it, loss=2.39]\u001b[A\n",
      "Iteration:  12%|█▏        | 18/147 [00:37<04:16,  1.99s/it, loss=2.39]\u001b[A\n",
      "Iteration:  12%|█▏        | 18/147 [00:39<04:16,  1.99s/it, loss=2.83]\u001b[A\n",
      "Iteration:  13%|█▎        | 19/147 [00:39<04:11,  1.97s/it, loss=2.83]\u001b[A\n",
      "Iteration:  13%|█▎        | 19/147 [00:41<04:11,  1.97s/it, loss=2.28]\u001b[A\n",
      "Iteration:  14%|█▎        | 20/147 [00:41<04:08,  1.96s/it, loss=2.28]\u001b[A\n",
      "Iteration:  14%|█▎        | 20/147 [00:43<04:08,  1.96s/it, loss=2.69]\u001b[A\n",
      "Iteration:  14%|█▍        | 21/147 [00:43<04:05,  1.95s/it, loss=2.69]\u001b[A\n",
      "Iteration:  14%|█▍        | 21/147 [00:45<04:05,  1.95s/it, loss=2.08]\u001b[A\n",
      "Iteration:  15%|█▍        | 22/147 [00:45<03:59,  1.92s/it, loss=2.08]\u001b[A\n",
      "Iteration:  15%|█▍        | 22/147 [00:46<03:59,  1.92s/it, loss=2.9] \u001b[A\n",
      "Iteration:  16%|█▌        | 23/147 [00:47<03:54,  1.89s/it, loss=2.9]\u001b[A\n",
      "Iteration:  16%|█▌        | 23/147 [00:48<03:54,  1.89s/it, loss=2.45]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  16%|█▋        | 24/147 [00:49<03:54,  1.91s/it, loss=2.45]\u001b[A\n",
      "Iteration:  16%|█▋        | 24/147 [00:50<03:54,  1.91s/it, loss=2.67]\u001b[A\n",
      "Iteration:  17%|█▋        | 25/147 [00:51<03:53,  1.91s/it, loss=2.67]\u001b[A\n",
      "Iteration:  17%|█▋        | 25/147 [00:52<03:53,  1.91s/it, loss=2.42]\u001b[A\n",
      "Iteration:  18%|█▊        | 26/147 [00:53<03:49,  1.89s/it, loss=2.42]\u001b[A\n",
      "Iteration:  18%|█▊        | 26/147 [00:54<03:49,  1.89s/it, loss=2.02]\u001b[A\n",
      "Iteration:  18%|█▊        | 27/147 [00:55<03:49,  1.91s/it, loss=2.02]\u001b[A\n",
      "Iteration:  18%|█▊        | 27/147 [00:56<03:49,  1.91s/it, loss=2.74]\u001b[A\n",
      "Iteration:  19%|█▉        | 28/147 [00:56<03:43,  1.88s/it, loss=2.74]\u001b[A\n",
      "Iteration:  19%|█▉        | 28/147 [00:58<03:43,  1.88s/it, loss=1.94]\u001b[A\n",
      "Iteration:  20%|█▉        | 29/147 [00:58<03:39,  1.86s/it, loss=1.94]\u001b[A\n",
      "Iteration:  20%|█▉        | 29/147 [01:00<03:39,  1.86s/it, loss=2.79]\u001b[A\n",
      "Iteration:  20%|██        | 30/147 [01:00<03:36,  1.85s/it, loss=2.79]\u001b[A\n",
      "Iteration:  20%|██        | 30/147 [01:01<03:36,  1.85s/it, loss=1.99]\u001b[A\n",
      "Iteration:  21%|██        | 31/147 [01:02<03:32,  1.84s/it, loss=1.99]\u001b[A\n",
      "Iteration:  21%|██        | 31/147 [01:03<03:32,  1.84s/it, loss=2.34]\u001b[A\n",
      "Iteration:  22%|██▏       | 32/147 [01:04<03:30,  1.83s/it, loss=2.34]\u001b[A\n",
      "Iteration:  22%|██▏       | 32/147 [01:05<03:30,  1.83s/it, loss=2.66]\u001b[A\n",
      "Iteration:  22%|██▏       | 33/147 [01:05<03:27,  1.82s/it, loss=2.66]\u001b[A\n",
      "Iteration:  22%|██▏       | 33/147 [01:07<03:27,  1.82s/it, loss=2.05]\u001b[A\n",
      "Iteration:  23%|██▎       | 34/147 [01:07<03:30,  1.86s/it, loss=2.05]\u001b[A\n",
      "Iteration:  23%|██▎       | 34/147 [01:09<03:30,  1.86s/it, loss=2.51]\u001b[A\n",
      "Iteration:  24%|██▍       | 35/147 [01:09<03:33,  1.90s/it, loss=2.51]\u001b[A\n",
      "Iteration:  24%|██▍       | 35/147 [01:11<03:33,  1.90s/it, loss=2.48]\u001b[A\n",
      "Iteration:  24%|██▍       | 36/147 [01:11<03:33,  1.92s/it, loss=2.48]\u001b[A\n",
      "Iteration:  24%|██▍       | 36/147 [01:13<03:33,  1.92s/it, loss=2.37]\u001b[A\n",
      "Iteration:  25%|██▌       | 37/147 [01:13<03:27,  1.89s/it, loss=2.37]\u001b[A\n",
      "Iteration:  25%|██▌       | 37/147 [01:15<03:27,  1.89s/it, loss=2.38]\u001b[A\n",
      "Iteration:  26%|██▌       | 38/147 [01:15<03:26,  1.90s/it, loss=2.38]\u001b[A\n",
      "Iteration:  26%|██▌       | 38/147 [01:16<03:26,  1.90s/it, loss=2.16]\u001b[A\n",
      "Iteration:  27%|██▋       | 39/147 [01:17<03:24,  1.89s/it, loss=2.16]\u001b[A\n",
      "Iteration:  27%|██▋       | 39/147 [01:18<03:24,  1.89s/it, loss=1.98]\u001b[A\n",
      "Iteration:  27%|██▋       | 40/147 [01:19<03:24,  1.91s/it, loss=1.98]\u001b[A\n",
      "Iteration:  27%|██▋       | 40/147 [01:20<03:24,  1.91s/it, loss=2.54]\u001b[A\n",
      "Iteration:  28%|██▊       | 41/147 [01:21<03:22,  1.91s/it, loss=2.54]\u001b[A\n",
      "Iteration:  28%|██▊       | 41/147 [01:22<03:22,  1.91s/it, loss=2.89]\u001b[A\n",
      "Iteration:  29%|██▊       | 42/147 [01:23<03:18,  1.89s/it, loss=2.89]\u001b[A\n",
      "Iteration:  29%|██▊       | 42/147 [01:24<03:18,  1.89s/it, loss=2.57]\u001b[A\n",
      "Iteration:  29%|██▉       | 43/147 [01:25<03:18,  1.91s/it, loss=2.57]\u001b[A\n",
      "Iteration:  29%|██▉       | 43/147 [01:26<03:18,  1.91s/it, loss=2.04]\u001b[A\n",
      "Iteration:  30%|██▉       | 44/147 [01:26<03:13,  1.88s/it, loss=2.04]\u001b[A\n",
      "Iteration:  30%|██▉       | 44/147 [01:28<03:13,  1.88s/it, loss=2.13]\u001b[A\n",
      "Iteration:  31%|███       | 45/147 [01:28<03:09,  1.86s/it, loss=2.13]\u001b[A\n",
      "Iteration:  31%|███       | 45/147 [01:30<03:09,  1.86s/it, loss=2.24]\u001b[A\n",
      "Iteration:  31%|███▏      | 46/147 [01:30<03:11,  1.89s/it, loss=2.24]\u001b[A\n",
      "Iteration:  31%|███▏      | 46/147 [01:32<03:11,  1.89s/it, loss=2.65]\u001b[A\n",
      "Iteration:  32%|███▏      | 47/147 [01:32<03:09,  1.89s/it, loss=2.65]\u001b[A\n",
      "Iteration:  32%|███▏      | 47/147 [01:33<03:09,  1.89s/it, loss=2.67]\u001b[A\n",
      "Iteration:  33%|███▎      | 48/147 [01:34<03:07,  1.90s/it, loss=2.67]\u001b[A\n",
      "Iteration:  33%|███▎      | 48/147 [01:35<03:07,  1.90s/it, loss=2.13]\u001b[A\n",
      "Iteration:  33%|███▎      | 49/147 [01:36<03:07,  1.91s/it, loss=2.13]\u001b[A\n",
      "Iteration:  33%|███▎      | 49/147 [01:37<03:07,  1.91s/it, loss=2.4] \u001b[A\n",
      "Iteration:  34%|███▍      | 50/147 [01:38<03:07,  1.93s/it, loss=2.4]\u001b[A\n",
      "Iteration:  34%|███▍      | 50/147 [01:39<03:07,  1.93s/it, loss=3.03]\u001b[A\n",
      "Iteration:  35%|███▍      | 51/147 [01:40<03:01,  1.89s/it, loss=3.03]\u001b[A\n",
      "Iteration:  35%|███▍      | 51/147 [01:41<03:01,  1.89s/it, loss=2.54]\u001b[A\n",
      "Iteration:  35%|███▌      | 52/147 [01:42<02:57,  1.87s/it, loss=2.54]\u001b[A\n",
      "Iteration:  35%|███▌      | 52/147 [01:43<02:57,  1.87s/it, loss=3.17]\u001b[A\n",
      "Iteration:  36%|███▌      | 53/147 [01:43<02:58,  1.90s/it, loss=3.17]\u001b[A\n",
      "Iteration:  36%|███▌      | 53/147 [01:45<02:58,  1.90s/it, loss=2.76]\u001b[A\n",
      "Iteration:  37%|███▋      | 54/147 [01:45<02:57,  1.91s/it, loss=2.76]\u001b[A\n",
      "Iteration:  37%|███▋      | 54/147 [01:47<02:57,  1.91s/it, loss=3.02]\u001b[A\n",
      "Iteration:  37%|███▋      | 55/147 [01:47<02:53,  1.88s/it, loss=3.02]\u001b[A\n",
      "Iteration:  37%|███▋      | 55/147 [01:49<02:53,  1.88s/it, loss=2.1] \u001b[A\n",
      "Iteration:  38%|███▊      | 56/147 [01:49<02:49,  1.86s/it, loss=2.1]\u001b[A\n",
      "Iteration:  38%|███▊      | 56/147 [01:50<02:49,  1.86s/it, loss=2.13]\u001b[A\n",
      "Iteration:  39%|███▉      | 57/147 [01:51<02:46,  1.85s/it, loss=2.13]\u001b[A\n",
      "Iteration:  39%|███▉      | 57/147 [01:52<02:46,  1.85s/it, loss=2.96]\u001b[A\n",
      "Iteration:  39%|███▉      | 58/147 [01:53<02:43,  1.84s/it, loss=2.96]\u001b[A\n",
      "Iteration:  39%|███▉      | 58/147 [01:54<02:43,  1.84s/it, loss=2.35]\u001b[A\n",
      "Iteration:  40%|████      | 59/147 [01:54<02:41,  1.83s/it, loss=2.35]\u001b[A\n",
      "Iteration:  40%|████      | 59/147 [01:56<02:41,  1.83s/it, loss=2.53]\u001b[A\n",
      "Iteration:  41%|████      | 60/147 [01:56<02:38,  1.83s/it, loss=2.53]\u001b[A\n",
      "Iteration:  41%|████      | 60/147 [01:58<02:38,  1.83s/it, loss=2.32]\u001b[A\n",
      "Iteration:  41%|████▏     | 61/147 [01:58<02:36,  1.82s/it, loss=2.32]\u001b[A\n",
      "Iteration:  41%|████▏     | 61/147 [01:59<02:36,  1.82s/it, loss=2.32]\u001b[A\n",
      "Iteration:  42%|████▏     | 62/147 [02:00<02:34,  1.82s/it, loss=2.32]\u001b[A\n",
      "Iteration:  42%|████▏     | 62/147 [02:01<02:34,  1.82s/it, loss=1.86]\u001b[A\n",
      "Iteration:  43%|████▎     | 63/147 [02:02<02:32,  1.82s/it, loss=1.86]\u001b[A\n",
      "Iteration:  43%|████▎     | 63/147 [02:03<02:32,  1.82s/it, loss=2.4] \u001b[A\n",
      "Iteration:  44%|████▎     | 64/147 [02:04<02:30,  1.82s/it, loss=2.4]\u001b[A\n",
      "Iteration:  44%|████▎     | 64/147 [02:05<02:30,  1.82s/it, loss=2.36]\u001b[A\n",
      "Iteration:  44%|████▍     | 65/147 [02:05<02:28,  1.81s/it, loss=2.36]\u001b[A\n",
      "Iteration:  44%|████▍     | 65/147 [02:07<02:28,  1.81s/it, loss=2.48]\u001b[A\n",
      "Iteration:  45%|████▍     | 66/147 [02:07<02:26,  1.81s/it, loss=2.48]\u001b[A\n",
      "Iteration:  45%|████▍     | 66/147 [02:09<02:26,  1.81s/it, loss=2.94]\u001b[A\n",
      "Iteration:  46%|████▌     | 67/147 [02:09<02:25,  1.81s/it, loss=2.94]\u001b[A\n",
      "Iteration:  46%|████▌     | 67/147 [02:10<02:25,  1.81s/it, loss=2.49]\u001b[A\n",
      "Iteration:  46%|████▋     | 68/147 [02:11<02:23,  1.82s/it, loss=2.49]\u001b[A\n",
      "Iteration:  46%|████▋     | 68/147 [02:12<02:23,  1.82s/it, loss=3.05]\u001b[A\n",
      "Iteration:  47%|████▋     | 69/147 [02:13<02:21,  1.81s/it, loss=3.05]\u001b[A\n",
      "Iteration:  47%|████▋     | 69/147 [02:14<02:21,  1.81s/it, loss=2.63]\u001b[A\n",
      "Iteration:  48%|████▊     | 70/147 [02:14<02:19,  1.81s/it, loss=2.63]\u001b[A\n",
      "Iteration:  48%|████▊     | 70/147 [02:16<02:19,  1.81s/it, loss=2.38]\u001b[A\n",
      "Iteration:  48%|████▊     | 71/147 [02:16<02:17,  1.81s/it, loss=2.38]\u001b[A\n",
      "Iteration:  48%|████▊     | 71/147 [02:18<02:17,  1.81s/it, loss=2.59]\u001b[A\n",
      "Iteration:  49%|████▉     | 72/147 [02:18<02:16,  1.81s/it, loss=2.59]\u001b[A\n",
      "Iteration:  49%|████▉     | 72/147 [02:19<02:16,  1.81s/it, loss=2.69]\u001b[A\n",
      "Iteration:  50%|████▉     | 73/147 [02:20<02:14,  1.81s/it, loss=2.69]\u001b[A\n",
      "Iteration:  50%|████▉     | 73/147 [02:21<02:14,  1.81s/it, loss=2.91]\u001b[A\n",
      "Iteration:  50%|█████     | 74/147 [02:22<02:12,  1.82s/it, loss=2.91]\u001b[A\n",
      "Iteration:  50%|█████     | 74/147 [02:23<02:12,  1.82s/it, loss=2.77]\u001b[A\n",
      "Iteration:  51%|█████     | 75/147 [02:24<02:10,  1.81s/it, loss=2.77]\u001b[A\n",
      "Iteration:  51%|█████     | 75/147 [02:25<02:10,  1.81s/it, loss=2.59]\u001b[A\n",
      "Iteration:  52%|█████▏    | 76/147 [02:25<02:08,  1.81s/it, loss=2.59]\u001b[A\n",
      "Iteration:  52%|█████▏    | 76/147 [02:27<02:08,  1.81s/it, loss=2.81]\u001b[A\n",
      "Iteration:  52%|█████▏    | 77/147 [02:29<02:35,  2.23s/it, loss=2.81]\u001b[A\n",
      "Iteration:  52%|█████▏    | 77/147 [02:31<02:35,  2.23s/it, loss=2.33]\u001b[A\n",
      "Iteration:  53%|█████▎    | 78/147 [02:32<03:06,  2.71s/it, loss=2.33]\u001b[A\n",
      "Iteration:  53%|█████▎    | 78/147 [02:35<03:06,  2.71s/it, loss=2.42]\u001b[A\n",
      "Iteration:  54%|█████▎    | 79/147 [02:36<03:26,  3.04s/it, loss=2.42]\u001b[A\n",
      "Iteration:  54%|█████▎    | 79/147 [02:38<03:26,  3.04s/it, loss=2.93]\u001b[A\n",
      "Iteration:  54%|█████▍    | 80/147 [02:38<03:02,  2.72s/it, loss=2.93]\u001b[A\n",
      "Iteration:  54%|█████▍    | 80/147 [02:39<03:02,  2.72s/it, loss=2.56]\u001b[A\n",
      "Iteration:  55%|█████▌    | 81/147 [02:40<02:41,  2.44s/it, loss=2.56]\u001b[A\n",
      "Iteration:  55%|█████▌    | 81/147 [02:41<02:41,  2.44s/it, loss=2.54]\u001b[A\n",
      "Iteration:  56%|█████▌    | 82/147 [02:42<02:26,  2.26s/it, loss=2.54]\u001b[A\n",
      "Iteration:  56%|█████▌    | 82/147 [02:43<02:26,  2.26s/it, loss=2.56]\u001b[A\n",
      "Iteration:  56%|█████▋    | 83/147 [02:44<02:15,  2.12s/it, loss=2.56]\u001b[A\n",
      "Iteration:  56%|█████▋    | 83/147 [02:45<02:15,  2.12s/it, loss=2.49]\u001b[A\n",
      "Iteration:  57%|█████▋    | 84/147 [02:45<02:08,  2.04s/it, loss=2.49]\u001b[A\n",
      "Iteration:  57%|█████▋    | 84/147 [02:47<02:08,  2.04s/it, loss=2.24]\u001b[A\n",
      "Iteration:  58%|█████▊    | 85/147 [02:47<02:04,  2.01s/it, loss=2.24]\u001b[A\n",
      "Iteration:  58%|█████▊    | 85/147 [02:49<02:04,  2.01s/it, loss=2.5] \u001b[A\n",
      "Iteration:  59%|█████▊    | 86/147 [02:49<02:01,  1.99s/it, loss=2.5]\u001b[A\n",
      "Iteration:  59%|█████▊    | 86/147 [02:51<02:01,  1.99s/it, loss=2.8]\u001b[A\n",
      "Iteration:  59%|█████▉    | 87/147 [02:51<01:57,  1.95s/it, loss=2.8]\u001b[A\n",
      "Iteration:  59%|█████▉    | 87/147 [02:53<01:57,  1.95s/it, loss=2.96]\u001b[A\n",
      "Iteration:  60%|█████▉    | 88/147 [02:53<01:54,  1.95s/it, loss=2.96]\u001b[A\n",
      "Iteration:  60%|█████▉    | 88/147 [02:54<01:54,  1.95s/it, loss=2.91]\u001b[A\n",
      "Iteration:  61%|██████    | 89/147 [02:55<01:50,  1.91s/it, loss=2.91]\u001b[A\n",
      "Iteration:  61%|██████    | 89/147 [02:56<01:50,  1.91s/it, loss=2.37]\u001b[A\n",
      "Iteration:  61%|██████    | 90/147 [02:57<01:47,  1.88s/it, loss=2.37]\u001b[A\n",
      "Iteration:  61%|██████    | 90/147 [02:58<01:47,  1.88s/it, loss=3.36]\u001b[A\n",
      "Iteration:  62%|██████▏   | 91/147 [02:59<01:45,  1.89s/it, loss=3.36]\u001b[A\n",
      "Iteration:  62%|██████▏   | 91/147 [03:00<01:45,  1.89s/it, loss=3.08]\u001b[A\n",
      "Iteration:  63%|██████▎   | 92/147 [03:01<01:43,  1.89s/it, loss=3.08]\u001b[A\n",
      "Iteration:  63%|██████▎   | 92/147 [03:02<01:43,  1.89s/it, loss=2.56]\u001b[A\n",
      "Iteration:  63%|██████▎   | 93/147 [03:02<01:43,  1.92s/it, loss=2.56]\u001b[A\n",
      "Iteration:  63%|██████▎   | 93/147 [03:04<01:43,  1.92s/it, loss=2.79]\u001b[A\n",
      "Iteration:  64%|██████▍   | 94/147 [03:04<01:42,  1.94s/it, loss=2.79]\u001b[A\n",
      "Iteration:  64%|██████▍   | 94/147 [03:06<01:42,  1.94s/it, loss=2.85]\u001b[A\n",
      "Iteration:  65%|██████▍   | 95/147 [03:06<01:41,  1.96s/it, loss=2.85]\u001b[A\n",
      "Iteration:  65%|██████▍   | 95/147 [03:08<01:41,  1.96s/it, loss=1.95]\u001b[A\n",
      "Iteration:  65%|██████▌   | 96/147 [03:08<01:40,  1.97s/it, loss=1.95]\u001b[A\n",
      "Iteration:  65%|██████▌   | 96/147 [03:10<01:40,  1.97s/it, loss=2.54]\u001b[A\n",
      "Iteration:  66%|██████▌   | 97/147 [03:10<01:37,  1.95s/it, loss=2.54]\u001b[A\n",
      "Iteration:  66%|██████▌   | 97/147 [03:12<01:37,  1.95s/it, loss=2.32]\u001b[A\n",
      "Iteration:  67%|██████▋   | 98/147 [03:12<01:34,  1.93s/it, loss=2.32]\u001b[A\n",
      "Iteration:  67%|██████▋   | 98/147 [03:14<01:34,  1.93s/it, loss=3.08]\u001b[A\n",
      "Iteration:  67%|██████▋   | 99/147 [03:14<01:31,  1.90s/it, loss=3.08]\u001b[A\n",
      "Iteration:  67%|██████▋   | 99/147 [03:15<01:31,  1.90s/it, loss=2.36]\u001b[A\n",
      "Iteration:  68%|██████▊   | 100/147 [03:16<01:28,  1.89s/it, loss=2.36]\u001b[A\n",
      "Iteration:  68%|██████▊   | 100/147 [03:17<01:28,  1.89s/it, loss=2.74]\u001b[A\n",
      "Iteration:  69%|██████▊   | 101/147 [03:18<01:26,  1.88s/it, loss=2.74]\u001b[A\n",
      "Iteration:  69%|██████▊   | 101/147 [03:19<01:26,  1.88s/it, loss=2.76]\u001b[A\n",
      "Iteration:  69%|██████▉   | 102/147 [03:20<01:24,  1.88s/it, loss=2.76]\u001b[A\n",
      "Iteration:  69%|██████▉   | 102/147 [03:21<01:24,  1.88s/it, loss=2.54]\u001b[A\n",
      "Iteration:  70%|███████   | 103/147 [03:22<01:22,  1.87s/it, loss=2.54]\u001b[A\n",
      "Iteration:  70%|███████   | 103/147 [03:23<01:22,  1.87s/it, loss=2.63]\u001b[A\n",
      "Iteration:  71%|███████   | 104/147 [03:23<01:20,  1.87s/it, loss=2.63]\u001b[A\n",
      "Iteration:  71%|███████   | 104/147 [03:25<01:20,  1.87s/it, loss=2.31]\u001b[A\n",
      "Iteration:  71%|███████▏  | 105/147 [03:25<01:18,  1.87s/it, loss=2.31]\u001b[A\n",
      "Iteration:  71%|███████▏  | 105/147 [03:27<01:18,  1.87s/it, loss=2.62]\u001b[A\n",
      "Iteration:  72%|███████▏  | 106/147 [03:27<01:16,  1.87s/it, loss=2.62]\u001b[A\n",
      "Iteration:  72%|███████▏  | 106/147 [03:29<01:16,  1.87s/it, loss=2.17]\u001b[A\n",
      "Iteration:  73%|███████▎  | 107/147 [03:29<01:14,  1.86s/it, loss=2.17]\u001b[A\n",
      "Iteration:  73%|███████▎  | 107/147 [03:30<01:14,  1.86s/it, loss=2.59]\u001b[A\n",
      "Iteration:  73%|███████▎  | 108/147 [03:31<01:12,  1.86s/it, loss=2.59]\u001b[A\n",
      "Iteration:  73%|███████▎  | 108/147 [03:32<01:12,  1.86s/it, loss=2.43]\u001b[A\n",
      "Iteration:  74%|███████▍  | 109/147 [03:33<01:10,  1.86s/it, loss=2.43]\u001b[A\n",
      "Iteration:  74%|███████▍  | 109/147 [03:34<01:10,  1.86s/it, loss=2.17]\u001b[A\n",
      "Iteration:  75%|███████▍  | 110/147 [03:35<01:08,  1.86s/it, loss=2.17]\u001b[A\n",
      "Iteration:  75%|███████▍  | 110/147 [03:36<01:08,  1.86s/it, loss=2.41]\u001b[A\n",
      "Iteration:  76%|███████▌  | 111/147 [03:36<01:07,  1.86s/it, loss=2.41]\u001b[A\n",
      "Iteration:  76%|███████▌  | 111/147 [03:38<01:07,  1.86s/it, loss=2.64]\u001b[A\n",
      "Iteration:  76%|███████▌  | 112/147 [03:38<01:05,  1.86s/it, loss=2.64]\u001b[A\n",
      "Iteration:  76%|███████▌  | 112/147 [03:40<01:05,  1.86s/it, loss=2.76]\u001b[A\n",
      "Iteration:  77%|███████▋  | 113/147 [03:40<01:03,  1.86s/it, loss=2.76]\u001b[A\n",
      "Iteration:  77%|███████▋  | 113/147 [03:42<01:03,  1.86s/it, loss=2.63]\u001b[A\n",
      "Iteration:  78%|███████▊  | 114/147 [03:42<01:01,  1.86s/it, loss=2.63]\u001b[A\n",
      "Iteration:  78%|███████▊  | 114/147 [03:43<01:01,  1.86s/it, loss=3.17]\u001b[A\n",
      "Iteration:  78%|███████▊  | 115/147 [03:44<00:59,  1.86s/it, loss=3.17]\u001b[A\n",
      "Iteration:  78%|███████▊  | 115/147 [03:45<00:59,  1.86s/it, loss=2.54]\u001b[A\n",
      "Iteration:  79%|███████▉  | 116/147 [03:46<00:57,  1.86s/it, loss=2.54]\u001b[A\n",
      "Iteration:  79%|███████▉  | 116/147 [03:47<00:57,  1.86s/it, loss=2.68]\u001b[A\n",
      "Iteration:  80%|███████▉  | 117/147 [03:48<00:55,  1.86s/it, loss=2.68]\u001b[A\n",
      "Iteration:  80%|███████▉  | 117/147 [03:49<00:55,  1.86s/it, loss=3.01]\u001b[A\n",
      "Iteration:  80%|████████  | 118/147 [03:49<00:53,  1.86s/it, loss=3.01]\u001b[A\n",
      "Iteration:  80%|████████  | 118/147 [03:51<00:53,  1.86s/it, loss=2.56]\u001b[A\n",
      "Iteration:  81%|████████  | 119/147 [03:51<00:52,  1.86s/it, loss=2.56]\u001b[A\n",
      "Iteration:  81%|████████  | 119/147 [03:53<00:52,  1.86s/it, loss=2.97]\u001b[A\n",
      "Iteration:  82%|████████▏ | 120/147 [03:53<00:50,  1.86s/it, loss=2.97]\u001b[A\n",
      "Iteration:  82%|████████▏ | 120/147 [03:55<00:50,  1.86s/it, loss=2.37]\u001b[A\n",
      "Iteration:  82%|████████▏ | 121/147 [03:55<00:48,  1.86s/it, loss=2.37]\u001b[A\n",
      "Iteration:  82%|████████▏ | 121/147 [03:56<00:48,  1.86s/it, loss=2.57]\u001b[A\n",
      "Iteration:  83%|████████▎ | 122/147 [03:57<00:46,  1.86s/it, loss=2.57]\u001b[A\n",
      "Iteration:  83%|████████▎ | 122/147 [03:58<00:46,  1.86s/it, loss=2.4] \u001b[A\n",
      "Iteration:  84%|████████▎ | 123/147 [03:59<00:44,  1.86s/it, loss=2.4]\u001b[A\n",
      "Iteration:  84%|████████▎ | 123/147 [04:00<00:44,  1.86s/it, loss=2.43]\u001b[A\n",
      "Iteration:  84%|████████▍ | 124/147 [04:01<00:43,  1.88s/it, loss=2.43]\u001b[A\n",
      "Iteration:  84%|████████▍ | 124/147 [04:02<00:43,  1.88s/it, loss=2.2] \u001b[A\n",
      "Iteration:  85%|████████▌ | 125/147 [04:03<00:41,  1.90s/it, loss=2.2]\u001b[A\n",
      "Iteration:  85%|████████▌ | 125/147 [04:04<00:41,  1.90s/it, loss=2.5]\u001b[A\n",
      "Iteration:  86%|████████▌ | 126/147 [04:05<00:39,  1.90s/it, loss=2.5]\u001b[A\n",
      "Iteration:  86%|████████▌ | 126/147 [04:06<00:39,  1.90s/it, loss=2.71]\u001b[A\n",
      "Iteration:  86%|████████▋ | 127/147 [04:06<00:37,  1.89s/it, loss=2.71]\u001b[A\n",
      "Iteration:  86%|████████▋ | 127/147 [04:08<00:37,  1.89s/it, loss=2.38]\u001b[A\n",
      "Iteration:  87%|████████▋ | 128/147 [04:08<00:35,  1.89s/it, loss=2.38]\u001b[A\n",
      "Iteration:  87%|████████▋ | 128/147 [04:10<00:35,  1.89s/it, loss=2.33]\u001b[A\n",
      "Iteration:  88%|████████▊ | 129/147 [04:10<00:33,  1.88s/it, loss=2.33]\u001b[A\n",
      "Iteration:  88%|████████▊ | 129/147 [04:12<00:33,  1.88s/it, loss=2.12]\u001b[A\n",
      "Iteration:  88%|████████▊ | 130/147 [04:12<00:31,  1.87s/it, loss=2.12]\u001b[A\n",
      "Iteration:  88%|████████▊ | 130/147 [04:13<00:31,  1.87s/it, loss=2.41]\u001b[A\n",
      "Iteration:  89%|████████▉ | 131/147 [04:14<00:29,  1.87s/it, loss=2.41]\u001b[A\n",
      "Iteration:  89%|████████▉ | 131/147 [04:15<00:29,  1.87s/it, loss=2.47]\u001b[A\n",
      "Iteration:  90%|████████▉ | 132/147 [04:16<00:28,  1.87s/it, loss=2.47]\u001b[A\n",
      "Iteration:  90%|████████▉ | 132/147 [04:17<00:28,  1.87s/it, loss=2.63]\u001b[A\n",
      "Iteration:  90%|█████████ | 133/147 [04:18<00:26,  1.87s/it, loss=2.63]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration:  90%|█████████ | 133/147 [04:19<00:26,  1.87s/it, loss=3.02]\u001b[A\n",
      "Iteration:  91%|█████████ | 134/147 [04:19<00:24,  1.86s/it, loss=3.02]\u001b[A\n",
      "Iteration:  91%|█████████ | 134/147 [04:21<00:24,  1.86s/it, loss=2.47]\u001b[A\n",
      "Iteration:  92%|█████████▏| 135/147 [04:21<00:22,  1.86s/it, loss=2.47]\u001b[A\n",
      "Iteration:  92%|█████████▏| 135/147 [04:23<00:22,  1.86s/it, loss=2.33]\u001b[A\n",
      "Iteration:  93%|█████████▎| 136/147 [04:23<00:20,  1.86s/it, loss=2.33]\u001b[A\n",
      "Iteration:  93%|█████████▎| 136/147 [04:25<00:20,  1.86s/it, loss=2.59]\u001b[A\n",
      "Iteration:  93%|█████████▎| 137/147 [04:25<00:18,  1.86s/it, loss=2.59]\u001b[A\n",
      "Iteration:  93%|█████████▎| 137/147 [04:26<00:18,  1.86s/it, loss=2.24]\u001b[A\n",
      "Iteration:  94%|█████████▍| 138/147 [04:27<00:16,  1.86s/it, loss=2.24]\u001b[A\n",
      "Iteration:  94%|█████████▍| 138/147 [04:28<00:16,  1.86s/it, loss=2.66]\u001b[A\n",
      "Iteration:  95%|█████████▍| 139/147 [04:29<00:14,  1.86s/it, loss=2.66]\u001b[A\n",
      "Iteration:  95%|█████████▍| 139/147 [04:30<00:14,  1.86s/it, loss=3]   \u001b[A\n",
      "Iteration:  95%|█████████▌| 140/147 [04:31<00:12,  1.86s/it, loss=3]\u001b[A\n",
      "Iteration:  95%|█████████▌| 140/147 [04:32<00:12,  1.86s/it, loss=2.39]\u001b[A\n",
      "Iteration:  96%|█████████▌| 141/147 [04:32<00:11,  1.85s/it, loss=2.39]\u001b[A\n",
      "Iteration:  96%|█████████▌| 141/147 [04:34<00:11,  1.85s/it, loss=2.49]\u001b[A\n",
      "Iteration:  97%|█████████▋| 142/147 [04:34<00:09,  1.85s/it, loss=2.49]\u001b[A\n",
      "Iteration:  97%|█████████▋| 142/147 [04:36<00:09,  1.85s/it, loss=2.42]\u001b[A\n",
      "Iteration:  97%|█████████▋| 143/147 [04:36<00:07,  1.85s/it, loss=2.42]\u001b[A\n",
      "Iteration:  97%|█████████▋| 143/147 [04:38<00:07,  1.85s/it, loss=2.48]\u001b[A05/18/2019 20:22:27 - WARNING - pytorch_pretrained_bert.optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.\n",
      "\n",
      "Iteration:  98%|█████████▊| 144/147 [04:38<00:05,  1.85s/it, loss=2.48]\u001b[A\n",
      "Iteration:  98%|█████████▊| 144/147 [04:39<00:05,  1.85s/it, loss=2.6] \u001b[A05/18/2019 20:22:29 - WARNING - pytorch_pretrained_bert.optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.\n",
      "\n",
      "Iteration:  99%|█████████▊| 145/147 [04:40<00:03,  1.86s/it, loss=2.6]\u001b[A\n",
      "Iteration:  99%|█████████▊| 145/147 [04:41<00:03,  1.86s/it, loss=2.62]\u001b[A05/18/2019 20:22:31 - WARNING - pytorch_pretrained_bert.optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.\n",
      "\n",
      "Iteration:  99%|█████████▉| 146/147 [04:42<00:01,  1.86s/it, loss=2.62]\u001b[A\n",
      "Iteration:  99%|█████████▉| 146/147 [04:42<00:01,  1.86s/it, loss=2.13]\u001b[A05/18/2019 20:22:32 - WARNING - pytorch_pretrained_bert.optimization -   Training beyond specified 't_total'. Learning rate multiplier set to 0.0. Please set 't_total' of WarmupLinearSchedule correctly.\n",
      "\n",
      "Epoch: 100%|██████████| 5/5 [22:21<00:00, 269.29s/it]67s/it, loss=2.13]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "trace = runTraining()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss=[]\n",
    "n =[]\n",
    "for item in trace:\n",
    "    n.append(item[0])\n",
    "    loss.append(item[1])\n",
    "n = running_mean(n,25)\n",
    "loss = running_mean(loss,25)\n",
    "plt.plot(n,loss,label='Training Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "05/18/2019 20:32:18 - INFO - __main__ -   device: cuda n_gpu: 1, distributed training: False, 16-bits training: False\n",
      "05/18/2019 20:32:18 - INFO - pytorch_pretrained_bert.modeling -   loading archive file ./models/Organic/Current\n",
      "05/18/2019 20:32:18 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "05/18/2019 20:32:20 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file ./models/Organic/Current/vocab.txt\n",
      "/home/farrukh/anaconda3/envs/nlpLab/lib/python3.7/site-packages/ipykernel_launcher.py:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "05/18/2019 20:32:20 - INFO - __main__ -   Writing example 0 of 414\n",
      "05/18/2019 20:32:20 - INFO - __main__ -   *** Example ***\n",
      "05/18/2019 20:32:20 - INFO - __main__ -   guid: validation-0\n",
      "05/18/2019 20:32:20 - INFO - __main__ -   tokens: [CLS] i do not consider myself someone who “ supports gm ##o ” , just like i also don ’ t consider myself someone who “ supports chemistry ” , or [SEP]\n",
      "05/18/2019 20:32:20 - INFO - __main__ -   input_ids: 101 1045 2079 2025 5136 2870 2619 2040 1523 6753 13938 2080 1524 1010 2074 2066 1045 2036 2123 1521 1056 5136 2870 2619 2040 1523 6753 6370 1524 1010 2030 102\n",
      "05/18/2019 20:32:20 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/18/2019 20:32:20 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/18/2019 20:32:20 - INFO - __main__ -   label: gg-c-0 (id = 393)\n",
      "05/18/2019 20:32:20 - INFO - __main__ -   *** Example ***\n",
      "05/18/2019 20:32:20 - INFO - __main__ -   guid: validation-1\n",
      "05/18/2019 20:32:20 - INFO - __main__ -   tokens: [CLS] same goes for “ anti - gm ##o ” . [SEP]\n",
      "05/18/2019 20:32:20 - INFO - __main__ -   input_ids: 101 2168 3632 2005 1523 3424 1011 13938 2080 1524 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/18/2019 20:32:20 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/18/2019 20:32:20 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/18/2019 20:32:20 - INFO - __main__ -   label: gg-g-0 (id = 378)\n",
      "05/18/2019 20:32:20 - INFO - __main__ -   *** Example ***\n",
      "05/18/2019 20:32:20 - INFO - __main__ -   guid: validation-2\n",
      "05/18/2019 20:32:20 - INFO - __main__ -   tokens: [CLS] this is also why so many scientists object to labeling foods as gm ##o or gm ##o - free : it ’ s essentially a meaningless label because it carries [SEP]\n",
      "05/18/2019 20:32:20 - INFO - __main__ -   input_ids: 101 2023 2003 2036 2339 2061 2116 6529 4874 2000 28847 9440 2004 13938 2080 2030 13938 2080 1011 2489 1024 2009 1521 1055 7687 1037 25120 3830 2138 2009 7883 102\n",
      "05/18/2019 20:32:20 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/18/2019 20:32:20 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/18/2019 20:32:20 - INFO - __main__ -   label: gg-g-0 (id = 378)\n",
      "05/18/2019 20:32:20 - INFO - __main__ -   *** Example ***\n",
      "05/18/2019 20:32:20 - INFO - __main__ -   guid: validation-3\n",
      "05/18/2019 20:32:20 - INFO - __main__ -   tokens: [CLS] might as well label food as “ made with electricity ” , or “ contains no chemicals ” ( fact check : * everything * is made out of chemicals [SEP]\n",
      "05/18/2019 20:32:20 - INFO - __main__ -   input_ids: 101 2453 2004 2092 3830 2833 2004 1523 2081 2007 6451 1524 1010 2030 1523 3397 2053 12141 1524 1006 2755 4638 1024 1008 2673 1008 2003 2081 2041 1997 12141 102\n",
      "05/18/2019 20:32:20 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/18/2019 20:32:20 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/18/2019 20:32:20 - INFO - __main__ -   label: cp-ll-0 (id = 216)\n",
      "05/18/2019 20:32:20 - INFO - __main__ -   *** Example ***\n",
      "05/18/2019 20:32:20 - INFO - __main__ -   guid: validation-4\n",
      "05/18/2019 20:32:20 - INFO - __main__ -   tokens: [CLS] for the current commercial gm ##o crops , there are essentially no health reasons to choose in favor or against gm ##o crops just because they are gm ##o . [SEP]\n",
      "05/18/2019 20:32:20 - INFO - __main__ -   input_ids: 101 2005 1996 2783 3293 13938 2080 8765 1010 2045 2024 7687 2053 2740 4436 2000 5454 1999 5684 2030 2114 13938 2080 8765 2074 2138 2027 2024 13938 2080 1012 102\n",
      "05/18/2019 20:32:20 - INFO - __main__ -   input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "05/18/2019 20:32:20 - INFO - __main__ -   segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "05/18/2019 20:32:20 - INFO - __main__ -   label: gg-g-0 (id = 378)\n",
      "05/18/2019 20:32:20 - INFO - __main__ -   ***** Running evaluation *****\n",
      "05/18/2019 20:32:20 - INFO - __main__ -     Num examples = 414\n",
      "05/18/2019 20:32:20 - INFO - __main__ -     Batch size = 32\n",
      "Evaluating: 100%|██████████| 13/13 [00:05<00:00,  2.50it/s]\n",
      "/home/farrukh/anaconda3/envs/nlpLab/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/farrukh/anaconda3/envs/nlpLab/lib/python3.7/site-packages/sklearn/metrics/classification.py:1439: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "05/18/2019 20:32:26 - INFO - __main__ -   ***** Eval results *****\n",
      "05/18/2019 20:32:26 - INFO - __main__ -     acc = {'acc': 0.178743961352657, 'f1': 0.12740896708145927, 'acc_and_f1': 0.15307646421705812}\n",
      "05/18/2019 20:32:26 - INFO - __main__ -     eval_loss = 3.716112026801476\n"
     ]
    }
   ],
   "source": [
    "runTest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
